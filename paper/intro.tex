\section{Introduction}
\label{sec:introduction}

As an implementor of a programming language, it is often useful to automatically
glean facts about a program such as ``this program is well-typed'', ``this
higher-order function is always called with argument $\Lam{x}{x+1}$'' or ``this
program never evaluates $x$'' by way of \emph{static (program) analysis}.

\paragraph{Analysis follows structure}
If the implementation language is a functional one, then usually such static
analyses are formulated as a function defined by \emph{structural recursion} on
the input term.
For example, given an application expression $(\pe_1~\pe_2)$,
the property ``$(\pe_1~\pe_2)$ never evaluates its free variable $x$'' can be
\emph{conservatively approximated} (here: It's OK to say ``No'' more often) by
the property ``$\pe_1$ and $\pe_2$ never evaluate $x$''.

Such a structural formulation is quite convenient:
(1) Structural recursion gives an immediate proof of termination and can
    further be exploited in other inductive proofs, all within decidable
    territory.
(2) A structurally-defined function $f$ is often \emph{compositional}, meaning that
    if you replace a sub-expression $e$ in a bigger expression $C[e]$ by another
    expression $e'$ with $f(e) = f(e')$, the overall result
    of the containing expression $f(C[e]) = f(C[e'])$ does not change.
    This makes it easy for humans to understand and reason about the function,
    because the result of a big expression depends on the results of its parts
    (and not on the shape of the sub-expressions themselves).

For static analyses, especially more complicated ones, it is good practice to
provide a proof of correctness of some sort. If the correctness statement can
be expressed in terms of a \emph{denotational
semantics}~\cite{ScottStrachey:71}, then the recursion structure of analysis
function and semantics function line up nicely. As a result, the proof of
correctness can be conducted by simple induction over the expression.

\paragraph{Domain Theory is a leaky abstraction}
Alas, even when the denotational semantics is ``standard'', the hard part is in
coming up with a suitable correctness predicate!
Traditionally, the semantic domain of denotational semantics models diverging
and stuck programs with $\bot$, the function that is undefined everywhere.
There is rich and complicated literature on defining an algebraic
domain~\cite{Scott:71} that is suitable to denote untyped lambda calculus.
The key is embedding the subclass of \emph{continuous} functions between domain
elements into the domain itself. All computable functions can be proven
continuous, so this is a sufficient substrate.
Now, to prove a predicate $P(d)$ of some denotation $d$, such as ``$d$ has type
$τ$'' by structural induction, one has to show first that $P$ is compatible
with continuity; perhaps by proving that $P$ characterises a sub-domain or an
ideal in the domain~\cite{Milner:78}.

Note that this is all \emph{before} even attempting the proof! Often, the
denotational semantics or even the domain itself needs adjustments.
The former case needs a proof of continuity, while the latter can be quite
involved and non-compositional for effects such as exceptions, concurrency and
state, as \citep{WrightFelleisen:87} noted.

We think that most of the troubles in the application of Domain Theory are
caused by the non-commital nature of the approximation order, in that any
predicate on total elements also needs to accept its partial approximation;
hence it is desirable to strive for \emph{total} descriptions of the
potentially program infinite behaviors.

the crucial property that any
, and it was
a great achievement of \citep{Scott:71} turns out that Domain Theory is the result of equipping Doing so invites
and stuck programs. Consider the following program using recursive let that is
infinitely-looping
\begin{equation}
  \label{eqn:loop}
  \pe_{loop} \triangleq \Let{id}{\Lam{x}{x}}{\Let{loop}{id~loop}{loop}}
\end{equation}
The traditional denotational semantics after Scott and Strachey would equate all
of the following programs:
$\semscott{\pe_{loop}}_ρ = \semscott{\Let{loop}{id~loop}{loop}}_ρ =
\semscott{\mathsf{segfault}}_ρ = \bot$.
Note that the first program has an infinite loop, the second one is not
well-scoped (thus stuck at some point) and the last one is a straight out crash
in the style of an imprecise exception \cite{imprecise-exceptions}.
To a compiler developer, this conflation is both a reason for joy (more
optimisation opportunities) and a reason for ... reflection (users didn't expect
their infinite loop to be optimised into a crash). Such issues come up in
practice \sg{cite GHC issues}.

More seriously, it is impossible to prove by way of a denotational semantics
that a static analysis does not misoptimise infinite behaviors.
(a) The \emph{potential liveness} analysis that says ``$\pe_{loop}$ never
evaluates $id$'' could be proven ``correct''.
(b) A type analysis that says ``$\Let{loop}{id~loop}{loop}$ is closed and
well-typed'' can still be proven progressing as long as $\pe_{loop}$ can
be well-typed (which would not be too surprising), because the latter is
denotationally equivalent to the former.
(c) Imagine that $id$ was supplied as a parameter to $loop$ instead, \eg
$...\ \Let{loop}{\Lam{f}{f~(loop~f)}}{loop~id}$. Then a control-flow analysis
\cite{Shivers:91} that says ``$f$ is never bound to $id$'' can be proven correct
in terms of the denotational semantics.

Furthermore, although it is sensible (in the terminating case) to ask whether or
not $x$ is \emph{never} evaluated in terms of the denotational semantics, asking
whether $x$ is evaluated \emph{at most once} is not, for the same reason that
traditional denotational semantics is not able to discern call-by-name from
call-by-need. Yet, to the Glasgow Haskell Compiler, this distinction is very
much of concern!

When denotational semantics fails the compiler developer, they turn to a
correctness criterion in terms of a \emph{structural operational semantics}.
This was the approach taken by \cite{cardinality} to prove evaluation
cardinality properties such as potential liveness. The drawback of this proof
framework is the immense complexity arising from the disconnect between a
structural definition and a transition system; matters such as substitution,
multiple heap activations of the let binding, non-determinism and fixpoint
induction abound. It is hard to raise confidence in such a proof without full
mechanisation.

One could adopt the approach of \emph{Abstracting Abstract Machines} \cite{aam}
and let the structure of the semantics dictate the structure of the
analysis for a re-usable proof of correctness via abstract interpretation
\cite{Cousot:21}.
However, that is not how the static analyses work that the authors are familiar
with.
For example, it would be quite an effort to rewrite the neat,
structurally-defined analyses of the Glasgow Haskell Compiler into a fixpoint
iteration on the approximated states of an abstract transition system.

The contributions of this work are as follows:
\begin{itemize}
  \item In \Cref{sec:problem}, we give a more formal exposition to the
    problems we just introduced and are inclined to solve.
  \item In \Cref{sec:semantics}, we give a \emph{structurally-defined} semantics
    for lambda calculus that is \emph{able to discern stuck, diverging and
    converging programs}. Furthermore, it is a semantics for \emph{call-by-need}
    lambda calculus that is distinct from similar ones for call-by-name or
    call-by-value and allows to observe evaluation cardinality as needed.
    We believe that our semantics is the first with the aforementioned two
    qualities and prove it correct \wrt a standard operational semantics. The
    idea borrows heavily from the idea of a maximal prefix trace semantics
    advocated by \citep{Cousot:21}.
  \item The semantics in \Cref{sec:semantics} is one generating \emph{stateful}
    traces in a standard operational semantics, and serves mostly as a
    convenient bridge for proving bisimulation. In \Cref{sec:stateless} we will
    define an equivalent, but more convenient \emph{stateless} semantics and we
    will see how to recover necessary state from program history as needed.
  \item We employ the stateless semantics as a collecting semantics and derive
    $\semlive{\wild}$ by calculational design \cite{Cousot:21}.
    Similar derivations will be made for a simple type system as well as for
    control flow analysis. \sg{Hopefully :)}
  \item Talk about prototype in Haskell?
  \item Related Work \Cref{sec:related-work}
\end{itemize}

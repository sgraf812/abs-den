\section{Introduction}
\label{sec:introduction}

A \emph{static program analysis} infers facts about a program, such
as ``this program is well-typed'', ``this higher-order function is always called
with argument $\Lam{x}{x+1}$'' or ``this program never evaluates $x$''.
In a functional-language setting, such static analyses are
often defined by \emph{structural recursion} on the input term.
In the application case, this recursion must produce a \emph{summary}
for the function to be applied (\ie, function type), and then apply that summary
to produce an abstraction that is sound for the particular argument.
Function summaries play a crucial role in achieving modular higher-order
analyses, because it is much more efficient to apply the summary of a function
instead of reanalysing its definition across modules.

To prove the analysis correct with respect to the language semantics,
it is much easier if the semantics is also defined by structural recursion,
as is the case for a \emph{denotational semantics}~\citep{ScottStrachey:71};
then the denotational semantics and the static analysis
``line up'' and the correctness proof is relatively straightforward.
Indeed, one can often break up the proof into manageable subgoals by regarding
the analysis as an \emph{abstract interpretation} of the denotational
semantics~\citep{Cousot:21}, particularly when the analysis shares structure
with the semantics.

Alas, traditional denotational semantics does not model operational details --
but those details might be the whole point of the analysis.
For example, we might want to ask ``How often does $\pe$ evaluate its free
variable $x$?''; but a standard denotational semantics simply does not express
the concept of ``evaluating a variable''.
So we are typically driven to use an \emph{operational
semantics}~\citep{Plotkin:81}, which directly models operational details like
the stack, heap, and sharing, and sees program execution as a sequence of
machine states.
Now we have two un-appealing alternatives:
\begin{itemize}
\item Peform a difficult correctness proof, that links an operational semantics for the language
  with an analysis defined by structural recursion.
\item Re-imagine and re-implement the analysis as an abstraction of the
  reachable states of an operational semantics.
  This is the essence of \emph{Abstracting Abstract Machines} (AAM) \cite{aam}
  recipe.
  A very fruitful approach, but one that is not summary-based and thus
  non-modular; in contrast to a type analysis.
\end{itemize}

Recent years have seen successful applications of the AAM recipe to big-step
style \emph{Definitional Interpreters} as well \cite{Reynolds:72,adi,Keidel:18},
however, the structural mismatch and resulting lack of modularity persists.
Furthermore, big-step interpreters are no substitute for a formal semantics
because they diverge when their input program diverges, thus providing no
safety guarantees for infinite program behaviors.

In this paper, we explore \emph{Denotational Interpreters}:
total, mathematical objects that live at the intersection of
structurally-defined definitional interpreters and denotational semantics,
enjoying a straightforward encoding in typical higher-order programming
languages.

We make the following contributions:
\begin{itemize}
\item \sg{Update}
  We use a concrete example (usage analysis) to explain the problems we sketched
  above: the lack of operational detail in denotational semantics, and the structural mismatch
  between the semantics and the analysis (\Cref{sec:problem}).
\item \Cref{sec:interp} walks through the structural definition of our shared
  denotational interpreter and its type class algebra in Haskell.
  Since our object language is in A-normal form, we illustrate the ease with
  which it is possible to endow it with call-by-name, variants of call-by-need,
  and even variants of call-by-value evaluation strategies via different type
  class instances, in most cases leading to a definition producing abstractions
  of small-step traces.
\item In the AAM spirit, our denotational interpreter can also be
  instantiated at more \emph{abstract} semantic domains.
  \Cref{sec:abstractions} gives three examples, covering a wide range of
  applications: Type analysis, usage analysis and 0CFA control-flow analysis.
  The former two have interesting summary mechanisms that our framework
  expresses in a natural way.
\item \Cref{sec:adequacy} delivers proof that denotational
  interpreters are actual semantics.
  Totality is demonstrated by embedding the shared interpreter and its by-name
  and by-need instantiations in Guarded Cubical Agda~\citep{tctt}.
  Furthermore, the by-need instantiation is shown to adequatly generate an
  abstraction of a lazy Krivine trace, preserving length as well as arbitrary
  information about each transition taken.
\item In \Cref{sec:soundness}, we apply abstract interpretation to characterise
  a set of soundness conditions that the type class instances of an abstract
  domain must satisfy in order to soundly approximate by-name interpretation.
  None of the conditions mention shared code, and, more remarkably, none of the
  conditions mention the concrete semantics or the Galois connection either!
  This enables us to finally prove usage analysis correct \wrt the by-name
  semantics in a third of a page, because all reasoning happens in the abstract.
\item
  \sg{Write more here once I revisited Related Work}
  We elaborate this claim in \Cref{sec:related-work}, where we compare to
  Related Work.
\end{itemize}


\begin{comment}
To an implementor of a programming language, it is often useful to employ a
\emph{static program analysis} to automatically infer facts about a program such
as ``this program is well-typed'', ``this higher-order function is always called
with argument $\Lam{x}{x+1}$'' or ``this program never evaluates $x$''.

If the implementation language is a functional one, then usually such static
analyses are formulated as a function defined by \emph{structural recursion} on
the input term.
For example, given an application expression $(\pe_1~\pe_2)$,
the property ``$(\pe_1~\pe_2)$ never evaluates its free variable $x$'' can be
conservatively approximated by the property ``$\pe_1$ never evaluates $x$
and $\pe_2$ never evaluates $x$''.

It is quite convenient to formulate a static analysis structurally:
(1) Structural recursion gives an immediate proof of termination and can
    further be exploited in other inductive proofs.
(2)~A structurally-defined function $f$ is often \emph{compositional}, meaning that
    the result of $f\denot{\pe_1~\pe_2}$ is obtained by combining the independent
    calls $f\denot{\pe_1}$ and $f\denot{\pe_2}$. % but not of the structure of either $\pe_1$ or $\pe_2$.
    Compositionality makes it easy for humans to understand and reason about the
    function, as well as allows to abstract away program syntax in the first
    place. \slpj{I don't understand the ``as well as''.  I would say ``This independence
    makes both analysis and proof much simpler.''}


\subsection{Compositional Analyses Abstract Denotational Semantics}

For static analyses, especially more complicated ones, it is good practice to
provide a proof of correctness.
If the correctness statement can be expressed in terms of a \emph{denotational
semantics}~\citep{ScottStrachey:71}, then the recursion structure of analysis
function and semantics function line up nicely.
As a result, the proof of correctness can be conducted by simple induction
over the expression.
Abstract interpretation~\citep{Cousot:21} provides a neat framework to derive
structural static analyses that are proven correct by simple equational
reasoning.

%\subsection{Domain Theory is leaky and too abstract}
%Alas, there are several shortcomings to traditional denotational semantics:
%\begin{itemize}
%  \item Program behaviors are denoted by elements of an algebraic domain.
%        Crucially, such algebraic domains allow embedding of functions
%        operating on algebraic domain elements~\citep{ScottStrachey:71}.
%        However, this embedding is restricted to the subset of
%        \emph{continuous} functions%
%        \footnote{Continuous functions in turn form a superset of all
%        \emph{computable} functions.}.
%
%        As a result, coming up with one's own denotational semantics requires a
%        proof of continuity of all involved functions that is often hand-waved
%        over when the semantics is ``sufficiently standard''.
%        In other cases, continuity is proven for a number of combinators and
%        their composition, and then the semantics is formulated in terms of this
%        meta language of combinators, adversely affecting the presentation of the
%        semantics.
%  \item Traditional domain theory proved non-compositional~\citep{WrightFelleisen:94},
%        in that language features such as exceptions, state and nondeterminism
%        require individual, complicated adjustments where it is far from clear
%        how to combine these adjustments in a way that the combined semantics'
%        functions are still continuous.
%  \item There have been denotational semantics for call-by-name and
%        call-by-value, but none for call-by-need. That is a symptom of the fact
%        that traditional domain theory eschews any account of operational
%        detail, such as evaluation cardinality.
%\end{itemize}
%
%The third point is important in the context of quantitative type
%systems~\citep{Atkey:18} and optimising compilers for call-by-need languages such
%as the Glasgow Haskell Compiler.
%The first two points have been noted by \citet{WrightFelleisen:94}, establishing
%operational semantics as the Gold Standard to this day.
%The past twenty years have proven that the methodology scales to
%big and complex type systems, building on techniques such as
%\emph{step-indexing}~\citep{AppelMcAllester:01,DreyerAhmedBirkedal:11}.

\subsection{Operational Semantics is Detailed and Systematically Scalable}

Alas, the existing denotational semantics for lambda calculus are insufficiently
expressive to model operational detail such as \emph{evaluation cardinality},
\eg, ``How often does $\pe$ evaluate its free variable $x$?'', because they lack
the concept of ``evaluating a variable''.
Thus, a static analysis that cares about evaluation cardinality (we will give
an example in \Cref{sec:problem}) cannot be proven correct \wrt traditional
denotational semantics.
A related issue is that up to this day, there are denotational semantics for
call-by-value and call-by-name but none for call-by-need, because memoisation is
not a concept that is observable in traditional denotational semantics.

Ultimately, traditional denotational semantics fell out of fashion due to
scaling issues we think that are ultimately caused by direct use of algebraic
domain theory. \citet{WrightFelleisen:94} established operational semantics
as the gold standard semantics for correctness proofs to this day.
The past thirty years have scaled the methodology to
big and complex type systems, building on techniques such as
\emph{step-indexing}~\citep{AppelMcAllester:01,DreyerAhmedBirkedal:11}.
However, operational semantics are expressed as transition systems which do not
easily admit a compositional definition as a function of the input expression.
%\footnote{While~\citet{Cousot:02} has derived trace-based fixpoint
%semantics from an arbitrary transition system, the resulting function is not
%structurally-recursive in the expression.}
In giving in to operational semantics, semanticists have chosen predictable
step-wise reasoning over elegant equational reasoning backed by
compositionality.

%In preservation proofs of type systems, step-wise reasoning necessitates
%substitution lemmas to cope with the non-structural recursion during variable
%lookup and beta reduction, an inconvenience that is readily accepted when the
%alternative would be to tame domain theory.

As a result, we are stuck with a large amount of literature and implementations
on \emph{compositional} program analyses such as in \citet{cardinality-ext}
that have to be proven correct \wrt a transition system.
Compared to when a denotational semantics is available, this \emph{structural
mismatch} necessitates the invention of ad-hoc, complicated correctness
relations that must proven to be preserved during transitions.

\subsection{Static Analysis by Abstraction of Operational Semantics}

When there is no semantics that structurally matches our analysis, we could
try to rearrange our analysis.
That is the idea of \emph{Abstracting Abstract Machines}~\citep{aam} which
applies abstract interpretation to the transition semantics.

The resulting static analysis is defined as an iterative fixpoint procedure that
approximates the set of possible states in the transition system that reach a
given sub-expression.
However, such a definition is hard to reconcile with existing battle-proven and
compositional static analyses in compilers such as the Glasgow Haskell
Compiler (GHC)~\citep{MarlowJones:12} due to the structural mismatch.
One could rewrite GHC's analyses using AAM, but that is a path that has seldomly
been taken.
It would be preferable to verify these existing implementations instead.

%It is not so simple in lattice-based compiler analyses, because recursive
%$\mathbf{let}$ bindings invite fixpoint iteration. If you think about
%``well-annotatedness'' of a machine configuration according to an analysis as a
%type system, the corresponding ``preservation lemma'' needs to relate fixpoints
%before and after \emph{every} kind of machine transition!
%This necessitates a dreadful application of fixpoint induction, unfolding
%the whole analysis function body before and after each step and thus culminating
%in beaurocratic nightmare.
%Of course, the definition of ``well-annotatedness'' needs to encompass whole
%machine configurations, including the (mutually recursive, in call-by-need)
%stack and heap, see the proofs in~\citep{cardinality-ext} for a taste. It is
%hard to raise confidence in such a proof without full mechanisation.
%A denotational semantics would arguably simplify the proof, unfortunately one
%allowing to observe evaluation cardinality of call-by-need is lacking to this
%day.

\subsection{Summary of Contributions}

We aim to resolve the aforementioned tensions around operational
detail and structural mismatch via the following contributions:
\begin{itemize}
  \item In \Cref{sec:problem}, we explicate the lack of operational detail in
    denotational semantics and the structural mismatch problem \wrt operational
    semantics by the example of \emph{usage analysis}, a generalisation of
    deadness analysis.
  \item In \Cref{sec:vanilla}, we give a compositional semantics
    for call-by-need lambda calculus that generates an abstraction of
    potentially infinite small-step traces.
    Unlike traditional formulations of denotational semantics, our semantics
    is defined by \emph{guarded recursion}~\citep{gdtt} and thus total as a
    mathematical function.
    Our semantics is distinct from similar ones for call-by-name or
    call-by-value and we prove it adequate \wrt a standard operational semantics.
  \item The semantics in \Cref{sec:vanilla} is one that encodes the bare minimum
    of information in traces to be useful.
    This is comparable to the traditional denotational approach, where
    denotations are either $\bot$ or a value.
    In \Cref{sec:eventful} we will see how simple it is to \emph{instrument} our
    semantics to provide richer traces, carrying \emph{events} describing
    small-step transitions.
    The resulting idea of an \emph{eventful} semantics borrows ideas from the
    \emph{maximal trace semantics} of \citet{Cousot:21}.
  \item
    We believe that our notion of \emph{trace-based semantics for lambda calculus}
    is the first to model operational detail of call-by-need behavior while
    enjoying a structural definition. Our pattern naturally generalises
    to similar definitions generating call-by-value traces and we believe
    that its trace-based nature enables unprecedented application of
    \citeauthor{Cousot:21}'s intraprocedural framework in interprocedural
    contexts without the need for a control-flow graph abstraction.
  \item In \Cref{sec:essence} we record a few meta-theoretic results about our
    new class of semantics, such as a partially-ordered heap forcing relation,
    a characterisation of denotations that allows us to recover a useful
    semantic equivalance relation and associated compositionality statements.
  \item We employ the eventful semantics as a collecting semantics and derive
    the usage analysis of \Cref{sec:problem} by \emph{calculational design}~\citep{Cousot:21}.
\end{itemize}

\end{comment}

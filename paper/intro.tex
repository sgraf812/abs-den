\section{Introduction}
\label{sec:introduction}

A \emph{static program analysis} infers facts about a program, such
as ``this program is well-typed'', ``this higher-order function is always called
with argument $\Lam{x}{x+1}$'' or ``this program never evaluates $x$''.
In a functional-language setting, such static analyses are
often defined by \emph{structural recursion} on the input term.
To prove the analysis correct with respect to the language semantics,
it is much easier if the semantics is also defined by structural recursion,
as is the case for a traditional \emph{denotational semantics};
then the denotational semantics and the static analysis
``line up'' and the correctness proof is relatively straightforward.
Indeed, one can often regard the analysis as an abstraction, or \emph{abstract interpretation},
of the denotational semantics \cite{Cousot:21}.

Alas, traditional denotational semantics does not
model operational details -- but those details might be the whole point of the analysis.  For example,
we might want to ask ``How often does $\pe$ evaluate its free variable $x$?'';
but a standard denotational semantics simply does not express the concept of ``evaluating a variable''.
So we are typically driven to use an \emph{operational semantics}~\citep{Plotkin:81}, which directly
models operational details like the stack, heap, and sharing, and sees program execution
as a sequence of machine states.  Now we have two un-appealing alternatives:
\begin{itemize}
\item Peform a difficult correctness proof, that links an operational semantics for the language
  with a compositional analysis defined by structural recursion.
\item Re-imagine and re-implement the analysis as an abstraction of the
  reachable states of an operational semantics.
  This is the essence of Abstracting Abstract Machines \cite{aam}, a very
  fruitful approach, but one that requires the entire analysis to cleverly
  reason about heaps.
\end{itemize}
In this paper we explore a new third path: \emph{to enrich a compositional denotational semantics so that
it expresses enough operational detail to support our static analysis}.
We make the following contributions:
\begin{itemize}
\item We use a concrete example (usage analysis) to explain the problems we sketched
  above: the lack of operational detail in denotational semantics, and the structural mismatch
  between the semantics and the analysis (\Cref{sec:problem}).
\item We describe a new \emph{trace-based denotational semantics for lambda calculus},
  that is defined compositionally by structural recursion, but still produces an execution
  trace that we can later elaborate with as much operational detail as we please (\Cref{sec:vanilla}).
  Unlike traditional formulations of denotational semantics, our semantics
  is defined by \emph{guarded recursion}~\citep{gdtt} and thus total as a
  mathematical function, even in the diverging case.
  We prove it adequate \wrt a standard operational semantics.
  \slpj{This is the ``hard proof''??  We do it once for all.}
  \sg{I'm not completely sure what you mean. Compared to later proofs, this
  proof is reasonably simple, but perhaps most important in consequence.}
\item This trace-based semantics encodes the bare minimum of information in traces to be useful.
  In \Cref{sec:eventful} we will see how simple it is to \emph{instrument} our
  semantics to provide richer traces, carrying \emph{events} describing
  small-step transitions.
  The resulting idea of an \emph{eventful} semantics borrows ideas from the
  \emph{maximal trace semantics} of \citet{Cousot:21}.
\item In \Cref{sec:essence} we provide several meta-theoretic results about our
  new class of semantics, such as a partially-ordered heap forcing relation,
  a characterisation of denotations that allows us to recover a useful
  semantic equivalance relation and statements of compositionality.
\item We finally prove usage analysis correct analysis \wrt the
  eventful semantics in \Cref{sec:abstractions}, showing that update
  avoidance~\citep{Gustavsson:98} is an
  improvement~\citep{MoranSands:99} whenever a let binding is evaluated at
  most once.
  The definition of ``evaluated at most once'' is in terms of the
  \emph{semantic usage abstraction} of our eventful semantics, and
  our usage analysis is shown to be an abstraction thereof.
\item
  We believe that our notion of trace-based semantics for lambda calculus
  is the first to model operational detail of call-by-need behavior while
  enjoying a structural definition. Our pattern naturally generalises
  to similar definitions generating call-by-value traces, and we believe
  that its trace-based nature enables unprecedented application of
  \citeauthor{Cousot:21}'s intraprocedural framework in interprocedural
  contexts without the need for a control-flow graph abstraction, a claim
  we elaborate on in \Cref{sec:related-work}, comparing to Related Work.
\end{itemize}


\begin{comment}
To an implementor of a programming language, it is often useful to employ a
\emph{static program analysis} to automatically infer facts about a program such
as ``this program is well-typed'', ``this higher-order function is always called
with argument $\Lam{x}{x+1}$'' or ``this program never evaluates $x$''.

If the implementation language is a functional one, then usually such static
analyses are formulated as a function defined by \emph{structural recursion} on
the input term.
For example, given an application expression $(\pe_1~\pe_2)$,
the property ``$(\pe_1~\pe_2)$ never evaluates its free variable $x$'' can be
conservatively approximated by the property ``$\pe_1$ never evaluates $x$
and $\pe_2$ never evaluates $x$''.

It is quite convenient to formulate a static analysis structurally:
(1) Structural recursion gives an immediate proof of termination and can
    further be exploited in other inductive proofs.
(2)~A structurally-defined function $f$ is often \emph{compositional}, meaning that
    the result of $f\denot{\pe_1~\pe_2}$ is obtained by combining the independent
    calls $f\denot{\pe_1}$ and $f\denot{\pe_2}$. % but not of the structure of either $\pe_1$ or $\pe_2$.
    Compositionality makes it easy for humans to understand and reason about the
    function, as well as allows to abstract away program syntax in the first
    place. \slpj{I don't understand the ``as well as''.  I would say ``This independence
    makes both analysis and proof much simpler.''}


\subsection{Compositional Analyses Abstract Denotational Semantics}

For static analyses, especially more complicated ones, it is good practice to
provide a proof of correctness.
If the correctness statement can be expressed in terms of a \emph{denotational
semantics}~\citep{ScottStrachey:71}, then the recursion structure of analysis
function and semantics function line up nicely.
As a result, the proof of correctness can be conducted by simple induction
over the expression.
Abstract interpretation~\citep{Cousot:21} provides a neat framework to derive
structural static analyses that are proven correct by simple equational
reasoning.

%\subsection{Domain Theory is leaky and too abstract}
%Alas, there are several shortcomings to traditional denotational semantics:
%\begin{itemize}
%  \item Program behaviors are denoted by elements of an algebraic domain.
%        Crucially, such algebraic domains allow embedding of functions
%        operating on algebraic domain elements~\citep{ScottStrachey:71}.
%        However, this embedding is restricted to the subset of
%        \emph{continuous} functions%
%        \footnote{Continuous functions in turn form a superset of all
%        \emph{computable} functions.}.
%
%        As a result, coming up with one's own denotational semantics requires a
%        proof of continuity of all involved functions that is often hand-waved
%        over when the semantics is ``sufficiently standard''.
%        In other cases, continuity is proven for a number of combinators and
%        their composition, and then the semantics is formulated in terms of this
%        meta language of combinators, adversely affecting the presentation of the
%        semantics.
%  \item Traditional domain theory proved non-compositional~\citep{WrightFelleisen:94},
%        in that language features such as exceptions, state and nondeterminism
%        require individual, complicated adjustments where it is far from clear
%        how to combine these adjustments in a way that the combined semantics'
%        functions are still continuous.
%  \item There have been denotational semantics for call-by-name and
%        call-by-value, but none for call-by-need. That is a symptom of the fact
%        that traditional domain theory eschews any account of operational
%        detail, such as evaluation cardinality.
%\end{itemize}
%
%The third point is important in the context of quantitative type
%systems~\citep{Atkey:18} and optimising compilers for call-by-need languages such
%as the Glasgow Haskell Compiler.
%The first two points have been noted by \citet{WrightFelleisen:94}, establishing
%operational semantics as the Gold Standard to this day.
%The past twenty years have proven that the methodology scales to
%big and complex type systems, building on techniques such as
%\emph{step-indexing}~\citep{AppelMcAllester:01,DreyerAhmedBirkedal:11}.

\subsection{Operational Semantics is Detailed and Systematically Scalable}

Alas, the existing denotational semantics for lambda calculus are insufficiently
expressive to model operational detail such as \emph{evaluation cardinality},
\eg, ``How often does $\pe$ evaluate its free variable $x$?'', because they lack
the concept of ``evaluating a variable''.
Thus, a static analysis that cares about evaluation cardinality (we will give
an example in \Cref{sec:problem}) cannot be proven correct \wrt traditional
denotational semantics.
A related issue is that up to this day, there are denotational semantics for
call-by-value and call-by-name but none for call-by-need, because memoisation is
not a concept that is observable in traditional denotational semantics.

Ultimately, traditional denotational semantics fell out of fashion due to
scaling issues we think that are ultimately caused by direct use of algebraic
domain theory. \citet{WrightFelleisen:94} established operational semantics
as the gold standard semantics for correctness proofs to this day.
The past thirty years have scaled the methodology to
big and complex type systems, building on techniques such as
\emph{step-indexing}~\citep{AppelMcAllester:01,DreyerAhmedBirkedal:11}.
However, operational semantics are expressed as transition systems which do not
easily admit a compositional definition as a function of the input expression.
%\footnote{While~\citet{Cousot:02} has derived trace-based fixpoint
%semantics from an arbitrary transition system, the resulting function is not
%structurally-recursive in the expression.}
In giving in to operational semantics, semanticists have chosen predictable
step-wise reasoning over elegant equational reasoning backed by
compositionality.

%In preservation proofs of type systems, step-wise reasoning necessitates
%substitution lemmas to cope with the non-structural recursion during variable
%lookup and beta reduction, an inconvenience that is readily accepted when the
%alternative would be to tame domain theory.

As a result, we are stuck with a large amount of literature and implementations
on \emph{compositional} program analyses such as in \citet{cardinality-ext}
that have to be proven correct \wrt a transition system.
Compared to when a denotational semantics is available, this \emph{structural
mismatch} necessitates the invention of ad-hoc, complicated correctness
relations that must proven to be preserved during transitions.

\subsection{Static Analysis by Abstraction of Operational Semantics}

When there is no semantics that structurally matches our analysis, we could
try to rearrange our analysis.
That is the idea of \emph{Abstracting Abstract Machines}~(AAM)~\citep{aam} which
applies abstract interpretation to the transition semantics.

The resulting static analysis is defined as an iterative fixpoint procedure that
approximates the set of possible states in the transition system that reach a
given sub-expression.
However, such a definition is hard to reconcile with existing battle-proven and
compositional static analyses in compilers such as the Glasgow Haskell
Compiler (GHC)~\citep{MarlowJones:12} due to the structural mismatch.
One could rewrite GHC's analyses using AAM, but that is a path that has seldomly
been taken.
It would be preferable to verify these existing implementations instead.

%It is not so simple in lattice-based compiler analyses, because recursive
%$\mathbf{let}$ bindings invite fixpoint iteration. If you think about
%``well-annotatedness'' of a machine configuration according to an analysis as a
%type system, the corresponding ``preservation lemma'' needs to relate fixpoints
%before and after \emph{every} kind of machine transition!
%This necessitates a dreadful application of fixpoint induction, unfolding
%the whole analysis function body before and after each step and thus culminating
%in beaurocratic nightmare.
%Of course, the definition of ``well-annotatedness'' needs to encompass whole
%machine configurations, including the (mutually recursive, in call-by-need)
%stack and heap, see the proofs in~\citep{cardinality-ext} for a taste. It is
%hard to raise confidence in such a proof without full mechanisation.
%A denotational semantics would arguably simplify the proof, unfortunately one
%allowing to observe evaluation cardinality of call-by-need is lacking to this
%day.

\subsection{Summary of Contributions}

We aim to resolve the aforementioned tensions around operational
detail and structural mismatch via the following contributions:
\begin{itemize}
  \item In \Cref{sec:problem}, we explicate the lack of operational detail in
    denotational semantics and the structural mismatch problem \wrt operational
    semantics by the example of \emph{usage analysis}, a generalisation of
    deadness analysis.
  \item In \Cref{sec:vanilla}, we give a compositional semantics
    for call-by-need lambda calculus that generates an abstraction of
    potentially infinite small-step traces.
    Unlike traditional formulations of denotational semantics, our semantics
    is defined by \emph{guarded recursion}~\citep{gdtt} and thus total as a
    mathematical function.
    Our semantics is distinct from similar ones for call-by-name or
    call-by-value and we prove it adequate \wrt a standard operational semantics.
  \item The semantics in \Cref{sec:vanilla} is one that encodes the bare minimum
    of information in traces to be useful.
    This is comparable to the traditional denotational approach, where
    denotations are either $\bot$ or a value.
    In \Cref{sec:eventful} we will see how simple it is to \emph{instrument} our
    semantics to provide richer traces, carrying \emph{events} describing
    small-step transitions.
    The resulting idea of an \emph{eventful} semantics borrows ideas from the
    \emph{maximal trace semantics} of \citet{Cousot:21}.
  \item
    We believe that our notion of \emph{trace-based semantics for lambda calculus}
    is the first to model operational detail of call-by-need behavior while
    enjoying a structural definition. Our pattern naturally generalises
    to similar definitions generating call-by-value traces and we believe
    that its trace-based nature enables unprecedented application of
    \citeauthor{Cousot:21}'s intraprocedural framework in interprocedural
    contexts without the need for a control-flow graph abstraction.
  \item In \Cref{sec:essence} we record a few meta-theoretic results about our
    new class of semantics, such as a partially-ordered heap forcing relation,
    a characterisation of denotations that allows us to recover a useful
    semantic equivalance relation and associated compositionality statements.
  \item We employ the eventful semantics as a collecting semantics and derive
    the usage analysis of \Cref{sec:problem} by \emph{calculational design}~\citep{Cousot:21}.
\end{itemize}

\end{comment}

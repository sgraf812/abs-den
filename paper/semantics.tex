\section{Semantics}
\label{sec:semantics}

\begin{figure}
\[\begin{array}{c}
 \begin{array}{rrclcl}
  \text{Variables}    & \px, \py & ∈ & \Var        &     & \text{finite} \\
  \text{Labels}       &     \lbl & ∈ & \Labels     &     & \text{finite} \\
  \text{Values}       &      \pv & ∈ & \Val        & ::= & \Lam{\px}{\pe} \\
  \text{Expressions}  &      \pe & ∈ & \Exp        & ::= & \slbl \px \mid \slbl \pv \mid \slbl \pe~\px \mid \slbl \Let{\px}{\pe_1}{\pe_2} \\
  \\[-0.5em]
  \text{States}        & σ   & ∈ & \States        & =      & \Control \times \Environments \times \Heaps \times \Continuations \\
  \text{Controls}      & γ   & ∈ & \Control       & ::=    & \pe \mid (\pv, v) \\
  \text{Environments}  & ρ   & ∈ & \Environments  & =      & \Var \pfun \Addresses \\
  \text{Addresses}     & \pa & ∈ & \Addresses     & \simeq & ℕ \\
  \text{Heaps}         & μ   & ∈ & \Heaps         & =      & \Addresses \pfun \Exp \times \Environments \times \StateD \\
  \text{Continuations} & κ   & ∈ & \Continuations & ::=    & \StopF \mid \ApplyF(\pa) \pushF κ \mid \UpdateF(\pa) \pushF κ \\
  \text{Parameters}    & \multicolumn{5}{l}{d∈D,~v∈V} \\
 \end{array} \\
  \\[-0.5em]
\end{array}\]

\newcolumntype{L}{>{$}l<{$}} % math-mode version of "l" column type
\newcolumntype{R}{>{$}r<{$}} % math-mode version of "r" column type
\newcolumntype{C}{>{$}c<{$}} % math-mode version of "c" column type
\begin{tabular}{LRCLL}
\toprule
\text{Rule} & σ_1 & \smallstep & σ_2 & \text{where} \\
\midrule
\LetT & (\Let{\px}{\pe_1}{\pe_2},ρ,μ,κ) & \smallstep & (\pe_2,ρ',μ[\pa↦(\pe_1,ρ',d_1)], κ) & \pa \not∈ \dom(μ),\ ρ'\! = ρ[\px↦\pa] \\
\AppIT & (\pe~\px,ρ,μ,κ) & \smallstep & (\pe,ρ,μ,\ApplyF(\pa) \pushF κ) & \pa = ρ(\px) \\
\LookupT & (\px, ρ, μ, κ) & \smallstep & (\pe, ρ', μ, \UpdateF(\pa) \pushF κ) & \pa = ρ(\px),\ (\pe,ρ',\wild) = μ(\pa) \\
\ValueT & (\pv, ρ, μ, κ) & \smallstep & ((\pv, v), ρ, μ, κ) & \\
\AppET & ((\Lam{\px}{\pe},\wild),ρ,μ, \ApplyF(\pa) \pushF κ) & \smallstep & (\pe,ρ[\px ↦ \pa],μ,κ) &  \\
\UpdateT & ((\pv,v), ρ, μ, \UpdateF(\pa) \pushF κ) & \smallstep & ((\pv,v), ρ, μ[\pa ↦ (\pv,ρ,d)], κ) & \\
⊥tomrule
\end{tabular}
\caption{Syntax of $Λ$ and Lazy Krivine (LK) transition semantics $\smallstep$}
  \label{fig:lk-syntax}
\end{figure}

% Note [On the role of labels]
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~
% Simon does not like program labels; he'd much rather just talk about
% the expressions those labels are attached to. Yet it is important for
% the simplicity of analyses *not* to identify structurally equivalent
% sub-expressions, example below.
%
% Our solution is to assume that every expression is implicitly labelled
% (uniquely, so). When we use the expression as an index, we implicitly use the
% label as its identity rather than its structure. When labels are explicitly
% required, we can obtain them via the at() function.
%
% When do we *not* want structural equality on expressions? Example:
%
%   \g -> f a + g (f a)
%
% Now imagine that given a call to the lambda with an unknown `g`, we'd like to
% find out which subexpressions are evaluated strictly. We could annotate our
% AST like this
%
%   \g -> ((f^S a^L)^S + (g (f^L a^L)^L)^S)^S
%
% Note how the two different occurrences of `f a` got different annotations.
%
% For obvious utility (who wants to redefine the entire syntax for *every*
% analysis?), we might want to maintain the analysis information *outside* of
% the syntax tree, as a map `Expr -> Strictness`. But doing so would conflate
% the entries for both occurrences of `f a`! So what we rather do is assume
% that every sub-expression of the syntax tree is labelled with a unique token
% l∈Label and then use that to maintain our external map `Label -> Strictness`.
%
% We write ⌊Expr⌋ to denote the set of expressions where labels are "erased",
% meaning that structural equivalent expressions are identified.
% The mapping `Label -> Expr` is well-defined and injective; the mapping
% `Label -> ⌊Expr⌋` is well-defined and often *not* injective.
% Conversely, `at : Expr -> Label` extracts the label of an expression, whereas
% `⌊Expr⌋ -> Label` is not well-defined.
%
% Note that as long as a function is defined by structural recursion over an
% expression, we'll never see two concrete, structurally equivalent expressions,
% so it's OK to omit labels and use the expression we recurse over (and its immediate
% subexpressions captured as meta variables) as if it contained the omitted labels.

\subsection{Labelled Syntax}

\Cref{fig:lk-syntax} defines syntax and semantics of $Λ$: An untyped,
call-by-name lambda calculus with recursive let bindings in the style of
\citep{Launchbury:93} and \citep{Sestoft:97}.
Any (sub-)expression of $Λ$ has a unique \emph{label} (think of it as the AST node's
pointer identity) that we usually omit. For example, a correct labelling of
$f~(g~f)$ would be
\[
  (\slbln{1} (\slbln{2} f)~(\slbln{3}(\slbln{4} g)~(\slbln{5} f))).
\]
Labels are there so that we do not conflate the (otherwise structurally equal)
sub-terms $(\slbln{2} f)$ and $(\slbln{5} f)$ as equivalent. This is an important
distinction for, \eg, control-flow analysis. Since labels introduce excessive
clutter, we will omit them unless they are distinctively important. If anything,
labels make it so that everything ``works as expected''.

\subsection{Transition System}

An operational semantics of $Λ$ is given in terms of a small-step transition
system closest to the lazy Krivine machine \cite{AgerDanvyMidtgaard:04} for
Launchbury's language.
It is worth having a closer look at the workings of our Gold Standard.
The machine's state comes as a quadruple in the style of a CESK machine
\cite{Felleisen:87}, consisting of the usual \emph{control} component
corresponding to the control-flow node $γ$ under evaluation, the
\emph{environment} $ρ$ mapping lexically-scoped variables to an address bound in
the \emph{heap} $μ$ and a \emph{continuation}, or \emph{stack}, $κ$.
The notation $f ∈ A \pfun B$ used in the definition of $ρ$ and $μ$ denotes a
finite map from $A$ to $B$, a partial function where the domain $\dom(f)$ is
finite and $\rng(f)$ denotes its range.
The literal notation $[a_1↦b_1,...,a_n↦b_n]$ denotes a finite map with domain
$\{a_1,...,a_n\}$ that maps $a_i$ to $b_i$. The extended finite map $f[a ↦ b]$
maps $a$ to $b$ and is otherwise equal to $f$.
The initial machine state for a closed expression $\pe$ is given by the
injection function $inj(\pe) = (\pe,[],[],\StopF)$.

The semantics differs from the standard in one way: It is parameterised over
occurrences of $d ∈ D$ and $v ∈ V$ which are meant to be \emph{elaborated} later
on. For now, we can assume the \emph{standard instantiation}, where both
parameters are instantiated to the unit type, $D \triangleq V \triangleq
\mathbb{1} \triangleq \{ () \}$.

The control component $γ$ is either an expression under evaluation $\pe$ or a
value pairing $(\pv,v)$ (remember, $v = ()$).
When the control of a state $σ$ is an expression $\pe$, we call $σ$ an
\emph{evaluation} state and say that $\pe$ drives evaluation, whereas when the
control is $(\pv, v)$ we call it a \emph{return} state in which the continuation
$κ$ drives evaluation.
The entries in the heap are \emph{closures} of the form $(e,ρ,d)$, where the
environment $ρ$ closes over the expression $e$ ($d = ()$).
Finally, the continuation $κ$ lists actions to be taken when the control reaches
a value, such as applying the result to an argument address or updating a heap
entry with its value.

Heap entries are introduced via $\LetT$ transitions under a \emph{fresh} address
$\pa \not∈ \dom(μ)$ that we call an \emph{activation} of the let-bound variable
$\px$. The lexical activation of every variable in scope is maintained
in $ρ$. The $\AppIT$ rule pushes an \emph{application frame} with the address of
the argument variable onto the stack, while the rule $\LookupT$ pushes an
\emph{update frame} with the address of the variable the heap entry of which is
accessed. When a return state is reached, the original heap entry is overwritten
with the value in the control.
An evaluation state transitions to a return state via rule $\ValueT$ when the
control expression is a value.%
\footnote{Operational semantics commonly don't explicate $\ValueT$
transitions, but the original formulation of the CESK machine
did~\cite{Felleisen:87}, as does the lazy Krivine
machine of \citep{AgerDanvyMidtgaard:04}.}

We bake into $σ$ the invariant of \emph{well-addressedness}: Any address $\pa$
occuring in $ρ$, $κ$ or the range of $μ$ must be an element of $\dom(μ)$.
It is easy to see that the transition system maintains this invariant and that
it is still possible to observe scoping errors which are thus confined to
lookup in $ρ$.

Note that whatever parameter sets $D,V$ we pick, the value of parameters $v$ and
$d$ never affect other state components or whether or not a transition can fire.
The transition system is deterministic iff the choice of the particular
$d$ or $v$ in rules $\LetT$, $\ValueT$ and $\UpdateT$ is. That is the case for
the proposed standard instantiation of $D$ and $V$ to the unit type.
Moreover, any deterministic instantiation yields a transition system that is
bisimilar to the standard one.

Let us conclude with an example trace in this transition system, evaluating
$\pe \triangleq \Let{i}{\Lam{x}{x}}{i~i}$ to completion:
\[\begin{array}{c}
  \arraycolsep2pt
  \begin{array}{clclclcl}
             & (\pe, [], [], \StopF)            & \smallstep & (i~i, ρ_1, μ, \StopF)
                & \smallstep & (i, ρ_1, μ, κ_1) & \smallstep & (\Lam{x}{x}, ρ_1, μ, κ_2)
                \\
  \smallstep & ((\Lam{x}{x}, ()), ρ_1, μ, κ_2)  & \smallstep & ((\Lam{x}{x}, ()), ρ_1, μ, κ_1)
                & \smallstep & (x, ρ_2, μ, \StopF) & \smallstep & (\Lam{x}{x}, ρ_1, μ, κ_3)
                \\
  \smallstep & ((\Lam{x}{x}, ()), ρ_1, μ, κ_3)  & \smallstep & ((\Lam{x}{x}, ()), ρ_1, μ, \StopF) \\
  \end{array} \\
  \\[-0.5em]
  \quad \text{where} \quad \begin{array}{ll}
  ρ_1 = [i ↦ \pa_1] & κ_1 = \ApplyF(\pa_1) \pushF \StopF \\
  ρ_2 = [i ↦ \pa_1, x ↦ \pa_1] & κ_2 = \UpdateF(\pa_1) \pushF κ_1 \\
  μ = [\pa_1 ↦ (\Lam{x}{x},ρ_1,())] & κ_3 = \UpdateF(\pa_1) \pushF \StopF \\
  \end{array} \\
\end{array}\]

\begin{figure}
\[\begin{array}{c}
 \begin{array}{rrclcl}
  \text{LK traces}                    & π      & ∈          & \STraces & ::=_{\gfp} & σ\trend \mid σ; π \\
 \end{array} \qquad
 \begin{array}{c}
  \ruleform{ \validtrace{π} } \\
  \\[-0.5em]
  \mprset{fraction={===}}
  \inferrule*
    {\quad}
    {\validtrace{σ\trend}}
  \quad
  \inferrule*
    {σ \smallstep src_\States(π) \quad \validtrace{π}}
    {\validtrace{σ; π}} \\
 \end{array} \\
 \\
 \begin{array}{rcl}
  \multicolumn{3}{c}{ \ruleform{ src_\States(π) = σ \qquad tgt_\States(π) = σ } } \\
  \\[-0.5em]
  src_\States(σ\trend)    & = & σ \\
  src_\States(σ; π) & = & σ \\
  \\[-0.5em]
  tgt_\States(π)    & = & \begin{cases}
    undefined & \text{if $π$ infinite} \\
    σ         & \text{if $π = ...; σ \trend$}
  \end{cases} \\
 \end{array} \quad
 \begin{array}{c}
  \ruleform{ π_1 \sconcat π_2 = π_3 } \\
  \\[-0.5em]
  π_1 \sconcat π_2 = \begin{cases}
    σ; (π_1' \sconcat π_2) & \text{if $π_1 = σ; π_1'$} \\
    π_2                    & \text{if $π_1 = σ\trend$ and $src_\States(π_2) = σ$} \\
    undefined              & \text{if $π_1 = σ\trend$ and $src_\States(π_2) \not= σ$} \\
  \end{cases} \\
 \end{array} \\
 \\
\end{array}\]
\caption{LK traces}
  \label{fig:lk-traces}
\end{figure}

\subsection{Characterisation of LK Traces}

A transition system is characterised precisely by the set of \emph{traces} it
generates.
A trace in $(\smallstep)$ is a non-empty and potentially infinite sequence of
states $(σ_i)_{i∈\overline{n}}$ (where $\overline{n} = \{ m \mid m ≤ n \}, n∈ℕ_+
∪ \{ω\}$), such that $σ_i \smallstep σ_{i+1}$ for $i,(i+1)∈\overline{n}$.
\Cref{fig:lk-traces} gives a (coinductive) definition of such a trace type
$\STraces$ which is to be understood as the greatest fixed-point of the
functional $F(X) = \States + (\States \times X)$.
This is isomorphic to the definition as a sequence, hence we will freely
switch between notations for $π$, in particular to index its states.

The well-formedness of a trace $π$ wrt. to $(\smallstep)$ is asserted by
$\validtrace{π}$, a predicate with a coinductive definition as indicated by use
of a double-line for inference rules.
When a trace is finite, we often re-associate the right-associative structure
of $;$ to the left and decompose it as $π'; σ\trend$, where $σ$ is the last,
or \emph{target}, state of $π$, and $π'$ is its initial component without the
target state. When $π'$ is omitted, such as in $...; σ\trend$, we even allow
the initial component to be empty, in which case $π = σ\trend$.
The \emph{source} state $src_\States(π)$ can be computed for finite and infinite
traces, while the target state of a trace $tgt_\States(π)$ is only defined for
finite traces.
The expression $π_1 \sconcat π_2$ denotes the concatenation of two traces; it is
simply $π_1$ when $π_1$ is infinite and undefined when the target state of $π_1$
does not coincide with the source state of $π_2$.

An important kind of trace is one that never leaves the evaluation context of its
source state:

\begin{definition}[Convex and balanced traces]
  An LK trace $π = (e_1,ρ_1,μ_1,κ_1); ... (e_i,ρ_i,μ_i,κ_i); ... $ is
  \emph{convex} if $\validtrace{π}$ and every intermediate continuation $κ_i$
  extends $κ_1$ (so $κ_i = κ_1$ or $κ_i = ... \pushF κ_1$).

  Furthermore, a convex trace $π$ is \emph{balanced} \cite{Sestoft:97} if the
  target state is a return state with continuation $κ_1$.
\end{definition}

\begin{example}
  Let $ρ=[x↦\pa_1],μ=[\pa_1↦(\Lam{y}{y},[],())]$ and $κ$ an arbitrary
  continuation. The trace
  \[
     (x, ρ, μ, κ) \smallstep (\Lam{y}{y}, ρ, μ, \UpdateF(\pa_1) \pushF κ) \smallstep ((\Lam{y}{y},()), ρ, μ, \UpdateF(\pa_1) \pushF κ) \smallstep ((\Lam{y}{y},()), ρ, μ, κ)
  \]
  is convex and balanced. Its prefixes are convex but not balanced. The suffix
  \[
     ((\Lam{y}{y},()), ρ, μ, \UpdateF(\pa_1) \pushF κ) \smallstep ((\Lam{y}{y},()), ρ, μ, κ)
  \]
  is neither convex nor balanced.
\end{example}


A balanced trace starting at a focus expression $\pe$ and ending with $(\pv,v)$
loosely corresponds to a derivation of $\pe \Downarrow \pv$ in a natural
big-step semantics~\cite{Sestoft:97} or a non-$⊥$ result in a denotational
semantics.

It is when a derivation in a natural semantics does not exist that a small-step
semantics shows finesse, in that it differentiates two different kinds of
\emph{maximally convex} (or, just \emph{maximal}) traces:

\begin{definition}[Maximal trace]
  An LK trace $π$ is \emph{maximal} if and only if it is convex and there is no $σ$ such
  that $π; σ \trend$ is convex.
  We notate maximal traces as $\maxtrace{π}$.
\end{definition}

We call infinite and convex traces \emph{diverging}.
A maximally finite, but unbalanced trace $π$ is called \emph{stuck}.
\footnote{Note that usually stuckness is associated with a state of a transition
system rather than a trace. That is not possible in our framework; depending on
the trace of which the state is a target, it might either be balanced or stuck.}

\begin{example}[Stuck and diverging traces]
The following trace is stuck because $x$ is not in scope:
\[
  (x~y,[y↦\pa], [\pa↦...], κ); (x,[y↦\pa], [\pa↦...], \ApplyF(\pa) \pushF κ)\trend
\]
An example for a diverging trace where $ρ=[x↦\pa_1]$ and $μ=[\pa_1↦(x,ρ,())]$ is
\[
  (\Let{x}{x}{x}, [], [], κ) \smallstep (x, ρ, μ, κ) \smallstep (x, ρ, μ, \UpdateF(\pa_1) \pushF κ) \smallstep ...
\]
\end{example}

A maximal trace that is not balanced either diverges or is stuck:

\begin{lemma}[Characterisation of maximal traces]
  A trace $π$ is maximal if and only if it is balanced, diverging or stuck.
\end{lemma}
\begin{proof}
  $\Rightarrow$: Let $π$ be maximal.
  If $π$ is infinite, then it is diverging due to convexity, and if $π$ is
  stuck, the goal follows immediately. So we assume that $π$ is maximal, finite
  and not stuck, so it must be balanced by the definition of stuckness.

  $\Leftarrow$: Both balanced and stuck traces are maximal.
  A diverging trace $π$ is infinite and convex.
  Indeed $π$ is maximal, because the expression $π; σ\trend$ is undefined for
  infinite $π$.
\end{proof}

\begin{figure}
\[\begin{array}{c}
 \begin{array}{rrclcl}
  \text{Domain of LK trace semantics} & D      & \triangleq & \StateD  & = & \States \to \STraces \\
  \text{Semantic values}              & V      & \triangleq & \StateV & ::= & \FunV(f ∈ \Addresses \to \StateD) \\
 \end{array} \\
\end{array}\]
\caption{Elaboration for LK trace semantics}
  \label{fig:lk-domain}
\end{figure}

\subsection{Specification}

Our goal is to define a function $\semst{\pe}$ that produces maximal traces that
start out having $\pe$ in control. But there are as many traces as there
are states that have $\pe$ in control, each corresponding to the different
evaluation contexts in which $\pe$ might occur. Hence it makes sense to tell
$\semst{\pe}$ the particular source state $σ$ for which we wish to generate a
trace, leading to functionality $\semst{\pe} ∈ \States → \STraces$.

We will call $\semst{\pe}$ the \emph{denotation} of $\pe$ (a term that is
justified when we flesh out its properties relative to $\pe$) and abbreviate
the set of denotations to $\StateD$, as in \Cref{fig:lk-domain}.
$\StateD$ is used as the instantiation for parameter $D$ for our elaborated
semantics.
Likewise, we instantiate $V$ to a semantic representation of values
$\StateV$.
The keen reader may note immediately that $\StateD$ is not well-defined because
of a recursive occurrence in negative position (through $\States \to \Heaps \to
\StateD$); we discuss the necessary domain theory in \Cref{sec:domain-theory}
and focus on its application here.

So far we have collected the following two requirements for $π = \semst{\pe}(σ)$:
The source state of $π$ should be $σ$ again and $π$ should be a maximal trace.
That is enough to begin ``calculating'' an elaborated trace with $\semst{\wild}$:
\[
  \semst{\Lam{x}{x}}(\Lam{x}{x}, ρ, μ, κ) = (\Lam{x}{x}, ρ, μ, κ); ((\Lam{x}{x},\FunV(f)), ρ, μ, κ)\trend
\]
This undoubtedly is a maximal trace, but what is $f$ supposed to do?
Like function values in denotational semantics, $f$ corresponds to the
\emph{reflection} of the lambda expression into the meta language (of Scott
domains), to model beta reduction in a way that is compatible with a structural
definition.
The $f$ here serves a similar purpose, guiding the $\AppET$ rule.
The following required equation captures this intuition:
\[
  f(\pa)((\Lam{x}{x},\FunV(f)), ρ, μ, \ApplyF(\pa) \pushF κ) = ((\Lam{x}{x},\FunV(f)), ρ, μ, \ApplyF(\pa) \pushF κ); \semst{x}(x, ρ[x ↦ \pa], μ, κ)
\]
This equation only specifies $f$'s behavior when the argument address on the
stack matches that of $f$'s first argument.
The free variables $\pa,ρ,μ,κ$ of this equation are intended to be universally
quantified; that is, it matches all states where the lambda expression is
applied.
Note also that $f$ closes over the semantics of the sub-program $\semst{x}$;
this is what ultimately enables a structural definition because it turns
``code'' for the recursive invocation into ``data''.

In general, semantic values pop exactly one frame from the stack and then produce
another balanced trace of the redex. This statement extends to other kinds of
values such as data constructors, as we shall see in \cref{ssec:adts}.

So not only do we require $\semst{\wild}$ to return maximal traces, we also pose
requirements on how these traces are elaborated. We have seen the requirement
on function values, and of course there is a similar requirement on the $d$
stored in a heap entry $(\pe, ρ, d)$: We require that $d = \semst{\pe}$.

The elaboration requirements can be summarised by the following predicate on
states and extends in the usual way to traces:

\begin{definition}[Well-elaborated]
  We say that a state $σ$ is \emph{well-elaborated} if
  \begin{itemize}
    \item For every heap entry $(\pe, ρ, d) ∈ \rng(μ)$, where $μ$ is the heap of
          $σ$, we have $d=\semst{\pe}$.
    \item If $σ$ is a return state with control $(\Lam{\px}{\pe}, \FunV(f))$,
          then for any state $σ'$ of the form \\
          $((\Lam{\px}{\pe}, \FunV(f)), ρ, μ, \ApplyF(\pa) \pushF κ)$,
          we have
          $f(\pa)(σ') = σ'; \semst{\pe}(\pe, ρ[\px ↦ \pa], μ, κ)$
          and $f(\pa)(σ') = σ'\trend$ everywhere else.
  \end{itemize}
  We notate well-elaborated states as $\elabstate{σ}$ and well-elaborated traces
  as $\elabtrace{π}$ (defined coinductively over the states in $π$).
\end{definition}

Note that the initial configurations $inj(\pe)$ are well-elaborated, because
they are never return states and their heap is empty.
Whenever $\semst{\wild}$ is given a well-elaborated state, the trace it produces
should be well-elaborated.

We conclude with the following specification:

\begin{definition}[Specification of $\semst{\wild}$]
\label{defn:semst-spec}
Let $σ$ be a well-elaborated state and $\pe$ its control expression. Then
\begin{itemize}
  \item[(S1)] $σ$ is the source state of the trace $\semst{\pe}(σ)$.
  \item[(S2)] $\maxtrace{\semst{\pe}(σ)}$
  \item[(S3)] $\elabtrace{\semst{\pe}(σ)}$
\end{itemize}
\end{definition}

\begin{figure}
\[\begin{array}{c}
 \begin{array}{rcl}
  \multicolumn{3}{c}{ \ruleform{ \semst{\wild} \colon \Exp → \StateD } } \\
  \\[-0.5em]
  (d_1 \sfcomp d_2)(σ) & = & d_1(σ) \sconcat d_2(tgt_\States(d_1(σ))) \\
  \\[-0.5em]
  step(f)(σ) & = & \begin{cases}
    σ; f(σ)     & \text{if $f(σ)$ is defined} \\
    σ\trend & \text{otherwise} \\
  \end{cases} \\
  \\[-0.5em]
  val(\pv,v)(\pv,ρ,μ,κ) & = & ((\pv,v),ρ,μ,κ) \trend \\
  \\[-0.5em]
  look(\px)(\px,ρ,μ,κ) & = &
    \begin{letarray}
      \text{let} & \pa = ρ(\px) \\
                 & (\pe,ρ',d) = μ(\pa) \\
      \text{in}  & d(\pe,ρ',μ,\UpdateF(\pa) \pushF κ) \\
    \end{letarray} \\
  \\[-0.5em]
  upd((\pv,v),ρ,μ,\UpdateF(\pa) \pushF κ) & = & ((\pv,v),ρ,μ[\pa ↦ (\pv,ρ,step(val(\pv,v)))], κ)\trend \\
  \\[-0.5em]
  app_1(\pe~\px)(\pe~\px,ρ,μ,κ) & = & (\pe,ρ,μ,\ApplyF(ρ(\px)) \pushF κ)\trend \\
  \\[-0.5em]
  app_2(\Lam{\px}{\pe},\pa)((\Lam{\px}{\pe},\FunV(\wild)),ρ,μ, \ApplyF(\pa) \pushF κ) & = & (\pe,ρ[\px ↦ \pa],μ,κ) \trend \\
  \\[-0.5em]
  let(d_1)(\Let{\px}{\pe_1}{\pe_2},ρ,μ,κ) & = &
    \begin{letarray}
      \text{let} & ρ' = ρ[\px ↦ \pa] \quad \text{where $\pa \not∈ \dom(μ)$} \\
      \text{in}  & (\pe_2,ρ',μ[\pa ↦ (\pe_1, ρ', d_1)],κ)\trend \\
    \end{letarray} \\
  \\[-0.5em]
  apply(σ) & = & \begin{cases}
    f(\pa)(σ) & \text{if $σ=((\wild,\FunV(f)),\wild,\wild,\ApplyF(\pa) \pushF \wild)$} \\
    σ \trend & \text{otherwise} \\
  \end{cases} \\
  \\[-0.5em]
  % We need this case, otherwise we'd continue e.g.
  %   S[x]((sv,v),ρ,μ,upd(a).κ) = ((sv,v),ρ,μ,upd(a).κ); ((sv,v),ρ,μ[a...],κ)
  % Because of how the composition operator works.
  \semst{\pe}(σ) & = & undefined \text{ if the control of $σ$ is not $\pe$} \\
  \\[-0.5em]
  \semst{\px} & = & step(look(\px)) \sfcomp step(upd) \\
  \\[-0.5em]
  \semst{\Lam{\px}{\pe}} & = & \begin{letarray}
    \text{let} & f = \pa \mapsto step(app_2(\Lam{\px}{\pe},\pa)) \sfcomp \semst{\pe} \\
    \text{in}  & step(val(\Lam{\px}{\pe},\FunV(f))) \\
  \end{letarray} \\
  \\[-0.5em]
  \semst{\pe~\px} & = & step(app_1(\pe~\px)) \sfcomp \semst{\pe} \sfcomp apply \\
  \\[-0.5em]
  \semst{\Let{\px}{\pe_1}{\pe_2}} & = & step(let(\semst{\pe_1})) \sfcomp \semst{\pe_2} \\
 \end{array} \\
\end{array}\]
\caption{Structural call-by-need stateful trace semantics $\semst{-}$}
  \label{fig:semst}
\end{figure}

\subsection{Definition}

\Cref{fig:semst} defines $\semst{\wild}$.
Let us understand the function by way of evaluating the example program from
earlier, $\pe \triangleq \Let{i}{\Lam{x}{x}}{i~i}$:
\[\begin{array}{l}
  \newcommand{\myleftbrace}[4]{\draw[mymatrixbrace] (m-#2-#1.west) -- node[right=2pt] {#4} (m-#3-#1.west);}
  \begin{tikzpicture}[mymatrixenv]
      \matrix [mymatrix] (m)
      {
        1  & (\pe, [], [], \StopF); & \hspace{3.5em} & \hspace{3.5em} & \hspace{3.5em} & \hspace{4.5em} & \hspace{7.5em} \\
        2  & (i~i, ρ_1, μ, \StopF); & & & & & \\
        3  & (i, ρ_1, μ, κ_1); & & & & & \\
        4  & (\Lam{x}{x}, ρ_1, μ, κ_2); & & & & & \\
        5  & ((\Lam{x}{x}, \FunV(f)), ρ_1, μ, κ_2); & & & & & \\
        6  & ((\Lam{x}{x}, \FunV(f)), ρ_1, μ, κ_1); & & & & & \\
        7  & (x, ρ_2, μ, \StopF); & & & & & \\
        8  & (\Lam{x}{x}, ρ_1, μ, κ_3); & & & & & \\
        9  & ((\Lam{x}{x}, \FunV(f)), ρ_1, μ, κ_3); & & & & & \\
        10 & ((\Lam{x}{x}, \FunV(f)), ρ_1, μ, \StopF) \trend & & & & & \\
      };
      % Braces, using the node name prev as the state for the previous east
      % anchor. Only the east anchor is relevant
      \foreach \i in {1,...,\the\pgfmatrixcurrentrow}
        \draw[dotted] (m.east|-m-\i-\the\pgfmatrixcurrentcolumn.east) -- (m-\i-2);
      \myleftbrace{3}{1}{10}{$\semst{\pe}$}
      \myleftbrace{4}{1}{2}{$let(\semst{\Lam{x}{x}})$}
      \myleftbrace{4}{2}{10}{$\semst{i~i}$}
      \myleftbrace{5}{2}{3}{$app_1(i~i)$}
      \myleftbrace{5}{3}{6}{$\semst{i}$}
      \myleftbrace{5}{6}{7}{$app_2(\Lam{x}{x},\pa_1)$}
      \myleftbrace{5}{7}{10}{$\semst{x}$}
      \myleftbrace{6}{3}{4}{$look(i)$}
      \myleftbrace{6}{4}{5}{$\semst{\Lam{x}{x}}$}
      \myleftbrace{6}{5}{6}{$upd$}
      \myleftbrace{6}{7}{8}{$look(x)$}
      \myleftbrace{6}{8}{9}{$\semst{\Lam{x}{x}}$}
      \myleftbrace{6}{9}{10}{$upd$}
      \myleftbrace{7}{4}{5}{$ret(\Lam{x}{x},\FunV(f))$}
      \myleftbrace{7}{8}{9}{$ret(\Lam{x}{x},\FunV(f))$}
  \end{tikzpicture} \\
  \quad \text{where} \quad \begin{array}{ll}
  ρ_1 = [i ↦ \pa_1] & κ_1 = \ApplyF(\pa_1) \pushF \StopF \\
  ρ_2 = [i ↦ \pa_1, x ↦ \pa_1] & κ_2 = \UpdateF(\pa_1) \pushF κ_1 \\
  μ = [\pa_1 ↦ (\Lam{x}{x},ρ_1,\semst{\Lam{x}{x}})] & κ_3 = \UpdateF(\pa_1) \pushF \StopF \\
  f = \pa \mapsto step(app_2(\Lam{\px}{\px},\pa)) \sfcomp \semst{\px}
  \end{array}
\end{array}\]
The annotations to the right of the trace can be understood as denoting the
``call graph'' of the $\semst{\pe}$, with the primitive transition $step$s such
as $let_1$, $app_1$, $look$ \etc as leaves, each elaborating the application of
the corresponding transition rule $\LetT$, $\AppIT$, $\LookupT$, and so on, in
the transition semantics with a matching denotation where necessary.

Evaluation begins by elaborating the $\LetT$ transition from state 1 to state 2.
Indeed, the $d$ in the heap is elaborated to $\semst{\Lam{x}{x}}$, as (S3) of
\Cref{defn:semst-spec} requires.

The auxiliary $step$ function and the forward composition operator $\sfcomp$
have been inlined.
One can find that $\sfcomp$ is quite well-behaved: It forms a monoid with
$λσ.σ\trend$ as its neutral element.

Evaluation of the let binding recurses into $\semst{i~i}$ on state 2,
generating an $\AppIT$ transition to state 3, on which $\semst{i}$ is run.
Note that the final state 6 of the call to $\semst{i}$ will later be fed
(via $\sfcomp$) into the auxiliary function $apply$. This scheme is quite
common: Continuation items of the transition semantics (``data'') are reflected
into the call stack of the trace semantics (``code'').

$\semst{i}$ guides the trace from state 3 to state 6, during which we
observe a heap update for the first time: Not only does $look$ look up
the binding $\Lam{x}{x}$ of $i$ in the heap and push an update frame,
it also hands over control to the $d = \semst{\Lam{x}{x}}$ stored alongside it.
(And we make a meta-level note to pick up with $upd$ when $d$ has finished.)
Crucially, that happens without $\semst{\Lam{x}{x}}$ syntactically occurring in
the definition of $\semst{i}$, thus the definition remains structural.

The value transition governed by $\semst{\Lam{x}{x}}$ from state 4 to 5 is
familiar from our earlier example.
$\semst{\Lam{x}{x}}$ finishes in state 5, where $upd$ picks up to guide the
$\UpdateT$ transition. To restore invariant (S3), it also updates the $d$ stored
in the heap so that its semantics matches that of the updated value; take note
of the similarity to the lambda clause in the definition of $\semst{\wild}$.
Since $\Lam{x}{x}$ was a value to begin with, there is no observable change to
the heap.

After $\semst{i}$ concludes in state 6, the $apply$ function from $\semst{i~i}$
picks up. Since $apply$ is defined on state 6, it reduces to $f(\pa_1)$%
\footnote{We omitted $apply$ and $f(\pa_1)$ from the call graph for space reasons, but
their activation spans from state 7 to the final state 10.}.
$f(\pa_1)$ will perform an $\AppET$ transition to state 7, binding $x$ to $i$'s
address $\pa_1$ and finally entering the lambda body $\semst{x}$. Since $x$ is
an alias for $i$, steps 7 to 10 just repeat the same same heap update sequence
we observed in steps 3 to 6.

It is useful to review another example involving an observable heap update.
The following trace begins right before the heap update occurs in
$\Let{i}{(\Lam{y}{\Lam{x}{x}})~i}{i~i}$, that is, after reaching the value
in $\semst{(\Lam{y}{\Lam{x}{x}})~i}$:
\[\begin{array}{l}
  \newcommand{\myleftbrace}[4]{\draw[mymatrixbrace] (m-#2-#1.west) -- node[right=2pt] {#4} (m-#3-#1.west);}
  \begin{tikzpicture}[baseline={-0.5ex},mymatrixenv]
      \matrix [mymatrix] (m)
      {
        1  & ((\Lam{x}{x},\FunV(f)), ρ_2, μ_1, κ_2); & \hspace{3.5em} & \hspace{3.5em} & \hspace{4.5em} & \hspace{7.5em} \\
        2  & ((\Lam{x}{x},\FunV(f)), ρ_2, μ_2, κ_1); & & & & \\
        3  & (x, ρ_3, μ, \StopF); & & & & \\
        4  & (\Lam{x}{x}, ρ_2, μ, κ_3); & & & & \\
        5  & ((\Lam{x}{x}, \FunV(f)), ρ_2, μ, κ_3); & & & & \\
        6  & ((\Lam{x}{x}, \FunV(f)), ρ_1, μ, \StopF); & & & & \\
      };
      % Braces, using the node name prev as the state for the previous east
      % anchor. Only the east anchor is relevant
      \foreach \i in {1,...,\the\pgfmatrixcurrentrow}
        \draw[dotted] (m.east|-m-\i-\the\pgfmatrixcurrentcolumn.east) -- (m-\i-2);
      \myleftbrace{3}{1}{6}{$\semst{i~i}$}
      \myleftbrace{4}{1}{2}{$\semst{i}$}
      \myleftbrace{4}{2}{3}{$app_2(\Lam{x}{x},\pa_1)$}
      \myleftbrace{4}{3}{6}{$\semst{x}$}
      \myleftbrace{5}{1}{2}{$upd$}
      \myleftbrace{5}{3}{4}{$look(x)$}
      \myleftbrace{5}{4}{5}{$\semst{\Lam{x}{x}}$}
      \myleftbrace{5}{5}{6}{$upd$}
      \myleftbrace{6}{4}{5}{$ret(\Lam{x}{x},\FunV(f))$}
  \end{tikzpicture} \\
  \quad \text{where} \quad \begin{array}{ll}
  ρ_1 = [i ↦ \pa_1] & κ_1 = \ApplyF(\pa_1) \pushF \StopF \\
  ρ_2 = [i ↦ \pa_1, y ↦ \pa_1] & κ_2 = \UpdateF(\pa_1) \pushF κ_1 \\
  ρ_3 = [i ↦ \pa_1, y ↦ \pa_1, x ↦ \pa_1] & κ_3 = \UpdateF(\pa_1) \pushF \StopF \\
  μ_1 = [\pa_1 ↦ ((\Lam{y}{\Lam{x}{x}})~i,ρ_1,\semst{(\Lam{y}{\Lam{x}{x}})~i})] & f = \pa \mapsto step(app_2(\Lam{\px}{\px},\pa)) \sfcomp \semst{\px} \\
  μ_2 = [\pa_1 ↦ (\Lam{x}{x},ρ_2,\semst{\Lam{x}{x}})] &  \\
  \end{array} \\
\end{array}\]
Note that both the denotation in the heap \emph{and} the environment are updated
in state 2, and that the new denotation is immediately visible on the next heap
lookup in state 3, so that $\semst{\Lam{x}{x}}$ takes control rather than
$\semst{(\Lam{y}{\Lam{x}{x}})~i}$, just as the transition system requires.

Note that $\semst{\pe}$ continues \emph{only} those states that have $\pe$ as
control and is undefined on any other expression of the program.
So even if $x~x$ and $y~y$ both structurally are well-scoped application
expressions, $\semst{x~x}(y~y,ρ,μ,κ)$ is undefined.
In other words, if $(\pe_i)_i$ enumerates the (labelled) sub-expressions of a
program $\pe$, then for every state $σ$ there is exactly one function
$\semst{\pe_i}$ that is defined on $σ$.
Take note that a correct labelling is crucial for this point: In an expression
like $f~(g~f)$ the two, structurally equal occurrences of $f$ are distinct
sub-expressions which would otherwise have the same image under $\semst{\wild}$.

Perhaps interestingly, the definition of the $\LetT$ case requires no explicit
fixpoint, although the semantics of let allows recursion. That is because the
action of the RHS $\semst{\pe_1}$ (``code'') is reified (into ``data'') and put
in the heap, to be reflected back into code on variable lookup.
Next we will prove that this process is actually well-defined and show that
$\semst{\pe}$ produces total values everywhere it is defined%
\footnote{We do not consider the partial value $⊥$ to be undefined, as we
clarify in the next section.}.

\subsection{Domain Theory}

\label{sec:domain-theory}

\subsubsection{Domain construction}

Recall that $\StateD$ in \Cref{fig:lk-syntax} is not a well-formed inductive
definition without further clarification: It recurses in negative position via
$\States \to \Heaps \to \StateD$ and similarly through $\StateV$.
Hence we must understand $\StateD$ as a Scott domain and its ``definition'' as
a \emph{domain equation}, of which $\StateD$ is the least solution.

For that to carry meaning, we interpret all occuring function types, such as
$\States \to \STraces$, as continuous between their hypothetical source and
target domains.
This applies in the following way to finite maps:
$f ∈ A \pfun B$ (for flat domain $A$ and arbitrary domain $B$) is the
sub-domain of $f ∈ A \to (\{\notfound\} + B)$ of strict functions (so $f(⊥) =
⊥$) where the map's domain $\dom(f) ⊆ A$ is finite. Whenever $a \not∈ \dom(f)
\setminus ⊥$, we have $f(a) = \notfound$.

Recall that that an element $d$ of some domain $D$ is a \emph{total} element iff
there exists no $d' ∈ D$ such that $d ⊏ d'$. Otherwise if such a $d'$ exists,
$d$ is \emph{partial}.

Remarkably, we \emph{do not} conflate $⊥ ∈ A \pfun B$ with $[] ∈ A \pfun B$, the
finite map with $\dom(f) = \varnothing$.
The former is a partial element of the domain and assigns $⊥_B$ everywhere;
the latter is totally defined as an element of the domain and assigns
$\notfound$ everywhere.
As a consequence, $[a ↦ b] \triangleq [][a ↦ b] \not\sqsupseteq []$ since the
total elements of the domain are discrete.

The other domain constructors for products $\times$, sums $+$, fixpoints and the
derived inductive EBNF-style syntax definitions such as the potentially infinite
$\STraces$ (indicated by the $\gfp$ subscript) or finite $\Continuations$ are
standard, see for example \cite{Cartwright:16}. The domains on $\Addresses$ and
$\Var$ are the flat ones; the one on $\Exp$ is the flat one on labels.

This proves that $\StateD$ is well-defined as a domain.

\subsubsection{Continuity}

It is easy to verify (by simple type checking) that $\semst{\wild}$
is indeed a function from elements of $\States$ to something close to the
elements of $\STraces$ (ignoring embedding of recursive calls for a moment).
The missing ingredient to show that $\semst{\wild} ∈ \StateD$ is well-defined is
\emph{continuity}.

As given, the definition of $\semst{\wild}$ does not account for partial
elements at all, although it does a sufficient job provided the elements it
manipulates are total. It quite clearly is computable, so it should have a
continuous extension to partial elements; such was the entire point of
\citep{ScottStrachey:71} and we don't need to repeat it here. Now, we might
spend much time and space here clarifying boring technical details of strictness
such as ``Is $step(look)(\wild,\wild,\wild,\ApplyF(\pa) \pushF ⊥) = ⊥$?'' and
``What is the continuous elaboration of $tgt_\States$?'' that are implicit in a
code listing, only to finally conclude that the clarified formulation is indeed
continuous.

We decide to give the code listing instead: In the Appendix you can find 2 pages
of a model implementation in SML of New Jersey serving as a ``continuity
specification'' of $\semst{\wild}$ that we will treat as its ``ground truth''.
We chose SML because it has a standardised small-step operational semantics that
gives rise to an adequate denotational semantics, the core of which can be found
in \cite{Milner:78}.

On the spectrum between viable continuous extensions of $\semst{\wild}$, the
specification in ML is quite on the strict end. In fact, it introduces a lazy
field in one key position: The tail $π$ of the cons form $σ; π$ of $\STraces$.
That is hardly surprising given the coinductive nature of $π$.

We find it ironic that in order to specify a semantics of a programming
language it is easier to reflect a code listing in a sufficiently
well-defined programming language (fittingly called ``Meta Language'') into a
continuous function than to find a language-agnostic, mathematical formulation
that we laboriously have to prove continuous.

Note that the considerably larger \emph{artifact} associated to this work is
implemented in Haskell because of familiarity to the authors and comfortable
ecosystem, serving as an example for a viable extension of $\semst{\wild}$
erring more towards the non-strict end. Still, it makes abundant use of
strictness annotations for good measure, debuggability and performance.
This demonstrates the semantic leeway a denotational formulation such as
$\semst{\wild}$ has while still being continuous and enjoying the termination
properties implied by our specification.

\subsection{Conformance}

Having a good grasp of the definition of $\semst{\wild}$ now, let us show that
$\semst{\wild}$ conforms to its specification in \Cref{defn:semst-spec}.

We will need some auxiliary lemmas about $step$ and $\sfcomp$ to simplify the
proofs that follow. We will abbreviate
\[
  OK(d,σ) = src_\States(d(σ)) = σ \wedge \maxtrace{d(σ)} \wedge \elabtrace{d(σ)}
\]

\begin{lemma}
  If $σ$ is well-elaborated, $\maxtrace{f(σ)}$ and $\elabtrace{f(σ)}$,
  then $OK(step(f),σ)$.
\end{lemma}
\begin{proof}
  For undefined $f$, $σ\trend$ is well-elaborated and
\end{proof}

\begin{lemma}
  If $OK(d_1,σ_1)$ and $OK(d_2, σ_2)$ whenever $σ_2 = tgt_\States(d(σ))$, then
  then $OK(d_1 \sfcomp d_2, σ_1)$.
\end{lemma}
\begin{proof}
By coinduction, following~\citep{Czajka:2019}.
The coinduction hypothesis is:
\begin{gather*}
  P(α) = ∀d_1,d_2,σ_1,σ_2.\ OK(d_1,σ_1) \wedge (σ_2 = tgt_\States(d(σ)) \Longrightarrow OK(d_2,σ_2)) \Longrightarrow \\ src_\States((d_1 \sfcomp d_2)(σ)) = σ \wedge \maxtracen{α}{(d_1 \sfcomp d_2)(σ)} \wedge \elabtracen{α}{(d_1 \sfcomp d_2)(σ_1)}
\end{gather*}
Where $α$ is a limit ordinal smaller than or equal to the closure ordinal $ζ$
and $\elabtracen{α}{π}$ and $\maxtracen{α}{π}$ are the $α$-approximants of the
coinductive predicates $\elabtrace{π}$ and $\maxtrace{π}$.

We have $src_\States((d_1 \sfcomp d_2)(σ_1)) = src_\States(d_1(σ_1)) = σ_1$,
where the former equality follows by calculating with $\sconcat$ and the latter
equality follows by $OK(d_1,σ_1)$.

If $d_1(σ_1)$ is infinite, we have $(d_1 \sfcomp d_2)(σ_1) = d_1(σ_1)$ and the
goal follows from $OK(d_1,σ_1)$. So $d_1(σ_1)$ must be finite. Proceed by case
analysis over $π_1 \triangleq d_1(σ_1)$.
If $π_1 = σ_1\trend$, then $tgt_\States(π_1) = σ_1$ and
$(d_1 \sfcomp d_2)(σ_1) = d_2(σ_1)$. The goal follows from the assumption
$OK(d_2,σ_1)$.
If $π_1 = σ_1; π_1'$, then $OK(\fn{\wild}{π_1'},src_\States(π_1'))$ is easily
derived from the assumption $OK(d_1,σ_1)$ and we can apply the coinduction
hypothesis to get
$\elabtracen{α}{π_1' \sconcat d_2(tgt_\States(π_1'))}$ and
$\maxtracen{α}{π_1' \sconcat d_2(tgt_\States(π_1'))}$.
But $\elabstate{σ_1}$ and $σ_1 \smallstep src_\States(π_1')$ by the assumption
$OK(d_1,σ_1)$.
Hence $\elabtracen{α+1}{π_1 \sconcat d_2(tgt_\States(π_1))}$
and $\maxtracen{α+1}{π_1 \sconcat d_2(tgt_\States(π_1))}$.

\end{proof}

\begin{lemma}[S1]
  Let $σ$ be a state and $\pe$ its control expression.
  Then $σ$ is the source state of $\semst{\pe}(σ)$.
\end{lemma}
\begin{proof}
  Trivial for the first clause of $\semst{\wild}$.
  Now, realising that $src_\States((l \sfcomp r)(σ)) = src_\States(l(σ))$
  and that $src_\States(step(f)(σ)) = σ$ for any $f$, we can see that the
  proposition follows for other clauses by applying these two rewrites to
  completion.
\end{proof}

The proof of (S2) depends on (S3), hence we will prove (S3) first.

\begin{lemma}[S3]
  Let $σ$ denote a well-elaborated state and $\pe$ its control expression.
  Then $\elabtrace{\semst{\pe}(σ)}$.
\end{lemma}
\begin{proof}
By coinduction, following~\citep{Czajka:2019}.
The coinduction hypothesis is:
\[
  P(α) = ∀σ,\pe.\ \elabstate{σ} \wedge (\pe = ctrl(σ)) \Longrightarrow \elabtracen{α}{\semst{\pe}(σ)}
\]
Where $α$ is a limit ordinal smaller than or equal to the closure ordinal $ζ$,
$ctrl(σ)$ selects the control of $σ$ and $\elabtracen{α}{π}$ is the
$α$-approximant of the coinductive predicate $\elabtrace{π}$.

Let us assume that $\elabstate{σ}$ for an arbitrary $σ$ with control expression
$\pe$. Whenever $f(σ)$ is undefined for some $f$, we can see that
$step(f)(σ) = σ\trend$ is well-elaborated. Furthermore, if both $g$ and $h$
yield well-elaborated traces given a well-elaborated input state, then
$g \sfcomp h$ does so, too.

We abbreviate $π \triangleq \semst{\pe}(σ)$ and proceed by case analysis over
$pe$. We have $π_1 = σ$ by (S1) which is thus well-elaborated.
\begin{itemize}
  \item \textbf{Case $\px$}:
    Let us assume first that $look(\px)(σ)$ is undefined.
    Then $step(look(\px))(σ) = σ\trend$.
    On the other hand, $step(upd)(σ) = σ\trend$ because $upd$ is only defined on
    return states.
    Thus, $π = σ\trend$ and $\elabtracen{α+1}{π}$.
    When $look(\px)(σ)$ is defined, $σ$ must be of the form $(\px,ρ,μ,κ)$, and
    $\pa,\pe,ρ',d$ must exist as in the definition of $look$ (shadowing $\pe =
    \px$ from the assumption, for simplicity).
    Clearly, $σ' \triangleq (\pe, ρ', μ, \UpdateF(\pa) \pushF κ)$ is
    well-elaborated, because $σ$ is and the heap did not change.
    Similarly, by $\elabstate{σ}$ we have $d = \semst{\pe}$.
    Now we apply the coinduction hypothesis to $π' = \semst{\pe}(σ')$
    (noting that $\semst{\pe}$ is well-defined on $σ'$) and see that
    $\elabtracen{α}{π'}$, hence $\elabtracen{α+1}{σ; π'}$.
    If $π'$ is infinite, we are done.
    Otherwise, $σ_u \triangleq tgt_\States(π')$ is well-elaborated and if
    $upd(σ_u)$ is undefined we are done, too, by our preceding considerations.
    If $upd(σ_u)$ is defined, we $σ_u$ must have the form
    $((\pv,v),ρ,μ,\UpdateF(\pa) \pushF κ)$. We must show that
    $step(val(\pv,v)) = \semst{\pv}$ in order to show that the successor state
    is well-elaborated.
    That is the case: well-elaboratedness of $σ_u$ implies that $v = \FunV(f)$ for
    an $f$ just like that in the definition of $\semst{\pe}$.
  \item \textbf{Case $\Lam{\px}{\pe}$}:
    The $\ValueT$ transition does not change the heap, but it transitions into a
    return state. We have established in the previous point that the semantic
    value $\FunV(f)$ fits our requirements for well-elaboratedness exactly,
    hence well-elaboratedness for the trace follows.
  \item \textbf{Case $\pe~\px$}:
    The $\AppIT$ transition preserves well-elaboratedness to its target state, so
    the input $σ'$ to $\semst{\pe}$ is well-elaborated. We apply the coinduction
    hypothesis and and have $\elabtracen{α+1}{σ; \semst{\pe}(σ')}$.
    Forward composition preserves well-elaboratedness, hence it suffices to
    prove that given a well-elaborated input state $σ''$, $apply(σ'')$ is
    well-elaborated.
    That is the case if the occuring $f(\pa)(σ'')$ produces a well-elaborated
    trace.
    By $\elabstate{σ''}$, we know that $f$ must come from the
    $\semst{\Lam{\px}{\pe'}}$ case.
    $\AppET$ preserves well-elaboratedness to its target state and for
    $\semst{\pe'}$ we can apply the coinductive hypothesis, concluding the argument.
  \item \textbf{Case $\Let{\px}{\pe_1}{\pe_2}$}:
    The $\LetT$ transition updates the heap, but it is clear that
    $let(\semst{\pe_1})$ elaborates the correct $d_1$ into $σ'$.
    For the recursive call, we apply the coinduction hypothesis
    and conclude $\elabtracen{α+1}{σ; \semst{\pe_2}(σ')}$.
\end{itemize}
\end{proof}

The preceding lemma implies a very desirable property:
Whenever $σ$ is well-elaborated (and thus total), $\semst{\pe}(σ)$ is total,
too. The approximation order restricted to total elements is discrete; hence we
may forget about domain theoretic considerations now in favor of a coinductive
framework.

\begin{lemma}[S2]
  Let $σ$ denote a well-elaborated state and $\pe$ its control expression.
  Then $\maxtrace{\semst{\pe}(σ)}$.
\end{lemma}
\begin{proof}
By coinduction, with the following hypothesis, similar to the proof of (S3):
\[\begin{array}{c}
  OK(α,d) = ∀σ.\ \elabstate{σ} \Longrightarrow \maxtracen{α}{d(σ)} \\
  P(α) = ∀σ,\pe.\ \elabstate{σ} \wedge (\pe = ctrl(σ)) \Longrightarrow \maxtracen{α}{\semst{\pe}(σ)}
\end{array}\]
Let us assume that $\elabstate{σ}$ for an arbitrary $σ$ with control expression
$\pe$. Whenever $f(σ)$ is undefined for some $f$, we can see that
$step(f)(σ) = σ\trend$ is well-elaborated. Furthermore, if both $g$ and $h$
yield well-elaborated traces given a well-elaborated input state, then
$g \sfcomp h$ does so, too.

We abbreviate $π \triangleq \semst{\pe}(σ)$ and proceed by case analysis over
$pe$. We have $π_1 = σ$ by (S1) which is thus well-elaborated.
\begin{itemize}
  \item \textbf{Case $\px$}:
    Let us assume first that $look(\px)(σ)$ is undefined.
    Then $step(look(\px))(σ) = σ\trend$.
    On the other hand, $step(upd)(σ) = σ\trend$ because $upd$ is only defined on
    return states.
    Thus, $π = σ\trend$ and $\elabtracen{α+1}{π}$.
    When $look(\px)(σ)$ is defined, $σ$ must be of the form $(\px,ρ,μ,κ)$, and
    $\pa,\pe,ρ',d$ must exist as in the definition of $look$ (shadowing $\pe =
    \px$ from the assumption, for simplicity).
    Clearly, $σ' \triangleq (\pe, ρ', μ, \UpdateF(\pa) \pushF κ)$ is
    well-elaborated, because $σ$ is and the heap did not change.
    Similarly, by $\elabstate{σ}$ we have $d = \semst{\pe}$.
    Now we apply the coinduction hypothesis to $π' = \semst{\pe}(σ')$
    (noting that $\semst{\pe}$ is well-defined on $σ'$) and see that
    $\elabtracen{α}{π'}$, hence $\elabtracen{α+1}{σ; π'}$.
    If $π'$ is infinite, we are done.
    Otherwise, $σ_u \triangleq tgt_\States(π')$ is well-elaborated and if
    $upd(σ_u)$ is undefined we are done, too, by our preceding considerations.
    If $upd(σ_u)$ is defined, we $σ_u$ must have the form
    $((\pv,v),ρ,μ,\UpdateF(\pa) \pushF κ)$. We must show that
    $step(val(\pv,v)) = \semst{\pv}$ in order to show that the successor state
    is well-elaborated.
    That is the case: well-elaboratedness of $σ_u$ implies that $v = \FunV(f)$ for
    an $f$ just like that in the definition of $\semst{\pe}$.
  \item \textbf{Case $\Lam{\px}{\pe}$}:
    The $\ValueT$ transition does not change the heap, but it transitions into a
    return state. We have established in the previous point that the semantic
    value $\FunV(f)$ fits our requirements for well-elaboratedness exactly,
    hence well-elaboratedness for the trace follows.
  \item \textbf{Case $\pe~\px$}:
    The $\AppIT$ transition preserves well-elaboratedness to its target state, so
    the input $σ'$ to $\semst{\pe}$ is well-elaborated. We apply the coinduction
    hypothesis and and have $\elabtracen{α+1}{σ; \semst{\pe}(σ')}$.
    Forward composition preserves well-elaboratedness, hence it suffices to
    prove that given a well-elaborated input state $σ''$, $apply(σ'')$ is
    well-elaborated.
    That is the case if the occuring $f(\pa)(σ'')$ produces a well-elaborated
    trace.
    By $\elabstate{σ''}$, we know that $f$ must come from the
    $\semst{\Lam{\px}{\pe'}}$ case.
    $\AppET$ preserves well-elaboratedness to its target state and for
    $\semst{\pe'}$ we can apply the coinductive hypothesis, concluding the argument.
  \item \textbf{Case $\Let{\px}{\pe_1}{\pe_2}$}:
    The $\LetT$ transition updates the heap, but it is clear that
    $let(\semst{\pe_1})$ elaborates the correct $d_1$ into $σ'$.
    For the recursive call, we apply the coinduction hypothesis
    and conclude $\elabtracen{α+1}{σ; \semst{\pe_2}(σ')}$.
\end{itemize}
\end{proof}
\begin{proof}
  Let us abbreviate the property of interest to
  \[
    OK(d) = ∀σ. σ\text{ well-addressed} \Longrightarrow \validtrace{d(σ)}
  \]

  Every other clause of $\semst{\wild}$ is built from applications of
  $\sfcomp$, $step$ or $apply$ and it suffices to show that these combinators
  yield valid CESK traces when applied to $σ$.

  We can see that $OK(d_1 \sfcomp d_2)$ whenever $OK(d_1)$ and $OK(d_2)$ holds,
  because the destination state of $d_1(σ)$, if it exists, is well-addressed by
  \Cref{lemma:preserve-well-addressed} and $OK(d_2)$ can be usefully applied.
  An interesting case is when $d_1(σ)$ is infinite; then
  $(d_1 \sfcomp d_2)(σ) = d_1(σ)$ and $\validtrace{(d_1 \sfcomp d_2)(σ)}$ follows
  from $\validtrace{d_1(σ)}$.

  We can see that $\validtrace{step(f)(σ)}$ whenever $f(σ)$ is undefined
  (by $\validtraceTriv$) or $\validtrace{σ;f(σ)}$ (by $\validtraceTrans$).
  Enumerating the arguments to $step$, we can see that $val, upd, app_1, app_2$
  and $let$ follow $\ValueT, \UpdateT, \AppIT, \AppET$ and $\LetT$ closely by
  making exactly one transition. That leaves $loop$, which corresponds to doing
  one step with $\LookupT$ and then evaluating the heap-bound expression $\pe$
  as expressed by $d$.

  By (bi-)induction that is always the case.
\end{proof}

Let us begin by defining that a denotation $d$ is \emph{valid everywhere} when
for every initial $σ$, we have $\validtrace{σ}$. This definition lets us express
an important observation:

We can now formulate our correctness theorem as follows:

\begin{theorem}[Adequacy of the stateful trace semantics]
Let $d = \semst{\pe}$. Then for all $ρ,μ$ with $\vdash_\Heaps μ$, we have $μ
\vdash_\StateD \pe \sim_ρ d$.
\end{theorem}

\begin{corollary} Let $d = \semst{\pe}$. Then $π=d(\pe,[],[],\StopF)$ is a
maximal trace for the transition semantics starting at $(\pe,[],[],\StopF)$.
\end{corollary}


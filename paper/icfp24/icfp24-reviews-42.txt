ICFP 2024 Paper #42 Reviews and Comments
===========================================================================
Paper #42 Abstracting Denotational Interpreters


Review #42A
===========================================================================

Overall merit
-------------
D. Reject

Reviewer Expertise
------------------
X. I am an expert in this area

Reviewer Confidence
-------------------
3. Good: I am reasonably sure of my assessment

Paper summary
-------------
This paper describes a method to derive correct by construction
abstract interpreters for higher-order languages that compute
summary-based analyses, by defining a generic denotational interpreter.
One instance of the interpreter produces possibly-infinite traces of
executions, and it is showed that it adequately describes call by need
evaluation (respectively, call by name for another instance). The
correspondence is proved with respect to a variant of the LK machine.
A class of other instances, that must respect "abstraction laws",
implement provably correct abstractions of the former instance. The
authors describe as an example an instance of their framework that
defines a usage static analysis for a λ-calculus endowed with a call
by need semantics. They also mention a larger-scale implementation in
the Glasgow Haskell Compiler, and other instances of their framework
that perform Hindley-Milner type inference, and 0-CFA analysis. A
large part of the paper's claims is formalized in Guarded Cubical
Agda.

Assessment of the paper
-----------------------
The paper is globally hard to read, mainly because of the lack of a
clear separation between the body of the article and its appendices.
The authors should strive to obtain a self-contained article body.
Moreover, the writing looks unfinished, since the paper lacks a
conclusion and a description of future work.

While the idea of defining an interpreter that is parameterized over
the interpretation of the language's constructs is nice, it is not
novel. The interpreter merely resembles a fold over the syntax tree,
which is a common technique to all denotational semantics of programs.
The parametrization over the semantics of basic language constructs
is, for example, very common in categorical semantics, where
interpretations are defined for any category that satisfies some
properties.

The choices for the decomposition of the interpreter is non-standard,
sometimes surprising. In particular, the reason why the interpreter is
interspersed with calls to `step` should be justified more clearly. In
particular, are the calls to `step` inserted only for soundness
reasons (wrt. the traces of the LK abstract machine), or for
termination reasons (so that the code is guarded and thus accepted by
Agda)? It seems the interpreter that is presented could be obtained as
a particular instance of a standard denotational semantics, that is
oblivious to the interpretation domain of traces (and thus that does
not call `step` at all).

The authors interpret programs as the maximal traces they produce.
Such traces can be infinite for diverging programs, i.e., they belong
to a co-inductive type. This creates issues with
guardedness/productivity, and proofs are subtle because they make use
of Löb induction, so it is a good idea that the authors mechanized
them. Interestingly, the authors make several remarks on whether this
was an appropriate choice, since, after all, they are interested in
safety properties. Moreover, it seems the appendix contains an
abstraction of the traces into sets of (finite) partial traces.
Considering such partial traces was the approach of [1]. Why didn't
you start with a semantics of partial traces in the first place? This
would have saved you the difficult work of dealing with co-inductive
traces.

Most of the paper is devoted to 1- the explanation of why the authors'
approach for proving soundness of analyses is simpler than more
syntactic approaches, and to 2- the description of the absence/usage
analysis. In comparison, too short a space is reserved to explain how
their framework can be used to actually prove the soundness of an
analysis, and why their framework is correct. This leaves several
important questions unaddressed, which makes the argumentation little
convincing.

In particular:

1- In figure 13, you require that all the functions of the type
   classes Trace, Domain and HasBind must be monotonic (with respect
   to the pre-order of the abstract domain). This is in practice very
   constraining, and can rule out several useful analyses. In
   particular, as soon as you make use of widening (which you claim
   you employ in the implementation of `bind`), monotonicity will most
   likely be broken. It is indeed well-known that widening on
   intervals, for example, is not monotonic. What is usually required
   is that transfer functions in the abstract domain must
   over-approximate the transfer functions in the concrete domain, and
   that the transfer functions in the concrete domain are monotonic
   (which is usually satisfied, as the concrete transfer functions are
   most of the time join morphisms from sets to sets).

2- It is extremely surprising that the soundness of the pre-order of
   the abstract domain is never mentioned or required. This is usually
   stated either as a Galois connection, or as the monotonicity of the
   concretization function or of the abstraction function. The
   soundness of the abstract pre-order is crucial to the soundness of
   a static analyzer, and crucial to the soundness of your `lfp`
   function. Without this soundness condition, one could take the
   total relation for the abstract pre-order ⊑. This would validate
   all of the abstraction laws that you prescribe, whatever the
   implementation of the abstract transfer functions. Therefore,
   Theorem 6, as it is currently stated, is very likely to be
   incorrect.

3- Theorem 6 mentions the `abstract` function, that is never defined.
   This is worrisome. Sentences like "`abstract` is entirely derived
   from type class instances" or "the precise definition of `abstract`
   can be found in the proof of the following theorem" without any
   further details do not provide a single clue of how it could be
   defined. Because this is a central piece of the soundness argument,
   you must give its definition to the reader. It is not acceptable to
   ask the reader to search in the appendix for this definition,
   that is buried inside a proof.

4- Can the abstraction laws be proved in a modular way? At least three
   of them (Beta-App, Beta-Sel and Bind-ByName) seem to be problematic
   in this respect, because they mention the analysis function
   (SD[e]ρ). Their proofs seem to require a substitution lemma
   (Lemma 7) as the authors indicate. This means that to prove these
   two rules, one must know the whole analysis function, and reason on
   it as a whole. In the case of Bind-ByName, one also has to reason
   about `lfp`. I presume this is not easy... It seems that your
   abstraction laws are a form of preservation proof at the abstract
   level, but then we are back to a syntactic proof method! This is in
   stark contrast with the abstract interpretation approach, where
   each transfer function can be proved correct independently of each
   other, and where generic theorems ensure the soundness of the least
   fixed point computation, for example.

Finally, it seems the authors use the vocable "summary-based analysis"
to denote two very different things. On the one hand, they refer to
the technique of analyzing once and for all the code of the function,
so that the calls of a function only have to instantiate the summary
(similarly to what type systems do). In this sense, a summary denotes
all the possible future uses of a function, and its computation is not
driven by the uses of the function, and a summary is computed only
once. On the other hand, they refer to the technique of representing
all the ways a function might have been called in a program in the
past (similarly to what CFAs do). In this sense, a summary denotes all
the possible uses of a function by a program, and its computation is
driven by the uses, and a summary might require multiple analyses of a
function body until a fixpoint is found (and it seems the functions in
the FunCache of your 0CFA are precisely there so that they can be
called multiple times!). These are two very different things. The
authors, however, blur this important distinction, because they give
instances of their framework that fit in the first category (usage
analysis, Hindley-Milner) of other instances that fit in the second
category (0CFA). The authors have to clarify this point in a future
version.

[1] Cousot, Patrick and Cousot, Radhia
    An abstract interpretation framework for termination.
    2012.

Questions for authors’ response
-------------------------------
- Please elaborate on the connections with interaction trees, that also
  represent the traces of programs as a co-inductive value [1,2]. They
  are also used to give denotational semantics to programs, and
  provide some intensional information about a program's behavior.

- Same question with game semantics.

- How does this paper compare to work that also use sets of traces for
  the λ-calculus to derive CFA, e.g. [3]?

- Which parts have you mechanized in Agda? Which parts are not
  mechanized?

- Please also answer the questions raised or issues mentioned in the
  "Assessment of the paper" section.

[1] Li-yao Xia, Yannick Zakowski, Paul He, Chung-Kil Hur, Gregory
    Malecha, Benjamin C. Pierce, and Steve Zdancewic.
    Interaction trees: representing recursive and impure programs in Coq.
    POPL 2019.

[2] Dan Frumin, Amin Timany, and Lars Birkedal.
    Modular Denotational Semantics for Effects with Guarded Interaction Trees.
    POPL 2024.

[3] Montagu, Benoît and Jensen, Thomas
    Trace-Based Control-Flow Analysis.
    PLDI 2021.

Comments for authors
--------------------
- The definition of figure 1 is not explained.

- Line 146: this footnote, at this place in the text begs the question
  of whether this fixpoint is well defined (does it exist? in which
  partially-ordered set?)

- Lines 155-166: the example is explained step by step, but this is
  hardly informative. It provides no explanation or no intuition of
  why this computation makes sense for absence analysis.

- Line 175: the pre-order relation ⊑ should probably be introduced and
  explained earlier

- Lines 263-271: this is obscure. The text depends too heavily on the
  contents of the appendix.

- Figure 2, rule Let1: at this point, we notice that let bindings are
  recursive, it seems, as opposed to the let bindings of the language
  of section 2.1. A surprise for the reader.

- Figure 2, rule Look(y): recording the variable `y` is a difference
  with the LK or the MarkII machines. The cases where x <> y are
  interesting for the reader, and the last example of section 3
  illustrate such a case. It is however not explained, and the reader
  has to understand/discover this point alone.

- Figure 5: you have chosen that `fun` takes a variable as a first
  parameter (the bound variable of the function). Why didn't you make
  similar choices for constructs that also involve bound variables?
  (bind, select)

- Figure 6 is useless, since the reader has to complete the definition
  by herself!

- The statement of Theorem 4 (Strong Adequacy) and its proof are too
  sketchy. I am not sure what the theorem statement really means.

- Line 908: "Occurrences of x must make do with the top value".
  I don't understand the sentence.

- Lines 923-926: "Much more could be said about..." Either say it, or
  do not mention it at all. I have no idea what you have in mind.

- Line 930 (footnote 24): please explain what you mean.

- Figure 12: it is very upsetting that `lub` is not used at all in
  this definition...

- Line 958: "The keen reader may feel indignant..." It is generally a
  good idea to avoid the reader entering a state of anger. Please
  rephrase the explanation so that it does not happen.

- Lines 960-961: "This is easily worked around in practice by
  employing appropriate widening measures such as bounding the depth
  of ValueU". Could you please give the definition, then, if you
  actually had to implement such measures? Does it have consequences
  about monotonicity?

- Lines 978-979 (footnote 25): "Never mind totality; why is the use of
  least fixpoints even correct?". Is this an remark that was targeted
  to the authors discussing a point while writing the paper, or is is
  targeted to the reader? Should the reader give you the answer?
  Please rephrase.

- Figure 13: please explain rules Unwind-Stuck and Intro-Stuck. The
  use of join in those rules seems rather surprising.

- Lines 1004-1007: this is obscure.

- Lines 1012-1015: this is not a proof rule. Please state what you
  mean precisely.

- Line 1018: "opinionated". Please use a more scientific vocabulary.
  This is not a blog post.

- Lines 1022-1023: "considering the final heap for nested abstraction
  (the subtle details are best left to the Appendix)". I don't have
  the slightest idea of what you mean...

- Lines 1142-1147: "... but it does not make any claim on whether a
  transformation conditional on some trace property is actually sound,
  yet alone an improvement, etc". Again, this is obscure. Please
  explain. I don't understand why your interpreter would not be
  appropriate for dead code elimination either.

- Lines 1169-1170: "we think that CFA can be used to turn any finite
  Trace instance such as TU into a static analysis without the need to
  define a custom summary mechanism". I don't understand what you have
  in mind.

- Lines 1197-1201: What do you mean? That you are not able to model
  demand transformers?



Review #42B
===========================================================================

Overall merit
-------------
B. OK paper, but I will not champion it

Reviewer Expertise
------------------
Y. I am knowledgeable in this area, but not an expert

Reviewer Confidence
-------------------
3. Good: I am reasonably sure of my assessment

Paper summary
-------------
The paper defines a generic interpreter for an untyped λ-calculus with
recursive let-bindings.  The interpreter generates a trace in the lazy
Krivine machine i.e.  each evaluation `step' is decorated with the
name of a machine transition.  The interpreter is parametric in the
semantic domain, a type plus a handful of semantic operations,
abstracted away using three type-classes.

A suitable instantiation of the semantic domain allows different
evaluation strategies (with call-by-name, call-by-need and
call-by-value) and various static analyses (usage analysis, type
analysis and 0CFA control-flow analysis) to be implemented.  The
authors claim that one of these instantiations is the first provably
adequate denotational semantics for call-by-need.  As all of these
instantiations share the same generic interpreter, soundness proofs
can be couched solely in terms of properties of the semantic
operations (called abstraction laws), while the plumbing is done only
once and for all.

Assessment of the paper
-----------------------
The introduction (§2) is simply great: the problem is clearly stated,
the solution well motivated and justified.  (Someone has read Leslie
Lamport's ``State the Problem Before Describing the Solution'' and
taken it to heart.) After reading the first six pages I meant to
champion the paper but in the end I didn't.  Here is why:

The paper is humongous: 55 (!) pages, that is, the main body plus a 30
page appendix.  (Even then the authors write ``we omit large parts of
the implementation and the Domain instance for space reasons.''). I
couldn't help feeling that the authors wanted to achieve too much:
they present 5 different evaluation strategies, and 4 different types
of static analyses (well, actually, only one is described in the main
body of the text, the others are related to the appendix).  Can I
profitably read the paper without repeatedly consulting the appendix?
I am not so sure.

I assume (but I may be wrong) that the main goal of the paper is to
provide a generic framework for summary-based *static analyses* and to
demonstrate the benefit of this approach (modularity, short soundness
proofs etc).  Alas, the main body of the paper only details a single
analysis (the absence analysis of §2 is a special case of the usage
analysis in §6), which doesn't really serve the paper's main goal.
Instead, 5 different dynamic semantics are presented. Why? In
particular, as two (?) of them are unfit as a denotational semantics
(Call-by-value, Clairvoyant Call-by-value).  Since the basis of the
generic interpreter (the reference semantics) is the *lazy* krivine
machine, these endeavours seem to be misguided.  (Perhaps, you should
also abstract away from the Event type, supporting different abstract
machines.) Instead, I would have liked to see a second or a third
static analysis (strictness analysis?) to support the modularity
claims.

The authors present two artefacts: a Haskell implementation of the
generic interpreter and the various instances, and an Agda (well,
Guarded Cubical Agda) implementation of the generic interpreter and
its instances at ByName and ByNeed (sadly, the Agda code doesn't
typecheck, see below).  I was disappointed to learn that the latter
artefact is only used to show that the ByName/ByNeed instances are
well-defined.  In particular, it is *not* used to show semantic
adequacy.  To be fair: the appendix contains all the proofs, but these
seem to be pencil-and-paper proofs (which I didn't check).

To be clear: I like the paper a lot, but not enough to champion it.

Comments for authors
--------------------
As I said, the introduction (§2) is simply great.  The remainder of
the paper could do with some more high-level sign-posting:

· What is the source language? It took me a while to figure out that
  let-bindings are recursive (this may be obvious for readers with a
  Haskell background, but less obvious for those with a Standard ML or
  Agda background).  Likewise, it took me a while to realise that the
  source language is untyped.  Tell me!

· What exactly do you achieve? In particular, what's the relation
  between the Agda artefacts and the Haskell artefacts — the Agda code
  is subtly (?) different to guarantee well-definedness.  Which
  theorems enjoy ``only'' pencil-and-paper proofs — what's the
  setting?  set theory? domain theory (CPOs)?, which theorems are
  fully mechanised?

There are too many footnotes for my taste. (Resist the temptation to
include everything you know about the domain.)  Instead, spend more
time on the basic/core definitions: carefully explaining the source
language, the generic interpreter, the machine etc.

The source language and the abstract machine support sum types, but
you never (?) use them in your examples.  Why not ditch them?  Rather
tellingly you write ``we will ignore Case and Case in this section
because we do not consider data types for now.'' Strangely, ``this
section'' is the section where you *introduce* the machine.

Why do you first define the semantic domain for a call-by-name variant
of your language and only then define the interpreter?  Surely, the
interpreter is independent of that instance.  Fig. 5 again mixes the
two, which put me on the wrong track for a while.  The generic
interpreter labels the evaluation `steps' with rule names of the lazy
krivine machine.  It would be nice if you actually use the same names
(i.e. in Fig.2 and in data Event).  I was expecting to see every Event
constructor in the definition of the Abstract Denotational
Interpreter.  Alas, Let_0 and Update do not make an appearance, which
seems odd.

Sadly, the Agda code doesn't typecheck.
---
Concrete.agda:181,54-58
The term α, given as an argument to the guarded value
  d▹ : Tick → D (ByNeed τ)
can not be used as a @tick argument, since it does not mention any
@tick variables.
when checking that the expression d▹ α has type
ByNeed _τ_540 _v_541
---
I use Agda version 2.6.4.3. Also the code contains a few postulates
(are these really parameters to the development?), and a dangerous
pragma: NO_POSITIVITY_CHECK.



Review #42C
===========================================================================

Overall merit
-------------
A. Good paper, I will champion it

Reviewer Expertise
------------------
X. I am an expert in this area

Reviewer Confidence
-------------------
3. Good: I am reasonably sure of my assessment

Paper summary
-------------
This paper presents a generic "denotational interpreter" for a core
functional language and various semantic domains that carry
intensional information characterizing operational aspects of the
interpreted program.  Domains are presented for call-by-name, -need,
and -value, and the computed semantics correspond to traces of
well-known abstract machines.  The generic interpreter and semantic
domains are defined both in Haskell and Cubical Agda.  The
instantiations of these denotational interpreters are proven total and
adequate (corresponding to the reference machines).

The combination of denotational style semantics (defined
compositionally over the syntax of expressions) and the computation of
intensional traces characteristic of operational semantics enables a
nice "best of both worlds" approaches compositionally defined semantic
artifacts with compositional proofs that nonetheless are able to
characterize operational aspects of programs.

Importantly, the paper shows that the generic interpreter can be
instantiated with domains for sound computable approximations of the
operational behaviors of programs, i.e. program analyses.  The paper
argues this approach lends to more nicely structured proofs of
soundness due to a shared interpreter structure and static analyses
that are naturally modular and don't require reanalysis of code at
each use site.

A number of representative static analyses are defined using the
framework of the paper such as a type analysis, 0CFA, and a demand
analysis.  To demonstrate the scalability of the approach, the demand
analysis is implemented for GHC, showing the technique works in the
context of a mature, real-world compiler.

Assessment of the paper
-----------------------
Strengths:

+ The paper synthesizes, crystallizes, and improves upon several
  existing themes in the literature on abstraction interpretation of
  functional languages.  Notably, the paper offers a rather nice
  successor to the Abstracting Definitional Interpreters work of
  Darais and notably provides a much nicer story for soundness proofs.

+ The paper contributes a novel, concise, and adequate denotational
  semantics for call-by-need.

+ The approach scales to real languages and compilers, namely Haskell
  and GHC.

+ The embedding of the approach with Cubical Agda suggests that the
  approach is amenable to mechanical certification.

+ The paper is well written, the technical details appear correct, and
  the results are well situated with respect to the existing
  literature.

Weaknesses:

+ Some important details of the demand analysis for GHC are omitted.

+ The paper could do a better job acknowledging limitations.

Questions for authors’ response
-------------------------------
One piece of the paper that stood out to me as not well integrated was
the section 4.3.5 on Clairvoyant Call-by-value.  In the end we see
that the definition is not denotational, but why this point is
relevant or interesting is never really addressed, except for a
passing reference in the final sentences of the paper saying that the
workings of the demand analysis are similar to the Clairvoyant CBV and
that it's a shame the instantiation is partial.

Can you elaborate on why the Clairvoyant semantics is important to the
paper?

Can you explain what's meant by the last discussion in the paper?  It
sounds as though the implementation of the GHC demand analysis does
not satisfy the totality requirement.  Is that the case and what are
the consequences?

A related question is: does the approach continue to work in
situations where the denotational interpreter is not total (and
therefore not really denotational) but the abstraction of it is?  Does
this have an impact on the soundness proofs?  (My guess would be this
works just fine.)

(In general, more description of the scaled-up implementation of
demand analysis in GHC would be interesting to this reader.)



Rebuttal Response by Author [<sgraf1337@gmail.com>] (2202 words)
---------------------------------------------------------------------------
We thank the reviewers for the helpful feedback and comments. Below we discuss each reviewers questions and feedback in turn.

# Overview

In the light of advice from all three reviews we accept that we were perhaps too carried away in our discussion of evaluation strategies in Section 4 (wonderful though it is that our framework is so expressive).  We are open to moving the Clairvoyant call-by-value strategy to the Appendix, freeing up space to present more details about static analyses.

We will clarify that we only used Agda to prove totality of the interpreter and proved all other statements using pen and paper.

# Reviewer A

#### RA.1: Widening most likely breaks monotonicity
This is a legitimate concern. However, based on our experience, we believe that requiring monotonicity is not nearly as constraining as alluded to by the reviewer, because all widening strategies we considered were monotone (example in "Widening" below). Moreover, even if some particular widening strategy violates the monotonicity requirement, we expect that it can be relaxed and the proofs adjusted accordingly, at the expense of more complicated abstraction laws.

#### RA.2: It is extremely surprising that the soundness of the pre-order of the abstract domain is never mentioned or required. [...] Therefore, Theorem 6, as it is currently stated, is very likely to be incorrect.
We do not believe that Theorem 6 is incorrect and are unsure what "soundness of the pre-order" means. If this question concerns the precise definition and lawfulness of the pre-order (i.e. reflexivity and transitivity), see “Order” below.

#### RA.3: Theorem 6 mentions the abstract function, that is never defined. .. It is not acceptable to ask the reader to search in the appendix for this definition, that is buried inside a proof.
We agree. We will fix this by giving its definition in the main body and referring to the underlying abstraction function on open expressions `nameNeed` in Appendix D.3.

#### RA.4: Can the abstraction laws be proved in a modular way?
Modularity of proofs depends on the analysis in question. Some first-order analyses like forward arity analysis [Gill 1996] admit modular proofs. Although the presented substitution lemma for usage analysis is non-modular, an alternative modular proof for summary application (f a ⊑ apply (fun x f) a) that characterises f more abstractly is conceivable. However as writers of a proof framework, we cannot anticipate all the proofs conducted with it; hence we must provide the strongest (syntactic) characterisation of f in rules such as Beta-App. Furthermore, even a non-modular proof is far better than the situation described in Section 1!

#### RA.5: Finally, it seems the authors use the term "summary-based analysis" to denote two very different things.
We agree that these are two different things, and we'll clarify the distinction in our revision.  (Our primary usage is the first, that is, compute a monovariant summary once per definition, not polyvariant summaries once per call, as in 0CFA.)

#### RA.6: While the idea of defining an interpreter that is parameterized over the interpretation of the language's constructs is nice, it is not novel.
True enough, but this was not our claim. Our contribution is in demonstrating that we can instantiate such an interpreter to

1. return *traces* of a small-step machine that express precise operational behaviour (e.g. heaps, thunk updates etc);
2. express diverse *evaluation strategies*;
3. be abstractable to express *summary-based* analyses.

Prior work does not do any of these three.

#### RA.7: How does this paper compare to work that also use sets of traces for the λ-calculus to derive CFA, e.g. [3]?
We think that a variant of our denotational interpreter would be a good fit for the collecting semantics in [3]. The semantic inclusions of Lemma 2.10 that govern the transition to a big-step interpreter follow simply by adequacy of our interpreter, Theorem 4.

#### RA.8: Please elaborate on the connections with interaction trees
Interaction trees (ITs) can implement the `Trace` instance in dynamic semantics to replace the vanilla trace type `T`. We would implement `step` via a `Vis`ible transition in the IT that does not need input (like a `putStrLn` effect). Concretely: `step evt d = Vis evt (\() -> d)`.
In the paper, we compare to ITs on line 439 and lines 1137.5-1139.5.
Interaction trees are preferable to `T` when the object language has means for input (e.g. reading from the terminal), which our object language does not have.

#### RA.9: Same question with game semantics.
Both our work as well as Game semantics have disparate notions of a trace, but there is no immediate connection.
Game semantics would be interesting to define a non-standard semantic equivalence relation on denotations, which we (consciously) do not. That is why we define absence in terms of syntactic evaluation contexts, for example.

#### RA.10: Why didn't you start with a semantics of partial traces in the first place?
We respectfully disagree with the suggestion that using partial traces would simplify this work.  On the contrary, we find it simpler to work with maximal traces.
In contrast to [1], our object language is deterministic. In that case we can simply take the prefix closure of the maximal trace generated by our fixpoint (maximal) trace semantics (4.3 in [1]) to recover an equivalent partial trace semantics as in 4.2 of [1]. Chapter 17.2 in [Cousot 2021] discusses the equivalence.

#### RA.11: The choices for the decomposition of the interpreter is non-standard, sometimes surprising.
Indeed it is a surprising decomposition! That is why we provide evidence for its usefulness in the paper. Perhaps another decomposition works as well, but our design worked well for us.

Guarding the recursion is essential for totality of the interpreter, as explained in the proof sketch of Theorem 5. To repeat line 785 and following, while some steps are only inserted for adequacy wrt. the LK machine, at least the steps before variable lookup are necessary to guarantee totality. That is, removing `step` from the interface would give up on totality.

Proving well-definedness and totality via traditional algebraic domains requires proving that all operations are monotone and ω-continuous, and that all total inputs lead to total outputs, to rule out that the output contains bottom. It is a path not taken because synthetic guarded domains are simply more appealing and less difficult to work with.

# Reviewer B

#### RB.1: The Agda code doesn’t typecheck
We are delighted that the reviewer had a look at our Agda supplement and pointed out this issue. Our code type-checks with Agda version 2.6.2.2. We will provide an artifact with that version of Agda, but we also managed to fix the issue in 2.6.4; more details in “Agda” below.

#### RB.2: Too many footnotes
Agreed. We will try to remove some footnotes for the revised version.

#### RB.3: The source language and the abstract machine support sum types, but you never use them in your examples.
We do have a short example using sum types on line 530 and we find `select` a type class method worth presenting. We will remove the confusing sentence in lines 325-326.

#### RB.4: Why do you first define the semantic domain for a call-by-name variant of your language and only then define the interpreter?
We did so because we found it hard to introduce the interpreter definition without discussing what it should return first.

#### RB.5: It would be nice if you actually use the same names (i.e. in Fig.2 and in data Event
We will rename the events Update and Lookup to match Fig. 2.

#### RB.6: Even then the authors write ``we omit large parts of the implementation and the Domain instance for space reasons.''
If there is interest in a more detailed Appendix C.1, we are open to including more of its implementation that is redundantly provided in the supplement (in `StaticAnalysis.hs`).

# Reviewer C

#### RC.1: Some important details of the demand analysis for GHC are omitted.
Good point. We will submit the adjusted Demand Analysis for artefact evaluation and will explain these adjustments in more detail (see “Demand Analysis” below).

#### RC.2: Can you elaborate on why the Clairvoyant semantics is important to the paper? Can you explain what's meant by the last discussion in the paper?
Originally, we wanted to prove Demand Analysis sound against the Clairvoyant call-by-value (CCbV) semantics. Such a proof would carry over to the (cost equivalent) call-by-need semantics. However, since we do not prove Demand Analysis sound, there is not really a point to introducing CCbV and we are open to moving it to the Appendix. More detail regarding the connection between Demand Analysis and CCbV in “Clairvoyant call-by-value” below.

#### RC.3: Does the approach continue to work in situations where the denotational interpreter is not total but the abstraction of it is?
Totality of a concrete interpreter instance has no bearing on totality of related static analyses and vice versa, so totality of Demand Analysis is independent of totality of CCbV. However, any correctness result derived by relation to CCbV might not hold for partial inputs.

# Detailed explanations

## Widening
Here is an example of a monotone widening strategy for usage analysis. We define `bind rhs body = body (kleeneFix (trim 42 . rhs))`, where `trim 42` ensures that any ValueU such as `u1:u2:…:u100:Rep U1` is trimmed to finite length 42, `u1:u2:…:u42:Rep Uω`. This definition of `bind` is monotonic *despite* the fact that it employs widening to guarantee convergence of fixpoint iteration.

## Order
The definition of ⊑ in the usage domain follows structurally from the order `U0 ⊏ U1 ⊏ Uω` (line 954.5), where *structurally* means pointwise, product-wise, etc.. These partial orders are widely accepted as standard and their lawfulness follow by parametricity, as detailed in [Backhouse and Backhouse 2004]. The only non-structural ordering is for ValueU because of the additional syntactic equality as explained on lines 151 and line 912.

## Agda
The development is fixed for Agda 2.6.4 by adding a type annotation to `α` in line 181 of Concrete.agda, so
```
                     	(λ d▹ → ∀ μ (@tick α : Tick) → ByNeed.get (d▹ α) μ ≡ f μ α)
```
(This inferene regression seems unfortunate.) The `NO_POSITIVITY_CHECK` pragma is necessary because the positivity checker does not (yet) know about guarded types. The postulates could well be expressed as parameters to the development. These issues sum up why we did not attempt to formalise more in Agda: We are inexperienced in doing so and building our research on the (largely untested and unmaintained) guarded cubical variant did not seem like an efficient use of our time. We would be happy to discuss and share what we have, though!
If possible, we would like to formalise all our proofs in Agda’s variant of ticked cubical type theory.

## Demand Analysis
We strive to submit the scaled-up implementation in GHC for artefact evaluation.
To get a feeling, this is the extended Domain type class shared between the denotational interpreter for GHC Core (which ANFises on-the-fly and, amazingly, works for reasonable programs) and Demand Analysis:
```
class Domain d where
  stuck :: d
  erased :: d
  lit :: Literal -> d
  global :: Id -> d
  classOp :: Id -> Class -> d
  primOp :: Id -> PrimOp -> d
  fun :: Id -> (d -> d) -> d
  con :: DataCon -> [d] -> d
  apply :: d -> d -> d
  applyTy :: d -> d
  select :: d -> CoreExpr -> Id -> [DAlt d] -> d
  keepAlive :: [d] -> d -- Used for coercion FVs, unfolding and RULE FVs
  seq_ :: d -> d -> d -- Just like `select a (const False) wildCardId [(DEFAULT, [], \_a _ds -> b)]`, but we don't have available the type of `a`.
type DAlt d = (AltCon, [Id], d -> [d] -> d)
```
The new methods are all due to special Core forms and its type erasure procedure. For `HasBind`, we need to generalise `bind` to mutual recursive binding groups:
```
data BindHint = BindArg Id | BindLet CoreBind
class HasBind d where
  bind :: BindHint -> [[d] -> d] -> ([d] -> d) -> d
```

## Clairvoyant call-by-value
CCbV is an evaluation strategy that “guesses” through non-determinism whether a let binding will later be needed, and either (A) drops the binding when it is not needed, or (B) evaluates it by-value when it will be needed. This is quite similar to Demand Analysis, which, for an expression `let x = e1 in e2`, conservatively approximates this guess on the evaluation cardinality of `x` in order to either (a) discard uses in `e1` when `x` is absent in `e2`, (b) treat all strict uses in `e1` as globally strict when `x` is used strictly in `e2`, or otherwise (c) treat all uses in `e1` as lazy. One can see that (A) corresponds to (a), (B) corresponds to (b) and (c) is the lub of (a) and (b). Thus, it would be natural to relate Demand Analysis to a clairvoyant semantics for a soundness proof, but annoyingly, such a proof would carry no meaning for programs that diverge under CCbV.
As future work we hope that CCbV can inspire a set of abstraction laws that can be used to prove Demand Analysis correct in terms of the call-by-need semantics, using a slightly different Galois connection than nameNeed in Fig. 18.

# References

[1] Cousot and Cousot, An abstract interpretation framework for termination. 2012.

[3] Montagu and Jensen, Trace-Based Control-Flow Analysis. PLDI 2021.

[Cousot 2021] Cousot, Principles of Abstract Interpretation. MIT Press, 2021.

[Backhouse and Backhouse 2004] Backhouse and Backhouse, Safety of abstract interpretations for free, via logical relations and Galois connections. 2004.

[Gill 1996] Gill, Cheap deforestation for non-strict functional languages. PhD thesis, University of Glasgow, 1996.

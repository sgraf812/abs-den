\pagebreak
\section{Related Work}
\label{sec:related-work}

\subsubsection*{Operational Semantics and Abstract Machines}
Plotkin's Aarhus lectures~\citep{Plotkin:81} in the late 70's systematically
introduced small-step operational semantics based on transition systems,
covering both state and higher-order functions.
The use of transition systems was novel, in contrast to the then
prevalent notion of \emph{abstract machine} definitions, such as for the
\emph{G-machine}~\citet{Johnsson:84} for executing Lazy ML programs.
\citet{SPJ:92} bridged the gap between graph reduction and transition systems,
giving an operational semantics for the still widely used \emph{STG machine} for
lazy evaluation.
Soon after, \citet{Launchbury:93} gave a big-step operational semantics for
call-by-need lambda calculus that was drastically simpler than the STG machine
(and thus simpler to formalise), featuring an explicit heap.
Our heap forcing relation $(\forcesto)$ from \Cref{sec:essence} is reminiscent
of Launchbury's $(\leq)$ ordering, although the latter was shown
inadequate in a mechanised correctness proof~\citep[Section 2.3.3]{Breitner:18}.
\citet{Sestoft:97}, evidently inspired by Krivine's by-name
machine~\citep{AgerDanvyMidtgaard:04}, derived a small-step semantics from
Launchbury's semantics, the metatheory of which was very influential for this
work.

\subsubsection*{Denotational Semantics}
Recent work on \emph{Clairvoyant call-by-value}
semantics~\citep{HackettHutton:19} shed light on a useful denotational
interpretation of call-by-need.
Their semantics could be factored in two:
One semantics that non-deterministically either drops let
bindings or evaluates them eagerly, and a downstream $\min$ function that picks
the trace with the least amount of steps that did not ``get stuck''.
The continuity restrictions of the algebraic domain on the semantics necessitate
fusing both functions.
Revisiting the problem with a guarded domain formulation with finite
non-determinism~\citep{Mogelberg:21} might yield interesting perspective.
Remarkably, non-determinism allows the semantics to absolve from the heap and
keep on working by simple substitution, an enviable trait of pure denotational
call-by-value semantics.
On the other hand, the same property means that the trace generated by $\pe$
may not even share a common prefix with the trace generated for $\pe~\px$.
We had trouble thinking about a good way to abstract such a semantics.

\subsubsection*{Control-Flow Analysis}
It is hard to come up with useful control-flow abstractions for higher-order
programs.
\emph{Control-flow analysis}~\citep{Shivers:91} is a seminal technique towards
that end, because not only does it construct a useful control-flow graph(-like)
abstraction, it also allows for trading precision for performance via the depth
$k$ of tracked \emph{contours}, \ie, call contexts.
The constructed control-flow graph can be used to apply classic
intraprocedural data-flow analyses such as constant propagation, dead code
elimination or partial redundancy elimination to the interprocedural setting.
On the other hand, we derived a simple, cheap and useful deadness analysis
$\semusg{\wild}$ for a higher-order language by componentwise abstraction
of the trace-based semantics, without the need to construct an intermediate
control-flow graph first.
The precision-performance trade-off manifests on the spectrum of abstractions
between the incomputable and precise usage abstraction $\usg_\EventD$ and the
rapidly computable but naïve deadness analysis $\semusg{\wild}$.

\subsubsection*{Abstracting Abstract Machines}
Abstracting Abstract Machines~\citep{aam} is an ingenious recipe to derive
a computable \emph{reachable states semantics}~\citep{Cousot:21} from any
small-step semantics.
By bounding the size of the store, the precision-performance trade-off
concentrates in the freely choosably address allocation function, which
necessarily needs to reuse addresses.
Many analyses such as control-flow analysis (and usage analysis) arise as
abstractions of reachable states.
On the other hand, $\semusg{\wild}$ is simple and effective although it does
not reason about a heap at all.
It simply treats a program variable as an alias-free representative of all
its activations, even though there might be multiple activations of the same
variable in the heap already as $i$ in, \eg,
\[
  \Let{f}{\Lam{x}{\Let{i}{\Lam{y}{y}}{i~x~x}}}{f~f}.
\]
$\semusg{\wild}$ is able to correctly deduce that $i$ is evaluated at most once.
We are curious about the precision-performance trade-offs of compositional
analyses such as $\semusg{\wild}$ viz-à-viz abstractions of reachabe states.
Of course, this work has provided no abstraction recipe like AAM by a long shot
-- we leave such explorations for the future.


% Compare to
% - Operational semantics: CESK Felleisen, Launchbury, Sestoft, Krivine
%        eval/apply or push/enter?
%        Given an expr like $f x$, we first push $ρ(x)$ onto the stack and then
%        evaluate $f$, which will look it up (pushing an udpate frame) and evaluate its
%        RHS. Since we will never return to the "eval site" of $f$, IMO this qualifies
%        as push/enter rather than eval/apply. Which is in contrast to what the Krivine
%        paper says, which dubs return states as "apply" transitions
% - AAM/Control-flow analysis
%     * Our approach at some point simply gets rid of the heap. AAM bounds it to finite size.
%       That is inelegant, as it would never allow us to treat variables as a non-aliased representative of their address
%     * AAM
%     * Trace-based CFA MontaguJensen:21
% - Nielson: correctness predicate
% - Clairvoyant blah, Garden of Forking Paths
% - Imprecise exceptions
%        There have been attempts to discern panices from other kinds of loops, such as
%        \citep{imprecise-exceptions}. Unfortunately, in Section 5.3 they find it
%        impossible give non-terminating programs a denotation other than $\bot$, which
%        still encompasses all possible exception behaviors.
% - coinductive big-step \citep{LeroyGrall:09}
% - Interaction trees
%        Mention how >>β= is similar to the bind operator on interaction trees and probably
%        corresponds to Moggi's monadic semantics.
%        Traces are somewhat like the simpler monad `M e a = Good a | Bad e | L |>(M e a)`
%        which combines the delay monad with the error monad
% - Definitional Interpreter work; we are using TCTT as the defining language
% - Pitts chapter in TAPL2, "largest congruence relation"
% - SGDT:
%     * Nakano:00 introduced modality
%     * DreyerAhmedBirkedal:11 refined and applied to step-indexing
%     * Birkedal:12 discovered how to hide step indices. Applied to System F like language with store. Later also depedent type theory. Connection to Kripke worlds
%     * BirkedalMogelbergEjlers:13 first described how to encode guarded recusive types ``syntactically'', e.g., as we use them in meta ::=-notation
%     * gdtt was all about lifting to dependent product types containing the later modality. For example `f <*> t` has to substitute `t` in `f`'s type, solved via delayed substitutions.
%       Important for properties! Hence guaded DEPENDENT type theory
%     * Guarded Cubical Type Theory: Reasoning about equality in GDTT can be undecidable, fix that. But no clock qunatification!
%     * Clocks are ticking: Clock quantification via tick binders which act similar to intervals in Cubical. (Still uses those delayed substitutions!)
%     * TCTT not only introduces ticked cubical, it also covers bisimilarity of guarded labelled transition systems. Our traces are just that, plus a bit more.
%       Also shows how to define a mixed guarded/inductive data type.
%     * Later credits: May solve the unguarded positive occurrence

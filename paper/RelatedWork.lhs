%if style == newcode
> module RelatedWork where
%endif

\section{Related Work}
\label{sec:related-work}

%\subsubsection*{Operational Semantics and Abstract Machines}
%Plotkin's Aarhus lectures~\citep{Plotkin:81} in the late 70's systematically
%introduced small-step operational semantics based on transition systems,
%covering both state and higher-order functions.
%The use of transition systems was novel, in contrast to the then
%prevalent notion of \emph{abstract machine} definitions, such as for the
%\emph{G-machine}~\citet{Johnsson:84} for executing Lazy ML programs.
%\citet{SPJ:92} bridged the gap between graph reduction and transition systems,
%giving an operational semantics for the still widely used \emph{STG machine} for
%lazy evaluation.
%Soon after, \citet{Launchbury:93} gave a big-step operational semantics for
%call-by-need lambda calculus that was drastically simpler than the STG machine
%(and thus simpler to formalise), featuring an explicit heap.
%Our heap forcing relation $(\forcesto)$ from \Cref{sec:essence} is reminiscent
%of Launchbury's $(\leq)$ ordering, although the latter was shown
%inadequate in a mechanised correctness proof~\citep[Section 2.3.3]{Breitner:18}.
%\citet{Sestoft:97}, evidently inspired by Krivine's by-name
%machine~\citep{AgerDanvyMidtgaard:04}, derived a small-step semantics from
%Launchbury's semantics, the metatheory of which was very influential for this
%work.

%        eval/apply or push/enter?
%        Given an expr like $f x$, we first push $ρ(x)$ onto the stack and then
%        evaluate $f$, which will look it up (pushing an udpate frame) and evaluate its
%        RHS. Since we will never return to the "eval site" of $f$, IMO this qualifies
%        as push/enter rather than eval/apply. Which is in contrast to what the Krivine
%        paper says, which dubs return states as "apply" transitions

% - coinductive big-step \citep{LeroyGrall:09}
\subsubsection*{Coinduction and Traces}
% TODO: Consider Capretta:05's argument for using the Delay monad to encode
% partiality
\citet{LeroyGrall:09} show that a coinductive encoding of big-step semantics
is able to encode diverging traces by proving it equivalent to a small-step
semantics.
Their Lemma 10 covers much the same ground as \Cref{thm:semvan-adequate}.
Our use of the $(\betastep)$ operator begs for a Monad instance that has been
explored by \citet{interaction-trees} for a coinductive trace type where the
event and result type are abstracted out as type parameters.

\subsubsection*{Denotational Semantics}
Recent work on \emph{Clairvoyant call-by-value}
semantics~\citep{HackettHutton:19} sheds light on a useful, heapless denotational
interpretation of call-by-need.
Their semantics could be factored in two:
A semantics that non-deterministically drops or eagerly evaluates let
bindings, and a downstream $\min$ function that picks the (non-stuck) trace with
the least amount of steps.
The continuity restrictions of the algebraic domain on the semantics necessitate
fusing both functions.
The trace generated by $\pe$ may not even share a common prefix with the trace
generated for $\pe~\px$.
We had trouble abstracting such a semantics.
It would be interesting to revisit the problem with a guarded domain
formulation such as \citet{Mogelberg:21}.

\subsubsection*{Control-Flow Analysis}
%It is hard to come up with useful control-flow abstractions for higher-order
%programs.
%\emph{Control-flow analysis}~\citep{Shivers:91} is a seminal technique towards
%that end, because not only does it construct a useful control-flow graph(-like)
%abstraction, it also allows for trading precision for performance via the depth
%$k$ of tracked \emph{contours}, \ie, call contexts.
\emph{Control-flow analysis}~\citep{Shivers:91} computes a useful control-flow
graph abstraction for higher-order programs.
Such an approximation is useful to apply classic data-flow analyses such as
constant propagation or dead code elimination to the interprocedural setting.
The contour depth parameter $k$ allows to trade precision for performance,
although practical applications never seem to go beyond $1$.
By comparison, our simple, cheerful usage analysis $\semusg{\wild}$
works without control-flow abstraction.
The precision-performance trade-off ranges between the incomputable
semantic usage abstraction $\usg_\EventD$ and the computable but naïve
usage analysis $\semusg{\wild}$.\\
\citet{MontaguJensen:21} derive control-flow analysis
from small-step traces.
Their chain of abstractions is inspiring and we think that a variant of
our trace-based semantics would be a good fit for their collecting semantics.
Specifically, the semantic inclusions of Lemma 2.10 could be replaced by an
abstraction similar to $\usg_\EventD$, reusing the correctness predicate in
\Cref{fig:semvan-correctness}.

\subsubsection*{Reachable States vs. Lexical Reasoning}
Abstracting Abstract Machines~\citep{aam} is an ingenious recipe to derive
a computable \emph{reachable states semantics}~\citep{Cousot:21} from any
small-step semantics.
By bounding the size of the store, the freely choosably
$\widehat{\mathit{alloc}}$ function embodies the precision-performance trade-off.
Many analyses such as control-flow analysis (and usage analysis) arise as
abstractions of reachable states. \\
It is unclear how $\semusg{\wild}$ fits into this framework, because it does not
reason about a heap at all.
$\semusg{\wild}$ treats a program variable as an alias-free representative of
all its activations, even though there might be multiple activations of the same
variable in the heap already.
Conservative measures need only be taken when a variable escapes its lexical
scope.
In particular, for multiple simultaneously live activations of $i$ in, \eg,
  ${\Let{f}{\Lam{x}{\Let{i}{\Lam{y}{y}}{i~x~x}}}{f~f}}$,
$\semusg{\wild}$ is able to correctly deduce that $i$ is evaluated at most once.
More complex analyses may infer interprocedural cardinality information via
\emph{demand transformers}~\citep{cardinality-ext}, likewise without reasoning
about heaps during analysis.
We intend to connect the notion of demand transformers to transformer
abstraction~\citep{Cousot:21} of $\FunV$ values in the future (\ie, functions
\emph{in the object language}), a connection that has not been possible with
prior semantics. \\
We are curious about the precision-performance trade-offs of lexical analyses
such as $\semusg{\wild}$ viz-à-viz abstractions of reachabe states. Of course,
this work has not provided a tunable abstraction recipe like AAM, so such a
comparison is presently impossible.


% These assumptions could perhaps be encoded in a dependently-typed language
% by formulating |eval| as an open recursive function, refining the type of
% |fun| to |fun :: Σ (D -> D) φ -> D| (we did something similar in
% \Cref{sec:totality} for the Agda encoding) and deriving the free theorem for
% that function.
% The recursion could then be closed with guarded/Löb |fix| after the fact.
% In general, we could do this refinement for all type class operations,
% reflecting ever more information from the definition of |eval| into its type;
% the |φ| would thus enumerate all syntactic use sites of |fun|.
% At this point, the use of parametricity to conclude the soundness proof is not
% too different from writing a custom tactic for a theorem prover.
% Alas, parametricity is hard to use without a tool verifying correct extraction
% of theorems, so we prove the below lemma by hand.
% However, parametricity is a strong argument that our approach can easily be
% generalised to other denotational interpreters.

% TODO: Compare the Speculate and Postpone properties to the frame rule of
% separation logic

% Other topics:
% - Operational semantics: CESK Felleisen, Launchbury, Sestoft, Krivine
% - Nielson: correctness predicate
% - Imprecise exceptions
%        There have been attempts to discern panices from other kinds of loops, such as
%        \citep{imprecise-exceptions}. Unfortunately, in Section 5.3 they find it
%        impossible give non-terminating programs a denotation other than $\bot$, which
%        still encompasses all possible exception behaviors.
% - Definitional Interpreter work; we are using TCTT as the defining language
% - Pitts chapter in TAPL2, "largest congruence relation"
% - SGDT:
%     * Nakano:00 introduced modality
%     * DreyerAhmedBirkedal:11 refined and applied to step-indexing
%     * Birkedal:12 discovered how to hide step indices. Applied to System F like language with store. Later also depedent type theory. Connection to Kripke worlds
%     * BirkedalMogelbergEjlers:13 first described how to encode guarded recusive types ``syntactically'', e.g., as we use them in meta ::=-notation
%     * gdtt was all about lifting to dependent product types containing the later modality. For example `f <*> t` has to substitute `t` in `f`'s type, solved via delayed substitutions.
%       Important for properties! Hence guaded DEPENDENT type theory
%     * Guarded Cubical Type Theory: Reasoning about equality in GDTT can be undecidable, fix that. But no clock qunatification!
%     * Clocks are ticking: Clock quantification via tick binders which act similar to intervals in Cubical. (Still uses those delayed substitutions!)
%     * TCTT (Bisimulation as Path Type for Guarded Recursive Types) not only introduces ticked cubical, it also covers bisimilarity of guarded labelled transition systems. Our traces are just that, plus a bit more.
%       Also shows how to define a mixed guarded/inductive data type.
%     * "Formalizing π-Calculus in Guarded Cubical Agda" is a great introduction to the TCTT implementation in Guarded Cubical Agda
%     * Later credits: May solve the unguarded positive occurrence

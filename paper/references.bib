@inproceedings{Cousot-Cousot:POPL77,
  author = {Cousot, Patrick and Cousot, Radhia},
  title = {Abstract Interpretation: A Unified Lattice Model for Static Analysis of Programs by Construction or Approximation of Fixpoints},
  year = {1977},
  isbn = {9781450373500},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/512950.512973},
  doi = {10.1145/512950.512973},
  abstract = {A program denotes computations in some universe of objects. Abstract interpretation of programs consists in using that denotation to describe computations in another universe of abstract objects, so that the results of abstract execution give some information on the actual computations. An intuitive example (which we borrow from Sintzoff [72]) is the rule of signs. The text -1515 * 17 may be understood to denote computations on the abstract universe {(+), (-), (±)} where the semantics of arithmetic operators is defined by the rule of signs. The abstract execution -1515 * 17 → -(+) * (+) → (-) * (+) → (-), proves that -1515 * 17 is a negative number. Abstract interpretation is concerned by a particular underlying structure of the usual universe of computations (the sign, in our example). It gives a summary of some facets of the actual executions of a program. In general this summary is simple to obtain but inaccurate (e.g. -1515 + 17 → -(+) + (+) → (-) + (+) → (±)). Despite its fundamentally incomplete results abstract interpretation allows the programmer or the compiler to answer questions which do not need full knowledge of program executions or which tolerate an imprecise answer, (e.g. partial correctness proofs of programs ignoring the termination problems, type checking, program optimizations which are not carried in the absence of certainty about their feasibility, …).},
  booktitle = {Proceedings of the 4th ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages},
  pages = {238–252},
  numpages = {15},
  location = {Los Angeles, California},
  series = {POPL '77}
}

@techreport{Scott:81,
  title = {LECTURES ON A MATHEMATICAL THEORY OF COMPUTATION},
  author = {Dana S. Scott},
  year = {1981},
  institution = {Department of Computer Science, University of Oxford},
  number = {PRG19},
  pages = {152},
}

@inproceedings{imprecise-exceptions,
  author = {Peyton Jones, Simon and Reid, Alastair and Henderson, Fergus and Hoare, Tony and Marlow, Simon},
  title = {A Semantics for Imprecise Exceptions},
  year = {1999},
  isbn = {1581130945},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/301618.301637},
  doi = {10.1145/301618.301637},
  abstract = {Some modern superscalar microprocessors provide only imprecise exceptions. That is, they do not guarantee to report the same exception that would be encountered by a straightforward sequential execution of the program. In exchange, they offer increased performance or decreased chip area (which amount to much the same thing).This performance/precision tradeoff has not so far been much explored at the programming language level. In this paper we propose a design for imprecise exceptions in the lazy functional programming language Haskell. We discuss several designs, and conclude that imprecision is essential if the language is still to enjoy its current rich algebra of transformations. We sketch a precise semantics for the language extended with exceptions.The paper shows how to extend Haskell with exceptions without crippling the language or its compilers. We do not yet have enough experience of using the new mechanism to know whether it strikes an appropriate balance between expressiveness and performance.},
  booktitle = {Proceedings of the ACM SIGPLAN 1999 Conference on Programming Language Design and Implementation},
  pages = {25–36},
  numpages = {12},
  location = {Atlanta, Georgia, USA},
  series = {PLDI '99}
}

@book{DaveyPriestley:02,
  place={Cambridge},
  edition={2},
  title={Introduction to Lattices and Order},
  DOI={10.1017/CBO9780511809088},
  publisher={Cambridge University Press},
  author={Davey, B. A. and Priestley, H. A.},
  year={2002}
}

@article{Sestoft:97,
  title={Deriving a lazy abstract machine},
  volume={7},
  DOI={10.1017/S0956796897002712},
  number={3},
  journal={Journal of Functional Programming},
  publisher={Cambridge University Press},
  author={Sestoft, Peter},
  year={1997},
  pages={231–264}
}

@book{Nielson:99,
  author    = {Flemming Nielson and
               Hanne Riis Nielson and
               Chris Hankin},
  title     = {Principles of program analysis},
  publisher = {Springer},
  year      = {1999},
  url       = {https://doi.org/10.1007/978-3-662-03811-6},
  doi       = {10.1007/978-3-662-03811-6},
  isbn      = {978-3-540-65410-0},
  timestamp = {Tue, 16 May 2017 01:00:00 +0200},
  biburl    = {https://dblp.org/rec/books/daglib/0098888.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{Cousot:21,
  title={Principles of Abstract Interpretation},
  author={Cousot, Patrick},
  isbn={9780262044905},
  lccn={2020041256},
  url={https://mitpress.mit.edu/9780262044905/principles-of-abstract-interpretation/},
  year={2021},
  publisher={MIT Press}
}

@techreport{Scott:70,
  title = "Outline of a Mathematical Theory of Computation",
  author = "Dana Scott",
  year = "1970",
  institution = "OUCL",
  month = "November",
  number = "PRG02",
  pages = "30",
}

@techreport{ScottStrachey:71,
  title = "Toward a Mathematical Semantics for Computer Languages",
  author = "Dana Scott and Christopher Strachey",
  year = "1971",
  institution = "OUCL",
  month = "August",
  number = "PRG06",
  pages = "49",
}

@inproceedings{Shivers:91,
  author = {Shivers, Olin},
  title = {The Semantics of Scheme Control-Flow Analysis},
  year = {1991},
  isbn = {0897914333},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/115865.115884},
  doi = {10.1145/115865.115884},
  booktitle = {Proceedings of the 1991 ACM SIGPLAN Symposium on Partial Evaluation and Semantics-Based Program Manipulation},
  pages = {190–198},
  numpages = {9},
  location = {New Haven, Connecticut, USA},
  series = {PEPM '91}
}

@inproceedings{cardinality,
  author = {Sergey, Ilya and Vytiniotis, Dimitrios and Peyton Jones, Simon},
  title = {Modular, Higher-Order Cardinality Analysis in Theory and Practice},
  year = {2014},
  isbn = {9781450325448},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/2535838.2535861},
  doi = {10.1145/2535838.2535861},
  abstract = {Since the mid '80s, compiler writers for functional languages (especially lazy ones) have been writing papers about identifying and exploiting thunks and lambdas that are used only once. However it has proved difficult to achieve both power and simplicity in practice. We describe a new, modular analysis for a higher-order language, which is both simple and effective, and present measurements of its use in a full-scale, state of the art optimising compiler. The analysis finds many single-entry thunks and one-shot lambdas and enables a number of program optimisations.},
  booktitle = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  pages = {335–347},
  numpages = {13},
  keywords = {types and effects, operational semantics, thunks, compilers, functional programming languages, program optimisation, lazy evaluation, haskell, static analysis, cardinality analysis},
  location = {San Diego, California, USA},
  series = {POPL '14}
}

@inproceedings{aam,
  author = {Van Horn, David and Might, Matthew},
  title = {Abstracting Abstract Machines},
  year = {2010},
  isbn = {9781605587943},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/1863543.1863553},
  doi = {10.1145/1863543.1863553},
  abstract = {We describe a derivational approach to abstract interpretation that yields novel and transparently sound static analyses when applied to well-established abstract machines. To demonstrate the technique and support our claim, we transform the CEK machine of Felleisen and Friedman, a lazy variant of Krivine's machine, and the stack-inspecting CM machine of Clements and Felleisen into abstract interpretations of themselves. The resulting analyses bound temporal ordering of program events; predict return-flow and stack-inspection behavior; and approximate the flow and evaluation of by-need parameters. For all of these machines, we find that a series of well-known concrete machine refactorings, plus a technique we call store-allocated continuations, leads to machines that abstract into static analyses simply by bounding their stores. We demonstrate that the technique scales up uniformly to allow static analysis of realistic language features, including tail calls, conditionals, side effects, exceptions, first-class continuations, and even garbage collection.},
  booktitle = {Proceedings of the 15th ACM SIGPLAN International Conference on Functional Programming},
  pages = {51–62},
  numpages = {12},
  keywords = {abstract machines, abstract interpretation},
  location = {Baltimore, Maryland, USA},
  series = {ICFP '10}
}

@inproceedings{Felleisen:87,
  author = {Felleisen, Mattias and Friedman, D. P.},
  title = {A Calculus for Assignments in Higher-Order Languages},
  year = {1987},
  isbn = {0897912152},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/41625.41654},
  doi = {10.1145/41625.41654},
  abstract = {Imperative assignments are abstractions of recurring programming patterns in purely functional programming languages. When added to higher-order functional languages, they provide a higher-level of modularity and security but invalidate the simple substitution semantics. We show that, given an operational interpretation of a denotational semantics for such a language, it is possible to design a two-level extension of the λu-calculus. This calculus provides a location-free rewriting semantics of the language and offers new possibilities for reasoning with assignments. The upper level of the calculus factors out all the steps in a reduction sequence which must be in a linear order; the lower level allows a partial ordering of reduction steps.},
  booktitle = {Proceedings of the 14th ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages},
  pages = {314},
  location = {Munich, West Germany},
  series = {POPL '87}
}

@article{cardinality-ext,
  title={Modular, higher order cardinality analysis in theory and practice},
  volume={27},
  DOI={10.1017/S0956796817000016},
  journal={Journal of Functional Programming},
  publisher={Cambridge University Press},
  author={Sergey, Ilya and Vytiniotis, Dimitrios and Peyton Jones, Simon and Breitner, Joachim},
  year={2017},
  pages={e11}
}

@inproceedings{Ariola:95,
  author = {Ariola, Zena M. and Maraist, John and Odersky, Martin and Felleisen, Matthias and Wadler, Philip},
  title = {A Call-by-Need Lambda Calculus},
  year = {1995},
  isbn = {0897916921},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/199448.199507},
  doi = {10.1145/199448.199507},
  abstract = {The mismatch between the operational semantics of the lambda calculus and the actual behavior of implementations is a major obstacle for compiler writers. They cannot explain the behavior of their evaluator in terms of source level syntax, and they cannot easily compare distinct implementations of different lazy strategies. In this paper we derive an equational characterization of call-by-need and prove it correct with respect to the original lambda calculus. The theory is a strictly smaller theory than the lambda calculus. Immediate applications of the theory concern the correctness proofs of a number of implementation strategies, e.g., the call-by-need continuation passing transformation and the realization of sharing via assignments.},
  booktitle = {Proceedings of the 22nd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  pages = {233–246},
  numpages = {14},
  location = {San Francisco, California, USA},
  series = {POPL '95}
}

@inproceedings{Atkey:18,
  author = {Atkey, Robert},
  title = {Syntax and Semantics of Quantitative Type Theory},
  year = {2018},
  isbn = {9781450355834},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3209108.3209189},
  doi = {10.1145/3209108.3209189},
  abstract = {We present Quantitative Type Theory, a Type Theory that records usage information for each variable in a judgement, based on a previous system by McBride. The usage information is used to give a realizability semantics using a variant of Linear Combinatory Algebras, refining the usual realizability semantics of Type Theory by accurately tracking resource behaviour. We define the semantics in terms of Quantitative Categories with Families, a novel extension of Categories with Families for modelling resource sensitive type theories.},
  booktitle = {Proceedings of the 33rd Annual ACM/IEEE Symposium on Logic in Computer Science},
  pages = {56–65},
  numpages = {10},
  keywords = {Type Theory, Linear Logic},
  location = {Oxford, United Kingdom},
  series = {LICS '18}
}

@inproceedings{Ullrich:19,
  author = {Ullrich, Sebastian and de Moura, Leonardo},
  title = {Counting Immutable Beans: Reference Counting Optimized for Purely Functional Programming},
  year = {2019},
  isbn = {9781450375627},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3412932.3412935},
  doi = {10.1145/3412932.3412935},
  abstract = {Most functional languages rely on some kind of garbage collection for automatic memory management. They usually eschew reference counting in favor of a tracing garbage collector, which has less bookkeeping overhead at runtime. On the other hand, having an exact reference count of each value can enable optimizations such as destructive updates. We explore these optimization opportunities in the context of an eager, purely functional programming language. We propose a new mechanism for efficiently reclaiming memory used by nonshared values, reducing stress on the global memory allocator. We describe an approach for minimizing the number of reference counts updates using borrowed references and a heuristic for automatically inferring borrow annotations. We implemented all these techniques in a new compiler for an eager and purely functional programming language with support for multi-threading. Our preliminary experimental results demonstrate our approach is competitive and often outperforms state-of-the-art compilers.},
  booktitle = {Proceedings of the 31st Symposium on Implementation and Application of Functional Languages},
  articleno = {3},
  numpages = {12},
  keywords = {lean, purely functional programming, reference counting},
  location = {Singapore, Singapore},
  series = {IFL '19}
}

@inproceedings{perceus,
author = {Reinking, Alex and Xie, Ningning and de Moura, Leonardo and Leijen, Daan},
title = {Perceus: Garbage Free Reference Counting with Reuse},
year = {2021},
isbn = {9781450383912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453483.3454032},
doi = {10.1145/3453483.3454032},
abstract = {We introduce Perceus, an algorithm for precise reference counting with reuse and specialization. Starting from a functional core language with explicit control-flow, Perceus emits precise reference counting instructions such that (cycle-free) programs are _garbage free_, where only live references are retained. This enables further optimizations, like reuse analysis that allows for guaranteed in-place updates at runtime. This in turn enables a novel programming paradigm that we call _functional but in-place_ (FBIP). Much like tail-call optimization enables writing loops with regular function calls, reuse analysis enables writing in-place mutating algorithms in a purely functional way. We give a novel formalization of reference counting in a linear resource calculus, and prove that Perceus is sound and garbage free. We show evidence that Perceus, as implemented in Koka, has good performance and is competitive with other state-of-the-art memory collectors.},
booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
pages = {96–111},
numpages = {16},
keywords = {Reference Counting, Handlers, Algebraic Effects},
location = {Virtual, Canada},
series = {PLDI 2021}
}

@article{Lorenzen:22,
  author = {Lorenzen, Anton and Leijen, Daan},
  title = {Reference Counting with Frame Limited Reuse},
  year = {2022},
  issue_date = {August 2022},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {6},
  number = {ICFP},
  url = {https://doi.org/10.1145/3547634},
  doi = {10.1145/3547634},
  abstract = {The recently introduced _Perceus_ algorithm can automatically insert reference count instructions such that the resulting (cycle-free) program is _garbage free_: objects are freed at the very moment they can no longer be referenced. An important extension is reuse analysis. This optimization pairs objects of known size with fresh allocations of the same size and tries to reuse the object in-place at runtime if it happens to be unique. Unfortunately, current implementations of reuse analysis are fragile with respect to small program transformations, or can cause an arbitrary increase in the peak heap usage. We present a novel _drop-guided_ reuse algorithm that is simpler and more robust than previous approaches. Moreover, we generalize the linear resource calculus to precisely characterize garbage-free and frame-limited evaluations. On each function call, a frame-limited evaluation may hold on to memory longer if the size is bounded by a constant factor. Using this framework we show that our drop-guided reuse _is_ frame-limited and find that an implementation of our new reuse approach in Koka can provide significant speedups.},
  journal = {Proc. ACM Program. Lang.},
  month = {aug},
  articleno = {103},
  numpages = {24},
  keywords = {Koka, Reuse, Frame Limited, Reference Counting}
}

@article{Cartwright:16,
  author    = {Robert Cartwright and
               Rebecca Parsons and
               Moez A. AbdelGawad},
  title     = {Domain Theory: An Introduction},
  journal   = {CoRR},
  volume    = {abs/1605.05858},
  year      = {2016},
  url       = {http://arxiv.org/abs/1605.05858},
  eprinttype = {arXiv},
  eprint    = {1605.05858},
  timestamp = {Mon, 13 Aug 2018 16:46:41 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/CartwrightPA16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Launchbury:93,
  author = {Launchbury, John},
  title = {A Natural Semantics for Lazy Evaluation},
  year = {1993},
  isbn = {0897915607},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/158511.158618},
  doi = {10.1145/158511.158618},
  abstract = {We define an operational semantics for lazy evaluation which provides an accurate model for sharing. The only computational structure we introduce is a set of bindings which corresponds closely to a heap. The semantics is set at a considerably higher level of abstraction than operational semantics for particular abstract machines, so is more suitable for a variety of proofs. Furthermore, because a heap is explicitly modelled, the semantics provides a suitable framework for studies about space behaviour of terms under lazy evaluation.},
  booktitle = {Proceedings of the 20th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  pages = {144–154},
  numpages = {11},
  location = {Charleston, South Carolina, USA},
  series = {POPL '93}
}

@article{AgerDanvyMidtgaard:04,
  title = {A functional correspondence between call-by-need evaluators and lazy abstract machines},
  journal = {Information Processing Letters},
  volume = {90},
  number = {5},
  pages = {223-232},
  year = {2004},
  issn = {0020-0190},
  doi = {https://doi.org/10.1016/j.ipl.2004.02.012},
  url = {https://www.sciencedirect.com/science/article/pii/S0020019004000638},
  author = {Mads Sig Ager and Olivier Danvy and Jan Midtgaard},
  keywords = {Functional programming, Program derivation, Interpreters, Abstract machines, Closure conversion, CPS transformation, Defunctionalization},
  abstract = {We bridge the gap between compositional evaluators and abstract machines for the lambda-calculus, using closure conversion, transformation into continuation-passing style, and defunctionalization of continuations. This article is a followup of our article at PPDP 2003, where we consider call by name and call by value. Here, however, we consider call by need. We derive a lazy abstract machine from an ordinary call-by-need evaluator that threads a heap of updatable cells. In this resulting abstract machine, the continuation fragment for updating a heap cell naturally appears as an ‘update marker’, an implementation technique that was invented for the Three Instruction Machine and subsequently used to construct lazy variants of Krivine's abstract machine. Tuning the evaluator leads to other implementation techniques such as unboxed values. The correctness of the resulting abstract machines is a corollary of the correctness of the original evaluators and of the program transformations used in the derivation.}
}

@article{Milner:78,
  title = {A theory of type polymorphism in programming},
  journal = {Journal of Computer and System Sciences},
  volume = {17},
  number = {3},
  pages = {348-375},
  year = {1978},
  issn = {0022-0000},
  doi = {https://doi.org/10.1016/0022-0000(78)90014-4},
  url = {https://www.sciencedirect.com/science/article/pii/0022000078900144},
  author = {Robin Milner},
  abstract = {The aim of this work is largely a practical one. A widely employed style of programming, particularly in structure-processing languages which impose no discipline of types, entails defining procedures which work well on objects of a wide variety. We present a formal type discipline for such polymorphic procedures in the context of a simple programming language, and a compile time type-checking algorithm W which enforces the discipline. A Semantic Soundness Theorem (based on a formal semantics for the language) states that well-type programs cannot “go wrong” and a Syntactic Soundness Theorem states that if W accepts a program then it is well typed. We also discuss extending these results to richer languages; a type-checking algorithm based on W is in fact already implemented and working, for the metalanguage ML in the Edinburgh LCF system.}
}

@misc{Czajka:2019,
  title={Coinduction: an elementary approach},
  author={Łukasz Czajka},
  year={2019},
  eprint={1501.04354},
  archivePrefix={arXiv},
  primaryClass={cs.LO}
}

@article{Plotkin:77,
  title = {LCF considered as a programming language},
  journal = {Theoretical Computer Science},
  volume = {5},
  number = {3},
  pages = {223-255},
  year = {1977},
  issn = {0304-3975},
  doi = {https://doi.org/10.1016/0304-3975(77)90044-5},
  url = {https://www.sciencedirect.com/science/article/pii/0304397577900445},
  author = {G.D. Plotkin},
  abstract = {The paper studies connections between denotational and operational semantics for a simple programming language based on LCF. It begins with the connection between the behaviour of a program and its denotation. It turns out that a program denotes ⊥ in any of several possible semantics if it does not terminate. From this it follows that if two terms have the same denotation in one of these semantics, they have the same behaviour in all contexts. The converse fails for all the semantics. If, however, the language is extended to allow certain parallel facilities behavioural equivalence does coincide with denotational equivalence in one of the semantics considered, which may therefore be called “fully abstract”. Next a connection is given which actually determines the semantics up to isomorphism from the behaviour alone. Conversely, by allowing further parallel facilities, every r.e. element of the fully abstract semantics becomes definable, thus characterising the programming language, up to interdefinability, from the set of r.e. elements of the domains of the semantics.}
}

@article{LeroyGrall:09,
  title = {Coinductive big-step operational semantics},
  journal = {Information and Computation},
  volume = {207},
  number = {2},
  pages = {284-304},
  year = {2009},
  note = {Special issue on Structural Operational Semantics (SOS)},
  issn = {0890-5401},
  doi = {https://doi.org/10.1016/j.ic.2007.12.004},
  url = {https://www.sciencedirect.com/science/article/pii/S0890540108001296},
  author = {Xavier Leroy and Hervé Grall},
  keywords = {Coinduction, Operational semantics, Big-step semantics, Natural semantics, Small-step semantics, Reduction semantics, Type soundness, Compiler correctness, Mechanized proofs, The Coq proof assistant},
  abstract = {Using a call-by-value functional language as an example, this article illustrates the use of coinductive definitions and proofs in big-step operational semantics, enabling it to describe diverging evaluations in addition to terminating evaluations. We formalize the connections between the coinductive big-step semantics and the standard small-step semantics, proving that both semantics are equivalent. We then study the use of coinductive big-step semantics in proofs of type soundness and proofs of semantic preservation for compilers. A methodological originality of this paper is that all results have been proved using the Coq proof assistant. We explain the proof-theoretic presentation of coinductive definitions and proofs offered by Coq, and show that it facilitates the discovery and the presentation of the results.}
}

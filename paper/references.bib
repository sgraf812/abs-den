@inproceedings{imprecise-exceptions,
  author = {Peyton Jones, Simon and Reid, Alastair and Henderson, Fergus and Hoare, Tony and Marlow, Simon},
  title = {A Semantics for Imprecise Exceptions},
  year = {1999},
  isbn = {1581130945},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/301618.301637},
  doi = {10.1145/301618.301637},
  abstract = {Some modern superscalar microprocessors provide only imprecise exceptions. That is, they do not guarantee to report the same exception that would be encountered by a straightforward sequential execution of the program. In exchange, they offer increased performance or decreased chip area (which amount to much the same thing).This performance/precision tradeoff has not so far been much explored at the programming language level. In this paper we propose a design for imprecise exceptions in the lazy functional programming language Haskell. We discuss several designs, and conclude that imprecision is essential if the language is still to enjoy its current rich algebra of transformations. We sketch a precise semantics for the language extended with exceptions.The paper shows how to extend Haskell with exceptions without crippling the language or its compilers. We do not yet have enough experience of using the new mechanism to know whether it strikes an appropriate balance between expressiveness and performance.},
  booktitle = {Proceedings of the ACM SIGPLAN 1999 Conference on Programming Language Design and Implementation},
  pages = {25–36},
  numpages = {12},
  location = {Atlanta, Georgia, USA},
  series = {PLDI '99}
}

@article{Sestoft:97,
  title={Deriving a lazy abstract machine},
  volume={7},
  DOI={10.1017/S0956796897002712},
  number={3},
  journal={Journal of Functional Programming},
  publisher={Cambridge University Press},
  author={Sestoft, Peter},
  year={1997},
  pages={231–264}
}

@book{Nielson:99,
  author    = {Flemming Nielson and
               Hanne Riis Nielson and
               Chris Hankin},
  title     = {Principles of program analysis},
  publisher = {Springer},
  year      = {1999},
  url       = {https://doi.org/10.1007/978-3-662-03811-6},
  doi       = {10.1007/978-3-662-03811-6},
  isbn      = {978-3-540-65410-0},
  timestamp = {Tue, 16 May 2017 01:00:00 +0200},
  biburl    = {https://dblp.org/rec/books/daglib/0098888.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Cousot:77,
  author = {Cousot, Patrick and Cousot, Radhia},
  title = {Abstract interpretation: a unified lattice model for static analysis of programs by construction or approximation of fixpoints},
  year = {1977},
  isbn = {9781450373500},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/512950.512973},
  doi = {10.1145/512950.512973},
  abstract = {A program denotes computations in some universe of objects. Abstract interpretation of programs consists in using that denotation to describe computations in another universe of abstract objects, so that the results of abstract execution give some information on the actual computations. An intuitive example (which we borrow from Sintzoff [72]) is the rule of signs. The text -1515 * 17 may be understood to denote computations on the abstract universe {(+), (-), (±)} where the semantics of arithmetic operators is defined by the rule of signs. The abstract execution -1515 * 17 → -(+) * (+) → (-) * (+) → (-), proves that -1515 * 17 is a negative number. Abstract interpretation is concerned by a particular underlying structure of the usual universe of computations (the sign, in our example). It gives a summary of some facets of the actual executions of a program. In general this summary is simple to obtain but inaccurate (e.g. -1515 + 17 → -(+) + (+) → (-) + (+) → (±)). Despite its fundamentally incomplete results abstract interpretation allows the programmer or the compiler to answer questions which do not need full knowledge of program executions or which tolerate an imprecise answer, (e.g. partial correctness proofs of programs ignoring the termination problems, type checking, program optimizations which are not carried in the absence of certainty about their feasibility, …).},
  booktitle = {Proceedings of the 4th ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages},
  pages = {238–252},
  numpages = {15},
  location = {Los Angeles, California},
  series = {POPL '77}
}

@book{Cousot:21,
  title={Principles of Abstract Interpretation},
  author={Cousot, Patrick},
  isbn={9780262044905},
  lccn={2020041256},
  url={https://mitpress.mit.edu/9780262044905/principles-of-abstract-interpretation/},
  year={2021},
  publisher={MIT Press}
}

@techreport{Scott:70,
  title = "Outline of a Mathematical Theory of Computation",
  author = "Dana Scott",
  year = "1970",
  institution = "OUCL",
  month = "November",
  number = "PRG02",
  pages = "30",
}

@techreport{ScottStrachey:71,
  title = "Toward a Mathematical Semantics for Computer Languages",
  author = "Dana Scott and Christopher Strachey",
  year = "1971",
  institution = "OUCL",
  month = "August",
  number = "PRG06",
  pages = "49",
}

@inproceedings{Shivers:91,
  Author = {Shivers, Olin Grigsby},
  Title = {Control-Flow Analysis of Higher-Order Languages or Taming Lambda},
  School = {Carnige-Mellon Univeristy},
  Month = {May},
  Year = {1991}
}

@inproceedings{aam,
  author = {Van Horn, David and Might, Matthew},
  title = {Abstracting Abstract Machines},
  year = {2010},
  isbn = {9781605587943},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/1863543.1863553},
  doi = {10.1145/1863543.1863553},
  abstract = {We describe a derivational approach to abstract interpretation that yields novel and transparently sound static analyses when applied to well-established abstract machines. To demonstrate the technique and support our claim, we transform the CEK machine of Felleisen and Friedman, a lazy variant of Krivine's machine, and the stack-inspecting CM machine of Clements and Felleisen into abstract interpretations of themselves. The resulting analyses bound temporal ordering of program events; predict return-flow and stack-inspection behavior; and approximate the flow and evaluation of by-need parameters. For all of these machines, we find that a series of well-known concrete machine refactorings, plus a technique we call store-allocated continuations, leads to machines that abstract into static analyses simply by bounding their stores. We demonstrate that the technique scales up uniformly to allow static analysis of realistic language features, including tail calls, conditionals, side effects, exceptions, first-class continuations, and even garbage collection.},
  booktitle = {Proceedings of the 15th ACM SIGPLAN International Conference on Functional Programming},
  pages = {51–62},
  numpages = {12},
  keywords = {abstract machines, abstract interpretation},
  location = {Baltimore, Maryland, USA},
  series = {ICFP '10}
}

@inproceedings{Sergey:13,
  author = {Sergey, Ilya and Devriese, Dominique and Might, Matthew and Midtgaard, Jan and Darais, David and Clarke, Dave and Piessens, Frank},
  title = {Monadic abstract interpreters},
  year = {2013},
  isbn = {9781450320146},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/2491956.2491979},
  doi = {10.1145/2491956.2491979},
  abstract = {Recent developments in the systematic construction of abstract interpreters hinted at the possibility of a broad unification of concepts in static analysis. We deliver that unification by showing context-sensitivity, polyvariance, flow-sensitivity, reachability-pruning, heap-cloning and cardinality-bounding to be independent of any particular semantics. Monads become the unifying agent between these concepts and between semantics. For instance, by plugging the same "context-insensitivity monad" into a monadically-parameterized semantics for Java or for the lambda calculus, it yields the expected context-insensitive analysis.To achieve this unification, we develop a systematic method for transforming a concrete semantics into a monadically-parameterized abstract machine. Changing the monad changes the behavior of the machine. By changing the monad, we recover a spectrum of machines---from the original concrete semantics to a monovariant, flow- and context-insensitive static analysis with a singly-threaded heap and weak updates.The monadic parameterization also suggests an abstraction over the ubiquitous monotone fixed-point computation found in static analysis. This abstraction makes it straightforward to instrument an analysis with high-level strategies for improving precision and performance, such as abstract garbage collection and widening.While the paper itself runs the development for continuation-passing style, our generic implementation replays it for direct-style lambda-calculus and Featherweight Java to support generality.},
  booktitle = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation},
  pages = {399–410},
  numpages = {12},
  keywords = {abstract garbage collection, abstract interpretation, abstract machines, collecting semantics, interpreters, monads, operational semantics},
  location = {Seattle, Washington, USA},
  series = {PLDI '13}
}

@inproceedings{Darais:15,
  author = {Darais, David and Might, Matthew and Van Horn, David},
  title = {Galois transformers and modular abstract interpreters: reusable metatheory for program analysis},
  year = {2015},
  isbn = {9781450336895},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/2814270.2814308},
  doi = {10.1145/2814270.2814308},
  abstract = {The design and implementation of static analyzers has become increasingly systematic. Yet for a given language or analysis feature, it often requires tedious and error prone work to implement an analyzer and prove it sound. In short, static analysis features and their proofs of soundness do not compose well, causing a dearth of reuse in both implementation and metatheory. We solve the problem of systematically constructing static analyzers by introducing Galois transformers: monad transformers that transport Galois connection properties. In concert with a monadic interpreter, we define a library of monad transformers that implement building blocks for classic analysis parameters like context, path, and heap (in)sensitivity. Moreover, these can be composed together independent of the language being analyzed. Significantly, a Galois transformer can be proved sound once and for all, making it a reusable analysis component. As new analysis features and abstractions are developed and mixed in, soundness proofs need not be reconstructed, as the composition of a monad transformer stack is sound by virtue of its constituents. Galois transformers provide a viable foundation for reusable and composable metatheory for program analysis. Finally, these Galois transformers shift the level of abstraction in analysis design and implementation to a level where non-specialists have the ability to synthesize sound analyzers over a number of parameters.},
  booktitle = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
  pages = {552–571},
  numpages = {20},
  keywords = {Galois connections, abstract interpretation, monads, program analysis},
  location = {Pittsburgh, PA, USA},
  series = {OOPSLA 2015}
}

@article{adi,
  author = {Darais, David and Labich, Nicholas and Nguyen, Ph\'{u}c C. and Van Horn, David},
  title = {Abstracting Definitional Interpreters (Functional Pearl)},
  year = {2017},
  issue_date = {September 2017},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {1},
  number = {ICFP},
  url = {https://doi.org/10.1145/3110256},
  doi = {10.1145/3110256},
  abstract = {In this functional pearl, we examine the use of definitional interpreters as a basis for abstract interpretation of higher-order programming languages. As it turns out, definitional interpreters, especially those written in monadic style, can provide a nice basis for a wide variety of collecting semantics, abstract interpretations, symbolic executions, and their intermixings. But the real insight of this story is a replaying of an insight from Reynold's landmark paper, Definitional Interpreters for Higher-Order Programming Languages, in which he observes definitional interpreters enable the defined-language to inherit properties of the defining-language. We show the same holds true for definitional abstract interpreters. Remarkably, we observe that abstract definitional interpreters can inherit the so-called “pushdown control flow” property, wherein function calls and returns are precisely matched in the abstract semantics, simply by virtue of the function call mechanism of the defining-language. The first approaches to achieve this property for higher-order languages appeared within the last ten years, and have since been the subject of many papers. These approaches start from a state-machine semantics and uniformly involve significant technical engineering to recover the precision of pushdown control flow. In contrast, starting from a definitional interpreter, the pushdown control flow property is inherent in the meta-language and requires no further technical mechanism to achieve.},
  journal = {Proc. ACM Program. Lang.},
  month = {aug},
  articleno = {12},
  numpages = {25},
  keywords = {abstract interpreters, interpreters}
}

@inproceedings{Felleisen:87,
  author = {Felleisen, Mattias and Friedman, D. P.},
  title = {A Calculus for Assignments in Higher-Order Languages},
  year = {1987},
  isbn = {0897912152},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/41625.41654},
  doi = {10.1145/41625.41654},
  abstract = {Imperative assignments are abstractions of recurring programming patterns in purely functional programming languages. When added to higher-order functional languages, they provide a higher-level of modularity and security but invalidate the simple substitution semantics. We show that, given an operational interpretation of a denotational semantics for such a language, it is possible to design a two-level extension of the λu-calculus. This calculus provides a location-free rewriting semantics of the language and offers new possibilities for reasoning with assignments. The upper level of the calculus factors out all the steps in a reduction sequence which must be in a linear order; the lower level allows a partial ordering of reduction steps.},
  booktitle = {Proceedings of the 14th ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages},
  pages = {314},
  location = {Munich, West Germany},
  series = {POPL '87}
}

@article{Sergey:14,
  title={Modular, higher order cardinality analysis in theory and practice},
  volume={27},
  DOI={10.1017/S0956796817000016},
  journal={Journal of Functional Programming},
  publisher={Cambridge University Press},
  author={Sergey, Ilya and Vytiniotis, Dimitrios and Peyton Jones, Simon and Breitner, Joachim},
  year={2017},
  pages={e11}
}

@inproceedings{Ariola:95,
  author = {Ariola, Zena M. and Maraist, John and Odersky, Martin and Felleisen, Matthias and Wadler, Philip},
  title = {A Call-by-Need Lambda Calculus},
  year = {1995},
  isbn = {0897916921},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/199448.199507},
  doi = {10.1145/199448.199507},
  abstract = {The mismatch between the operational semantics of the lambda calculus and the actual behavior of implementations is a major obstacle for compiler writers. They cannot explain the behavior of their evaluator in terms of source level syntax, and they cannot easily compare distinct implementations of different lazy strategies. In this paper we derive an equational characterization of call-by-need and prove it correct with respect to the original lambda calculus. The theory is a strictly smaller theory than the lambda calculus. Immediate applications of the theory concern the correctness proofs of a number of implementation strategies, e.g., the call-by-need continuation passing transformation and the realization of sharing via assignments.},
  booktitle = {Proceedings of the 22nd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  pages = {233–246},
  numpages = {14},
  location = {San Francisco, California, USA},
  series = {POPL '95}
}

@inproceedings{Atkey:18,
  author = {Atkey, Robert},
  title = {Syntax and Semantics of Quantitative Type Theory},
  year = {2018},
  isbn = {9781450355834},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3209108.3209189},
  doi = {10.1145/3209108.3209189},
  abstract = {We present Quantitative Type Theory, a Type Theory that records usage information for each variable in a judgement, based on a previous system by McBride. The usage information is used to give a realizability semantics using a variant of Linear Combinatory Algebras, refining the usual realizability semantics of Type Theory by accurately tracking resource behaviour. We define the semantics in terms of Quantitative Categories with Families, a novel extension of Categories with Families for modelling resource sensitive type theories.},
  booktitle = {Proceedings of the 33rd Annual ACM/IEEE Symposium on Logic in Computer Science},
  pages = {56–65},
  numpages = {10},
  keywords = {Type Theory, Linear Logic},
  location = {Oxford, United Kingdom},
  series = {LICS '18}
}

@inproceedings{Launchbury:93,
  author = {Launchbury, John},
  title = {A Natural Semantics for Lazy Evaluation},
  year = {1993},
  isbn = {0897915607},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/158511.158618},
  doi = {10.1145/158511.158618},
  abstract = {We define an operational semantics for lazy evaluation which provides an accurate model for sharing. The only computational structure we introduce is a set of bindings which corresponds closely to a heap. The semantics is set at a considerably higher level of abstraction than operational semantics for particular abstract machines, so is more suitable for a variety of proofs. Furthermore, because a heap is explicitly modelled, the semantics provides a suitable framework for studies about space behaviour of terms under lazy evaluation.},
  booktitle = {Proceedings of the 20th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  pages = {144–154},
  numpages = {11},
  location = {Charleston, South Carolina, USA},
  series = {POPL '93}
}

@article{Krivine:07,
  author       = {Jean{-}Louis Krivine},
  title        = {A call-by-name lambda-calculus machine},
  journal      = {High. Order Symb. Comput.},
  volume       = {20},
  number       = {3},
  pages        = {199--207},
  year         = {2007},
  url          = {https://doi.org/10.1007/s10990-007-9018-9},
  doi          = {10.1007/S10990-007-9018-9},
  timestamp    = {Thu, 05 Mar 2020 12:05:01 +0100},
  biburl       = {https://dblp.org/rec/journals/lisp/Krivine07.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{AgerDanvyMidtgaard:04,
  title = {A functional correspondence between call-by-need evaluators and lazy abstract machines},
  journal = {Information Processing Letters},
  volume = {90},
  number = {5},
  pages = {223-232},
  year = {2004},
  issn = {0020-0190},
  doi = {https://doi.org/10.1016/j.ipl.2004.02.012},
  url = {https://www.sciencedirect.com/science/article/pii/S0020019004000638},
  author = {Mads Sig Ager and Olivier Danvy and Jan Midtgaard},
  keywords = {Functional programming, Program derivation, Interpreters, Abstract machines, Closure conversion, CPS transformation, Defunctionalization},
  abstract = {We bridge the gap between compositional evaluators and abstract machines for the lambda-calculus, using closure conversion, transformation into continuation-passing style, and defunctionalization of continuations. This article is a followup of our article at PPDP 2003, where we consider call by name and call by value. Here, however, we consider call by need. We derive a lazy abstract machine from an ordinary call-by-need evaluator that threads a heap of updatable cells. In this resulting abstract machine, the continuation fragment for updating a heap cell naturally appears as an ‘update marker’, an implementation technique that was invented for the Three Instruction Machine and subsequently used to construct lazy variants of Krivine's abstract machine. Tuning the evaluator leads to other implementation techniques such as unboxed values. The correctness of the resulting abstract machines is a corollary of the correctness of the original evaluators and of the program transformations used in the derivation.}
}

@article{Milner:78,
  title = {A theory of type polymorphism in programming},
  journal = {Journal of Computer and System Sciences},
  volume = {17},
  number = {3},
  pages = {348-375},
  year = {1978},
  issn = {0022-0000},
  doi = {https://doi.org/10.1016/0022-0000(78)90014-4},
  url = {https://www.sciencedirect.com/science/article/pii/0022000078900144},
  author = {Robin Milner},
  abstract = {The aim of this work is largely a practical one. A widely employed style of programming, particularly in structure-processing languages which impose no discipline of types, entails defining procedures which work well on objects of a wide variety. We present a formal type discipline for such polymorphic procedures in the context of a simple programming language, and a compile time type-checking algorithm W which enforces the discipline. A Semantic Soundness Theorem (based on a formal semantics for the language) states that well-type programs cannot “go wrong” and a Syntactic Soundness Theorem states that if W accepts a program then it is well typed. We also discuss extending these results to richer languages; a type-checking algorithm based on W is in fact already implemented and working, for the metalanguage ML in the Edinburgh LCF system.}
}

@article{Plotkin:77,
  title = {LCF considered as a programming language},
  journal = {Theoretical Computer Science},
  volume = {5},
  number = {3},
  pages = {223-255},
  year = {1977},
  issn = {0304-3975},
  doi = {https://doi.org/10.1016/0304-3975(77)90044-5},
  url = {https://www.sciencedirect.com/science/article/pii/0304397577900445},
  author = {Plotkin, Gordon D.},
  abstract = {The paper studies connections between denotational and operational semantics for a simple programming language based on LCF. It begins with the connection between the behaviour of a program and its denotation. It turns out that a program denotes ⊥ in any of several possible semantics if it does not terminate. From this it follows that if two terms have the same denotation in one of these semantics, they have the same behaviour in all contexts. The converse fails for all the semantics. If, however, the language is extended to allow certain parallel facilities behavioural equivalence does coincide with denotational equivalence in one of the semantics considered, which may therefore be called “fully abstract”. Next a connection is given which actually determines the semantics up to isomorphism from the behaviour alone. Conversely, by allowing further parallel facilities, every r.e. element of the fully abstract semantics becomes definable, thus characterising the programming language, up to interdefinability, from the set of r.e. elements of the domains of the semantics.}
}

@article{Plotkin:81,
    title = {A structural approach to operational semantics},
    volume = {60-61},
    issn = {15678326},
    url = {https://linkinghub.elsevier.com/retrieve/pii/S1567832604000402},
    doi = {10.1016/j.jlap.2004.05.001},
    journal = {The Journal of Logic and Algebraic Programming},
    author = {Plotkin, Gordon D.},
    year = {2004},
    pages = {17--139}
}

@inproceedings{Johnsson:84,
  author = {Johnsson, Thomas},
  title = {Efficient Compilation of Lazy Evaluation},
  year = {1984},
  isbn = {0897911393},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/502874.502880},
  doi = {10.1145/502874.502880},
  abstract = {This paper describes the principles underlying an efficient implementation of a lazy functional language, compiling to code for ordinary computers. It is based on combinator-like graph reduction: the user defined functions are used as rewrite rules in the graph. Each function is compiled into an instruction sequence for an abstract graph reduction machine, called the G-machine, the code reduces a function application graph to its value. The G-machine instructions are then translated into target code. Speed improvements by almost two orders of magnitude over previous lazy evaluators have been measured; we provide some performance figures.},
  booktitle = {Proceedings of the 1984 SIGPLAN Symposium on Compiler Construction},
  pages = {58–69},
  numpages = {12},
  location = {Montreal, Canada},
  series = {SIGPLAN '84}
}

@article{SPJ:92,
  author = {Peyton Jones, Simon},
  title = {Implementing Lazy Functional Languages on Stock Hardware: The Spineless Tagless G-machine},
  year = {1992},
  month = {July},
  abstract = {The Spineless Tagless G-machine is an abstract machine designed to support non- strict higher-order functional languages. This presentation of the machine falls into three parts. Firstly, we give a general discussion of the design issues involved in implementing non-strict functional languages.
  Next, we present the STG language, an austere but recognisably-functional language, which as well as a denotational meaning has a well-defined operational semantics. The STG language is the \abstract machine code" for the Spineless Tagless G-machine.
  Lastly, we discuss the mapping of the STG language onto stock hardware. The success of an abstract machine model depends largely on how efficient this mapping can be made, though this topic is often relegated to a short section. Instead, we give a detailed discussion of the design issues and the choices we have made. Our principal target is the C language, treating the C compiler as a portable assembler.},
  publisher = {Cambridge University Press},
  url = {https://www.microsoft.com/en-us/research/publication/implementing-lazy-functional-languages-on-stock-hardware-the-spineless-tagless-g-machine/},
  pages = {127-202},
  journal = {Journal of Functional Programming},
  volume = {2},
  edition = {Journal of Functional Programming},
}

@article{LeroyGrall:09,
  title = {Coinductive big-step operational semantics},
  journal = {Information and Computation},
  volume = {207},
  number = {2},
  pages = {284-304},
  year = {2009},
  note = {Special issue on Structural Operational Semantics (SOS)},
  issn = {0890-5401},
  doi = {https://doi.org/10.1016/j.ic.2007.12.004},
  url = {https://www.sciencedirect.com/science/article/pii/S0890540108001296},
  author = {Xavier Leroy and Hervé Grall},
  keywords = {Coinduction, Operational semantics, Big-step semantics, Natural semantics, Small-step semantics, Reduction semantics, Type soundness, Compiler correctness, Mechanized proofs, The Coq proof assistant},
  abstract = {Using a call-by-value functional language as an example, this article illustrates the use of coinductive definitions and proofs in big-step operational semantics, enabling it to describe diverging evaluations in addition to terminating evaluations. We formalize the connections between the coinductive big-step semantics and the standard small-step semantics, proving that both semantics are equivalent. We then study the use of coinductive big-step semantics in proofs of type soundness and proofs of semantic preservation for compilers. A methodological originality of this paper is that all results have been proved using the Coq proof assistant. We explain the proof-theoretic presentation of coinductive definitions and proofs offered by Coq, and show that it facilitates the discovery and the presentation of the results.}
}

@article{WrightFelleisen:94,
  title = {A Syntactic Approach to Type Soundness},
  journal = {Information and Computation},
  volume = {115},
  number = {1},
  pages = {38-94},
  year = {1994},
  issn = {0890-5401},
  doi = {https://doi.org/10.1006/inco.1994.1093},
  url = {https://www.sciencedirect.com/science/article/pii/S0890540184710935},
  author = {A.K. Wright and M. Felleisen},
  abstract = {We present a new approach to proving type soundness for Hindley/Milner-style polymorphic type systems. The keys to our approach are (1) an adaptation of subject reduction theorems from combinatory logic to programming languages, and (2) the use of rewriting techniques for the specification of the language semantics. The approach easily extends from polymorphic functional languages to imperative languages that provide references, exceptions, continuations, and similar features. We illustrate the technique with a type soundness theorem for the core of Standard ML, which includes the first type soundness proof for polymorphic exceptions and continuations.}
}

@article{AppelMcAllester:01,
  author = {Appel, Andrew W. and McAllester, David},
  title = {An Indexed Model of Recursive Types for Foundational Proof-Carrying Code},
  year = {2001},
  issue_date = {September 2001},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {23},
  number = {5},
  issn = {0164-0925},
  url = {https://doi.org/10.1145/504709.504712},
  doi = {10.1145/504709.504712},
  abstract = {The proofs of "traditional" proof carrying code (PCC) are type-specialized in the sense that they require axioms about a specific type system. In contrast, the proofs of foundational PCC explicitly define all required types and explicitly prove all the required properties of those types assuming only a fixed foundation of mathematics such as higher-order logic. Foundational PCC is both more flexible and more secure than type-specialized PCC.For foundational PCC we need semantic models of type systems on von Neumann machines. Previous models have been either too weak (lacking general recursive types and first-class function-pointers), too complex (requiring machine-checkable proofs of large bodies of computability theory), or not obviously applicable to von Neumann machines. Our new model is strong, simple, and works either in λ-calculus or on Pentiums.},
  journal = {ACM Trans. Program. Lang. Syst.},
  month = {sep},
  pages = {657–683},
  numpages = {27}
}

@article{DreyerAhmedBirkedal:11,
  author       = {Derek Dreyer and
                  Amal Ahmed and
                  Lars Birkedal},
  title        = {Logical Step-Indexed Logical Relations},
  journal      = {Log. Methods Comput. Sci.},
  volume       = {7},
  number       = {2},
  year         = {2011},
  url          = {https://doi.org/10.2168/LMCS-7(2:16)2011},
  doi          = {10.2168/LMCS-7(2:16)2011},
  timestamp    = {Sun, 16 Apr 2023 20:31:20 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1103-0510.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{ClarksonSchneider:10,
  author = {Clarkson, Michael R. and Schneider, Fred B.},
  title = {Hyperproperties},
  year = {2010},
  issue_date = {September 2010},
  publisher = {IOS Press},
  address = {NLD},
  volume = {18},
  number = {6},
  issn = {0926-227X},
  abstract = {Trace properties, which have long been used for reasoning about systems, are sets of execution traces. Hyperproperties, introduced here, are sets of trace properties. Hyperproperties can express security policies, such as secure information flow and service level agreements, that trace properties cannot. Safety and liveness are generalized to hyperproperties, and every hyperproperty is shown to be the intersection of a safety hyperproperty and a liveness hyperproperty. A verification technique for safety hyperproperties is given and is shown to generalize prior techniques for verifying secure information flow. Refinement is shown to be applicable with safety hyperproperties. A topological characterization of hyperproperties is given.},
  journal = {J. Comput. Secur.},
  month = {sep},
  pages = {1157–1210},
  numpages = {54},
  keywords = {liveness, safety, Security policies, information flow}
}

@book{Abramsky:94,
  author       = {Samson Abramsky and
                  Dov M. Gabbay and
                  T. S. E. Maibaum},
  title        = {Handbook of logic in computer science. Volume 3. Semantic Structures},
  publisher    = {Clarendon Press},
  year         = {1994},
  url          = {https://global.oup.com/academic/product/handbook-of-logic-in-computer-science-9780198537625},
  isbn         = {019853762X},
  timestamp    = {Tue, 16 Mar 2021 16:16:12 +0100},
  biburl       = {https://dblp.org/rec/books/lib/Abramsky94.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Nakano:00,
  author = {Nakano, Hiroshi},
  title = {A Modality for Recursion},
  year = {2000},
  isbn = {0769507255},
  publisher = {IEEE Computer Society},
  address = {USA},
  abstract = {We propose a modal logic that enables us to handle self-referential formulae, including ones with negative self-references, which on one hand, would introduce a logical contradiction, namely Russell's paradox, in the conventional setting, while on the other hand, are necessary to capture a certain class of programs such as fixed point combinators and objects with so-called binary methods in object-oriented programming. Our logic provides a basis for axiomatic semantics of such a wider range of programs and a new framework for natural construction of recursive programs in the proofs-as-programs paradigm.},
  booktitle = {Proceedings of the 15th Annual IEEE Symposium on Logic in Computer Science},
  pages = {255},
  keywords = {Logics of Programs, Lambda and Combinatory Caluculi, Type Systems and Type Theory, Specifications, Modal and Temporal Logics},
  series = {LICS '00}
}

@inproceedings{gdtt,
  author="Bizjak, Ale{\v{s}} and Grathwohl, Hans Bugge and Clouston, Ranald and M{\o}gelberg, Rasmus E. and Birkedal, Lars", editor="Jacobs, Bart and L{\"o}ding, Christof",
  title="Guarded Dependent Type Theory with Coinductive Types",
  booktitle="Foundations of Software Science and Computation Structures",
  year="2016",
  publisher="Springer Berlin Heidelberg",
  address="Berlin, Heidelberg",
  pages="20--35",
  isbn="978-3-662-49630-5"
}

@inproceedings{Coquand:94,
  author="Coquand, Thierry",
  editor="Barendregt, Henk
  and Nipkow, Tobias",
  title="Infinite objects in type theory",
  booktitle="Types for Proofs and Programs",
  year="1994",
  publisher="Springer Berlin Heidelberg",
  address="Berlin, Heidelberg",
  pages="62--78",
  abstract="We show that infinite objects can be constructively understood without the consideration of partial elements, or greatest fixed-points, through the explicit consideration of proof objects. We present then a proof system based on these explanations. According to this analysis, the proof expressions should have the same structure as the program expressions of a pure functional lazy language: variable, constructor, application, abstraction, case expressions, and local let expressions.",
  isbn="978-3-540-48440-0"
}

@inproceedings{BirkedalMogelbergEjlers:13,
  author = {Birkedal, Lars and Mogelberg, Rasmus Ejlers},
  title = {Intensional Type Theory with Guarded Recursive Types qua Fixed Points on Universes},
  year = {2013},
  isbn = {9780769550206},
  publisher = {IEEE Computer Society},
  address = {USA},
  url = {https://doi.org/10.1109/LICS.2013.27},
  doi = {10.1109/LICS.2013.27},
  booktitle = {Proceedings of the 2013 28th Annual ACM/IEEE Symposium on Logic in Computer Science},
  pages = {213–222},
  numpages = {10},
  series = {LICS '13}
}

@article{Moggi:07,
  title = {Structuring Operational Semantics: Simplification and Computation},
  journal = {Electronic Notes in Theoretical Computer Science},
  volume = {172},
  pages = {479-497},
  year = {2007},
  note = {Computation, Meaning, and Logic: Articles dedicated to Gordon Plotkin},
  issn = {1571-0661},
  doi = {https://doi.org/10.1016/j.entcs.2007.02.016},
  url = {https://www.sciencedirect.com/science/article/pii/S1571066107000898},
  author = {Eugenio Moggi},
  keywords = {Operational Semantics, Confluent Rewriting, Multiset Rewriting},
}

@article{McBridePaterson:08,
  title={Applicative programming with effects},
  volume={18},
  DOI={10.1017/S0956796807006326},
  number={1},
  journal={Journal of Functional Programming},
  publisher={Cambridge University Press},
  author={McBride, Conor and Paterson, Ross},
  year={2008},
  pages={1–13}
}

@inproceedings{Reynolds:72,
  author = {Reynolds, John C.},
  title = {Definitional Interpreters for Higher-Order Programming Languages},
  year = {1972},
  isbn = {9781450374927},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/800194.805852},
  doi = {10.1145/800194.805852},
  abstract = {Higher-order programming languages (i.e., languages in which procedures or labels can occur as values) are usually defined by interpreters which are themselves written in a programming language based on the lambda calculus (i.e., an applicative language such as pure LISP). Examples include McCarthy's definition of LISP, Landin's SECD machine, the Vienna definition of PL/I, Reynolds' definitions of GEDANKEN, and recent unpublished work by L. Morris and C. Wadsworth. Such definitions can be classified according to whether the interpreter contains higher-order functions, and whether the order of application (i.e., call-by-value versus call-by-name) in the defined language depends upon the order of application in the defining language. As an example, we consider the definition of a simple applicative programming language by means of an interpreter written in a similar language. Definitions in each of the above classifications are derived from one another by informal but constructive methods. The treatment of imperative features such as jumps and assignment is also discussed.},
  booktitle = {Proceedings of the ACM Annual Conference - Volume 2},
  pages = {717–740},
  numpages = {24},
  keywords = {GEDANKEN, Lambda calculus, Continuation, Language definition, SECD machine, Higher-order function, PAL, Interpreter, Order of application, Closure, LISP, Programming language, Applicative language, J-operator, Reference},
  location = {Boston, Massachusetts, USA},
  series = {ACM '72}
}

@article{tctt,
  author = {M\o{}gelberg, Rasmus Ejlers and Veltri, Niccol\`{o}},
  title = {Bisimulation as Path Type for Guarded Recursive Types},
  year = {2019},
  issue_date = {January 2019},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {3},
  number = {POPL},
  url = {https://doi.org/10.1145/3290317},
  doi = {10.1145/3290317},
  abstract = {In type theory, coinductive types are used to represent processes, and are thus crucial for the formal verification of non-terminating reactive programs in proof assistants based on type theory, such as Coq and Agda. Currently, programming and reasoning about coinductive types is difficult for two reasons: The need for recursive definitions to be productive, and the lack of coincidence of the built-in identity types and the important notion of bisimilarity. Guarded recursion in the sense of Nakano has recently been suggested as a possible approach to dealing with the problem of productivity, allowing this to be encoded in types. Indeed, coinductive types can be encoded using a combination of guarded recursion and universal quantification over clocks. This paper studies the notion of bisimilarity for guarded recursive types in Ticked Cubical Type Theory, an extension of Cubical Type Theory with guarded recursion. We prove that, for any functor, an abstract, category theoretic notion of bisimilarity for the final guarded coalgebra is equivalent (in the sense of homotopy type theory) to path equality (the primitive notion of equality in cubical type theory). As a worked example we study a guarded notion of labelled transition systems, and show that, as a special case of the general theorem, path equality coincides with an adaptation of the usual notion of bisimulation for processes. In particular, this implies that guarded recursion can be used to give simple equational reasoning proofs of bisimilarity. This work should be seen as a step towards obtaining bisimilarity as path equality for coinductive types using the encodings mentioned above.},
  journal = {Proc. ACM Program. Lang.},
  month = {jan},
  articleno = {4},
  numpages = {29},
  keywords = {cubical type theory, Dependent types, bisimulation, guarded recursion, homotopy type theory, coinductive types, labelled transition systems, CCS}
}

@inproceedings{gca,
  author = {Veltri, Niccol\`{o} and Vezzosi, Andrea},
  title = {Formalizing $\pi$-Calculus in Guarded Cubical Agda},
  year = {2020},
  isbn = {9781450370974},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3372885.3373814},
  doi = {10.1145/3372885.3373814},
  abstract = {Dependent type theories with guarded recursion have shown themselves suitable for the development of denotational semantics of programming languages. In particular Ticked Cubical Type Theory (TCTT) has been used to show that for guarded labelled transition systems (GLTS) interpretation into the denotational semantics maps bisimilar processes to equal values. In fact the two notions are proved equivalent, allowing one to reason about equality in place of bisimilarity. We extend that result to the π-calculus, picking early congruence as the syntactic notion of equivalence between processes, showing that denotational models based on guarded recursive types can handle the dynamic creation of channels that goes beyond the scope of GLTSs. Hence we present a fully abstract denotational model for the early π-calculus, formalized as an extended example for Guarded Cubical Agda: a novel implementation of Ticked Cubical Type Theory based on Cubical Agda.},
  booktitle = {Proceedings of the 9th ACM SIGPLAN International Conference on Certified Programs and Proofs},
  pages = {270–283},
  numpages = {14},
  keywords = {denotational semantics, pi-calculus, ticked cubical type theory, guarded recursion},
  location = {New Orleans, LA, USA},
  series = {CPP 2020}
}

@inproceedings{MoranSands:99,
  author = {Moran, Andrew and Sands, David},
  title = {Improvement in a Lazy Context: An Operational Theory for Call-by-Need},
  year = {1999},
  isbn = {1581130953},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/292540.292547},
  doi = {10.1145/292540.292547},
  abstract = {The standard implementation technique for lazy functional languages is call-by-need, which ensures that an argument to a function in any given call is evaluated at most once. A significant problem with call-by-need is that it is difficult -- even for compiler writers -- to predict the effects of program transformations. The traditional theories for lazy functional languages are based on call-by-name models, and offer no help in determining which transformations do indeed optimize a program.In this article we present an operational theory for call-by-need, based upon an improvement ordering on programs: M is improved by N if in all program-contexts C, when C[M] terminates then C[N] terminates at least as cheaply.We show that this improvement relation satisfies a "context lemma", and supports a rich inequational theory, subsuming the call-by-need lambda calculi of Ariola et al. [AFM+95]. The reduction-based call-by-need calculi are inadequate as a theory of lazy-program transformation since they only permit transformations which speed up programs by at most a constant factor (a claim we substantiate); we go beyond the various reduction-based calculi for call-by-need by providing powerful proof rules for recursion, including syntactic continuity -- the basis of fixed-point-induction style reasoning, and an improvement theorem, suitable for arguing the correctness and safety of recursion-based program transformations.},
  booktitle = {Proceedings of the 26th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  pages = {43–56},
  numpages = {14},
  location = {San Antonio, Texas, USA},
  series = {POPL '99}
}

@inbook{MarlowJones:12,
  author = {Marlow, Simon and Peyton Jones, Simon},
  title = {The Glasgow Haskell Compiler},
  booktitle = {The Architecture of Open Source Applications, Volume 2},
  year = {2012},
  month = {January},
  abstract = {The Glasgow Haskell Compiler (GHC) started as part of an academic research project funded by the UK government at the beginning of the 1990’s, with several goals in mind:

   	To make freely available a robust and portable compiler for Haskell that generates high performance code;
   	To provide a modular foundation that other researchers can extend and develop;
   	To learn how real programs behave, so that we can design and build better compilers.

  GHC is now over 20 years old, and has been under continuous active development since its inception. Today, GHC releases are downloaded by hundreds of thousands of people, the online repository of Haskell libraries has over 3,000 packages, GHC is used to teach Haskell in many undergraduate courses, and there are a growing number of instances of Haskell being depended upon commercially.},
  publisher = {Lulu},
  url = {https://www.microsoft.com/en-us/research/publication/the-glasgow-haskell-compiler/},
  edition = {The Architecture of Open Source Applications, Volume 2},
}

@inproceedings{Turner:95,
author = {Turner, David N. and Wadler, Philip and Mossin, Christian},
title = {Once upon a type},
year = {1995},
isbn = {0897917197},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/224164.224168},
doi = {10.1145/224164.224168},
booktitle = {Proceedings of the Seventh International Conference on Functional Programming Languages and Computer Architecture},
pages = {1–11},
numpages = {11},
location = {La Jolla, California, USA},
series = {FPCA '95}
}

@inproceedings{Gustavsson:98,
  author = {Gustavsson, J\"{o}rgen},
  title = {A Type Based Sharing Analysis for Update Avoidance and Optimisation},
  year = {1998},
  isbn = {1581130244},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/289423.289427},
  doi = {10.1145/289423.289427},
  abstract = {Sharing of evaluation is crucial for the efficiency of lazy functional languages, but unfortunately the machinery to implement it carries an inherent overhead. In abstract machines this overhead shows up as the cost of performing updates, many of them actually unnecessary, and also in the cost of the associated bookkeeping, that is keeping track of when and where to update. In spineless abstract machines, such as the STG-machine and the TIM, this bookkeeping consists of pushing, checking for and popping update markers. Checking for update markers is a very frequent operation and indeed the implementation of the STG-machine has been optimised for fast update marker checks at the expense of making the pushing and popping of update markers more costly.In this paper we present a type based sharing analysis that can determine when updates can be safely omitted and marker checks bypassed. The type system is proved sound with respect to the lazy Krivine machine. We have implemented the analysis and the preliminary benchmarks seem very promising. Most notably, virtually all update marker checks can be avoided. This may make the tradeoffs of current implementations obsolete and calls for new abstract machine designs.},
  booktitle = {Proceedings of the Third ACM SIGPLAN International Conference on Functional Programming},
  pages = {39–50},
  numpages = {12},
  location = {Baltimore, Maryland, USA},
  series = {ICFP '98}
}

@article{Birkedal:12,
  TITLE = {{First steps in synthetic guarded domain theory: step-indexing in the
  topos of trees}},
  AUTHOR = {Lars Birkedal and Rasmus Ejlers Møgelberg and Jan Schwinghammer and Kristian Støvring},
  URL = {https://lmcs.episciences.org/1041},
  DOI = {10.2168/LMCS-8(4:1)2012},
  JOURNAL = {{Logical Methods in Computer Science}},
  VOLUME = {{Volume 8, Issue 4}},
  YEAR = {2012},
  MONTH = Oct,
  KEYWORDS = {Computer Science - Logic in Computer Science ; D.3.1 ; F.3.2},
}

@article{Spies:22,
  author = {Spies, Simon and G\"{a}her, Lennard and Tassarotti, Joseph and Jung, Ralf and Krebbers, Robbert and Birkedal, Lars and Dreyer, Derek},
  title = {Later Credits: Resourceful Reasoning for the Later Modality},
  year = {2022},
  issue_date = {August 2022},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {6},
  number = {ICFP},
  url = {https://doi.org/10.1145/3547631},
  doi = {10.1145/3547631},
  abstract = {In the past two decades, step-indexed logical relations and separation logics have both come to play a major role in semantics and verification research. More recently, they have been married together in the form of step-indexed separation logics like VST, iCAP, and Iris, which provide powerful tools for (among other things) building semantic models of richly typed languages like Rust. In these logics, propositions are given semantics using a step-indexed model, and step-indexed reasoning is reflected into the logic through the so-called “later” modality. On the one hand, this modality provides an elegant, high-level account of step-indexed reasoning; on the other hand, when used in sufficiently sophisticated ways, it can become a nuisance, turning perfectly natural proof strategies into dead ends. In this work, we introduce later credits, a new technique for escaping later-modality quagmires. By leveraging the second ancestor of these logics—separation logic—later credits turn “the right to eliminate a later” into an ownable resource, which is subject to all the traditional modular reasoning principles of separation logic. We develop the theory of later credits in the context of Iris, and present several challenging examples of proofs and proof patterns which were previously not possible in Iris but are now possible due to later credits.},
  journal = {Proc. ACM Program. Lang.},
  month = {aug},
  articleno = {100},
  numpages = {29},
  keywords = {Iris, Separation logic, later modality, step-indexing, transfinite}
}

@inproceedings{MontaguJensen:21,
  author = {Montagu, Beno\^{\i}t and Jensen, Thomas},
  title = {Trace-Based Control-Flow Analysis},
  year = {2021},
  isbn = {9781450383912},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3453483.3454057},
  doi = {10.1145/3453483.3454057},
  abstract = {We define a small-step semantics for the untyped λ-calculus, that traces the β-reductions that occur during evaluation. By abstracting the computation traces, we reconstruct k-CFA using abstract interpretation, and justify constraint-based k-CFA in a semantic way. The abstract interpretation of the trace semantics also paves the way for introducing widening operators in CFA that go beyond existing analyses, that are all based on exploring a finite state space. We define ∇CFA, a widening-based analysis that limits the cycles in call stacks, and can achieve better precision than k-CFA at a similar cost.},
  booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
  pages = {482–496},
  numpages = {15},
  keywords = {lambda-calculus, control flow analysis, widening, program traces, abstract interpretation},
  location = {Virtual, Canada},
  series = {PLDI 2021}
}

@phdthesis{Breitner:16,
  title = {Lazy Evaluation: From natural semantics to a machine-checked compiler transformation},
  year = {2016},
  month = apr,
  author = {Joachim Breitner},
  school = {Karlsruher Institut f{\"u}r Technologie, Fakult{\"a}t f{\"u}r Informatik},
  url = {http://www.joachim-breitner.de/thesis/},
  doi = {10.5445/IR/1000054251},
}

@article{Breitner:18,
  title={The adequacy of Launchbury's natural semantics for lazy evaluation},
  volume={28},
  DOI={10.1017/S0956796817000144},
  journal={Journal of Functional Programming},
  publisher={Cambridge University Press},
  author={Breitner, Joachim},
  year={2018},
  pages={e1}
}
s
@article{HackettHutton:19,
  author = {Hackett, Jennifer and Hutton, Graham},
  title = {Call-by-Need is Clairvoyant Call-by-Value},
  year = {2019},
  issue_date = {August 2019},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {3},
  number = {ICFP},
  url = {https://doi.org/10.1145/3341718},
  doi = {10.1145/3341718},
  abstract = {Call-by-need evaluation, also known as lazy evaluation, provides two key benefits: compositional programming and infinite data. The standard semantics for laziness is Launchbury’s natural semantics&nbsp;DBLP:conf/popl/Launchbury93, which uses a heap to memoise the results of delayed evaluations. However, the stateful nature of this heap greatly complicates reasoning about the operational behaviour of lazy programs. In this article, we propose an alternative semantics for laziness, clairvoyant evaluation, that replaces the state effect with nondeterminism, and prove this semantics equivalent in a strong sense to the standard semantics. We show how this new semantics greatly simplifies operational reasoning, admitting much simpler proofs of a number of results from the literature, and how it leads to the first denotational cost semantics for lazy evaluation.},
  journal = {Proc. ACM Program. Lang.},
  month = {jul},
  articleno = {114},
  numpages = {23},
  keywords = {lazy evaluation}
}

@article{Mogelberg:21,
  doi = {10.4204/eptcs.351.13},
  url = {https://doi.org/10.4204%2Feptcs.351.13},
  year = 2021,
  month = {dec},
  publisher = {Open Publishing Association},
  volume = {351},
  pages = {200--217},
  author = {Rasmus Ejlers M{\o}gelberg and Andrea Vezzosi},
  title = {Two Guarded Recursive Powerdomains for Applicative Simulation},
  journal = {Electronic Proceedings in Theoretical Computer Science}
}

@article{interaction-trees,
  author = {Xia, Li-yao and Zakowski, Yannick and He, Paul and Hur, Chung-Kil and Malecha, Gregory and Pierce, Benjamin C. and Zdancewic, Steve},
  title = {Interaction Trees: Representing Recursive and Impure Programs in Coq},
  year = {2019},
  issue_date = {January 2020},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {4},
  number = {POPL},
  url = {https://doi.org/10.1145/3371119},
  doi = {10.1145/3371119},
  abstract = {Interaction trees (ITrees) are a general-purpose data structure for representing the behaviors of recursive programs that interact with their environments. A coinductive variant of “free monads,” ITrees are built out of uninterpreted events and their continuations. They support compositional construction of interpreters from event handlers, which give meaning to events by defining their semantics as monadic actions. ITrees are expressive enough to represent impure and potentially nonterminating, mutually recursive computations, while admitting a rich equational theory of equivalence up to weak bisimulation. In contrast to other approaches such as relationally specified operational semantics, ITrees are executable via code extraction, making them suitable for debugging, testing, and implementing software artifacts that are amenable to formal verification. We have implemented ITrees and their associated theory as a Coq library, mechanizing classic domain- and category-theoretic results about program semantics, iteration, monadic structures, and equational reasoning. Although the internals of the library rely heavily on coinductive proofs, the interface hides these details so that clients can use and reason about ITrees without explicit use of Coq’s coinduction tactics. To showcase the utility of our theory, we prove the termination-sensitive correctness of a compiler from a simple imperative source language to an assembly-like target whose meanings are given in an ITree-based denotational semantics. Unlike previous results using operational techniques, our bisimulation proof follows straightforwardly by structural induction and elementary rewriting via an equational theory of combinators for control-flow graphs.},
  journal = {Proc. ACM Program. Lang.},
  month = {dec},
  articleno = {51},
  numpages = {32},
  keywords = {Coq, coinduction, monads, compiler correctness}
}

@inproceedings{Reynolds:02,
  author={Reynolds, J.C.},
  booktitle={Proceedings 17th Annual IEEE Symposium on Logic in Computer Science},
  title={Separation logic: a logic for shared mutable data structures},
  year={2002},
  volume={},
  number={},
  pages={55-74},
  doi={10.1109/LICS.2002.1029817}
}

@phdthesis{Morris:69,
  author  = "Morris, James Hiram Jr",
  title   = "Lambda-calculus models of programming languages",
  school  = "Massachusetts Institute of Technology",
  year    = "1969",
}

@article{Lamport:77,
  author={Lamport, L.},
  journal={IEEE Transactions on Software Engineering},
  title={Proving the Correctness of Multiprocess Programs},
  year={1977},
  volume={SE-3},
  number={2},
  pages={125-143},
  doi={10.1109/TSE.1977.229904}
}

@article{Capretta:05,
  TITLE = {{General Recursion via Coinductive Types}},
  AUTHOR = {Venanzio Capretta},
  URL = {http://lmcs.episciences.org/2265},
  DOI = {10.2168/LMCS-1(2:1)2005},
  JOURNAL = {{Logical Methods in Computer Science}},
  VOLUME = {{Volume 1, Issue 2}},
  YEAR = {2005},
  MONTH = Jul,
  KEYWORDS = {Computer Science - Logic in Computer Science ; F.3.1},
}

@inproceedings{Carette:07,
  author="Carette, Jacques
  and Kiselyov, Oleg
  and Shan, Chung-chieh",
  editor="Shao, Zhong",
  title="Finally Tagless, Partially Evaluated",
  booktitle="Programming Languages and Systems",
  year="2007",
  publisher="Springer Berlin Heidelberg",
  address="Berlin, Heidelberg",
  pages="222--238",
  abstract="We have built the first family of tagless interpretations for a higher-order typed object language in a typed metalanguage (Haskell or ML) that require no dependent types, generalized algebraic data types, or postprocessing to eliminate tags. The statically type-preserving interpretations include an evaluator, a compiler (or staged evaluator), a partial evaluator, and call-by-name and call-by-value CPS transformers.",
  isbn="978-3-540-76637-7"
}

@article{Keidel:18,
  author = {Keidel, Sven and Poulsen, Casper Bach and Erdweg, Sebastian},
  title = {Compositional Soundness Proofs of Abstract Interpreters},
  year = {2018},
  issue_date = {September 2018},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {2},
  number = {ICFP},
  url = {https://doi.org/10.1145/3236767},
  doi = {10.1145/3236767},
  abstract = {Abstract interpretation is a technique for developing static analyses. Yet, proving abstract interpreters sound is challenging for interesting analyses, because of the high proof complexity and proof effort. To reduce complexity and effort, we propose a framework for abstract interpreters that makes their soundness proof compositional. Key to our approach is to capture the similarities between concrete and abstract interpreters in a single shared interpreter, parameterized over an arrow-based interface. In our framework, a soundness proof is reduced to proving reusable soundness lemmas over the concrete and abstract instances of this interface; the soundness of the overall interpreters follows from a generic theorem. To further reduce proof effort, we explore the relationship between soundness and parametricity. Parametricity not only provides us with useful guidelines for how to design non-leaky interfaces for shared interpreters, but also provides us soundness of shared pure functions as free theorems. We implemented our framework in Haskell and developed a k-CFA analysis for PCF and a tree-shape analysis for Stratego. We were able to prove both analyses sound compositionally with manageable complexity and effort, compared to a conventional soundness proof.},
  journal = {Proc. ACM Program. Lang.},
  month = {jul},
  articleno = {72},
  numpages = {26},
  keywords = {Abstract Interpretation, Soundness}
}

@article{Keidel:19,
  author = {Keidel, Sven and Erdweg, Sebastian},
  title = {Sound and reusable components for abstract interpretation},
  year = {2019},
  issue_date = {October 2019},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {3},
  number = {OOPSLA},
  url = {https://doi.org/10.1145/3360602},
  doi = {10.1145/3360602},
  abstract = {Abstract interpretation is a methodology for defining sound static analysis. Yet, building sound static analyses for modern programming languages is difficult, because these static analyses need to combine sophisticated abstractions for values, environments, stores, etc. However, static analyses often tightly couple these abstractions in the implementation, which not only complicates the implementation, but also makes it hard to decide which parts of the analyses can be proven sound independently from each other. Furthermore, this coupling makes it hard to combine soundness lemmas for parts of the analysis to a soundness proof of the complete analysis. To solve this problem, we propose to construct static analyses modularly from reusable analysis components. Each analysis component encapsulates a single analysis concern and can be proven sound independently from the analysis where it is used. We base the design of our analysis components on arrow transformers, which allows us to compose analysis components. This composition preserves soundness, which guarantees that a static analysis is sound, if all its analysis components are sound. This means that analysis developers do not have to worry about soundness as long as they reuse sound analysis components. To evaluate our approach, we developed a library of 13 reusable analysis components in Haskell. We use these components to define a k-CFA analysis for PCF and an interval and reaching definition analysis for a While language.},
  journal = {Proc. ACM Program. Lang.},
  month = {oct},
  articleno = {176},
  numpages = {28},
  keywords = {Static Analysis, Soundness, Abstract Interpretation}
}

@article{Keidel:23,
  author = {Keidel, Sven and Erdweg, Sebastian and Homb\"{u}cher, Tobias},
  title = {Combinator-Based Fixpoint Algorithms for Big-Step Abstract Interpreters},
  year = {2023},
  issue_date = {August 2023},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {7},
  number = {ICFP},
  url = {https://doi.org/10.1145/3607863},
  doi = {10.1145/3607863},
  abstract = {Big-step abstract interpreters are an approach to build static analyzers based on big-step interpretation. While big-step interpretation provides a number of benefits for the definition of an analysis, it also requires particularly complicated fixpoint algorithms because the analysis definition is a recursive function whose termination is uncertain. This is in contrast to other analysis approaches, such as small-step reduction, abstract machines, or graph reachability, where the analysis essentially forms a finite transition system between widened analysis states.

  We show how to systematically develop sophisticated fixpoint algorithms for big-step abstract interpreters and how to ensure their soundness. Our approach is based on small and reusable fixpoint combinators that can be composed to yield fixpoint algorithms. For example, these combinators describe the order in which the program is analyzed, how deep recursive functions are unfolded and loops unrolled, or they record auxiliary data such as a (context-sensitive) call graph. Importantly, each combinator can be developed separately, reused across analyses, and can be verified sound independently. Consequently, analysis developers can freely compose combinators to obtain sound fixpoint algorithms that work best for their use case. We provide a formal metatheory that guarantees a fixpoint algorithm is sound if its composed from sound combinators only. We experimentally validate our combinator-based approach by describing sophisticated fixpoint algorithms for analyses of Stratego, Scheme, and WebAssembly.},
  journal = {Proc. ACM Program. Lang.},
  month = {aug},
  articleno = {221},
  numpages = {27},
  keywords = {Static Analysis, Fixpoint Algorithm, Big-Step Abstract Interpretation}
}

@inproceedings{Nakata:10,
  author       = {Keiko Nakata},
  editor       = {Luigi Santocanale},
  title        = {Denotational Semantics for Lazy Initialization of letrec},
  booktitle    = {7th Workshop on Fixed Points in Computer Science, {FICS} 2010, Brno,
                  Czech Republic, August 21-22, 2010},
  pages        = {61--67},
  publisher    = {Laboratoire d'Informatique Fondamentale de Marseille},
  year         = {2010},
  url          = {https://hal.archives-ouvertes.fr/hal-00512377/document\#page=62},
  timestamp    = {Tue, 21 Jul 2020 00:40:32 +0200},
  biburl       = {https://dblp.org/rec/conf/fics/Nakata10.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Nakata:06,
  author = {Nakata, Keiko and Garrigue, Jacques},
  title = {Recursive Modules for Programming},
  year = {2006},
  issue_date = {September 2006},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {41},
  number = {9},
  issn = {0362-1340},
  url = {https://doi.org/10.1145/1160074.1159813},
  doi = {10.1145/1160074.1159813},
  abstract = {TheML module system is useful for building large-scale programs. The programmer can factor programs into nested and parameterized modules, and can control abstraction with signatures. Yet ML prohibits recursion between modules. As a result of this constraint, the programmer may have to consolidate conceptually separate components into a single module, intruding on modular programming. Introducing recursive modules is a natural way out of this predicament. Existing proposals, however, vary in expressiveness and verbosity. In this paper, we propose a type system for recursive modules, which can infer their signatures. Opaque signatures can also be given explicitly, to provide type abstraction either inside or outside the recursion. The type system is decidable, and is sound for a call-by-value semantics. We also present a solution to the expression problem, in support of our design choices.},
  journal = {SIGPLAN Not.},
  month = {sep},
  pages = {74–86},
  numpages = {13},
  keywords = {the expression problem, type systems, recursive modules, applicative functors, type inference}
}

@article{stg,
  abstract = {The Spineless Tagless G-machine is an abstract machine designed to support non-strict higher-order functional languages. This presentation of the machine falls into three parts. Firstly, we give a general discussion of the design issues involved in implementing non-strict functional languages. Next, we present the STG language, an austere but recognizably-functional language, which as well as a denotational meaning has a well-defined operational semantics. The STG language is the abstract machine code for the Spineless Tagless G-machine. Lastly, we discuss the mapping of the STG language onto stock hardware. The success of an abstract machine model depends largely on how efficient this mapping can be made, though this topic is often relegated to a short section. Instead, we give a detailed discussion of the design issues and the choices we have made. Our principal target is the C language, treating the C compiler as a portable assembler.},
  author = {Peyton Jones, Simon L.},
  doi = {10.1017/S0956796800000319},
  issn = {14697653},
  journal = {Journal of Functional Programming},
  title = {Implementing lazy functional languages on stock hardware: The Spineless Tagless G-machine},
  year = {1992},
}

@inproceedings{Mangal:14,
  author="Mangal, Ravi
  and Naik, Mayur
  and Yang, Hongseok",
  editor="Shao, Zhong",
  title="A Correspondence between Two Approaches to Interprocedural Analysis in the Presence of Join",
  booktitle="Programming Languages and Systems",
  year="2014",
  publisher="Springer Berlin Heidelberg",
  address="Berlin, Heidelberg",
  pages="513--533",
  abstract="Many interprocedural static analyses perform a lossy join for reasons of termination or efficiency. We study the relationship between two predominant approaches to interprocedural analysis, the summary-based (or functional) approach and the call-strings (or k-CFA) approach, in the presence of a lossy join. Despite the use of radically different ways to distinguish procedure contexts by these two approaches, we prove that post-processing their results using a form of garbage collection renders them equivalent. Our result extends the classic result by Sharir and Pnueli that showed the equivalence between these two approaches in the setting of distributive analysis, wherein the join is lossless.",
  isbn="978-3-642-54833-8"
}

@article{Kalita:2022,
  author = {Kalita, Pankaj Kumar and Muduli, Sujit Kumar and D’Antoni, Loris and Reps, Thomas and Roy, Subhajit},
  title = {Synthesizing Abstract Transformers},
  year = {2022},
  issue_date = {October 2022},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {6},
  number = {OOPSLA2},
  url = {https://doi.org/10.1145/3563334},
  doi = {10.1145/3563334},
  abstract = {This paper addresses the problem of creating abstract transformers automatically. The method we present automates the construction of static analyzers in a fashion similar to the way yacc automates the construction of parsers. Our method treats the problem as a program-synthesis problem. The user provides specifications of (i) the concrete semantics of a given operation op, (ii) the abstract domain A to be used by the analyzer, and (iii) the semantics of a domain-specific language L in which the abstract transformer is to be expressed. As output, our method creates an abstract transformer for op in abstract domain A, expressed in L (an “L-transformer for op over A”). Moreover, the abstract transformer obtained is a most-precise L-transformer for op over A; that is, there is no other L-transformer for op over A that is strictly more precise. We implemented our method in a tool called AMURTH. We used AMURTH to create sets of replacement abstract transformers for those used in two existing analyzers, and obtained essentially identical performance. However, when we compared the existing transformers with the transformers obtained using AMURTH, we discovered that four of the existing transformers were unsound, which demonstrates the risk of using manually created transformers.},
  journal = {Proc. ACM Program. Lang.},
  month = {oct},
  articleno = {171},
  numpages = {29},
  keywords = {DSL, abstract transformer, program synthesis}
}

@article{Danielsson:06,
  author = {Danielsson, Nils Anders and Hughes, John and Jansson, Patrik and Gibbons, Jeremy},
  title = {Fast and Loose Reasoning is Morally Correct},
  year = {2006},
  issue_date = {January 2006},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {41},
  number = {1},
  issn = {0362-1340},
  url = {https://doi.org/10.1145/1111320.1111056},
  doi = {10.1145/1111320.1111056},
  abstract = {Functional programmers often reason about programs as if they were written in a total language, expecting the results to carry over to non-total (partial) languages. We justify such reasoning.Two languages are defined, one total and one partial, with identical syntax. The semantics of the partial language includes partial and infinite values, and all types are lifted, including the function spaces. A partial equivalence relation (PER) is then defined, the domain of which is the total subset of the partial language. For types not containing function spaces the PER relates equal values, and functions are related if they map related values to related values.It is proved that if two closed terms have the same semantics in the total language, then they have related semantics in the partial language. It is also shown that the PER gives rise to a bicartesian closed category which can be used to reason about values in the domain of the relation.},
  journal = {SIGPLAN Not.},
  month = {jan},
  pages = {206–217},
  numpages = {12},
  keywords = {partial and total languages, equational reasoning, inductive and coinductive types, partial and infinite values, lifted types, non-strict and strict languages}
}

@inproceedings{Hughes:96,
  author = {Hughes, John and Pareto, Lars and Sabry, Amr},
  title = {Proving the Correctness of Reactive Systems Using Sized Types},
  year = {1996},
  isbn = {0897917693},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/237721.240882},
  doi = {10.1145/237721.240882},
  abstract = {We have designed and implemented a type-based analysis for proving some basic properties of reactive systems. The analysis manipulates rich type expressions that contain information about the sizes of recursively defined data structures. Sized types are useful for detecting deadlocks, nontermination, and other errors in embedded programs. To establish the soundness of the analysis we have developed an appropriate semantic model of sized types.},
  booktitle = {Proceedings of the 23rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  pages = {410–423},
  numpages = {14},
  location = {St. Petersburg Beach, Florida, USA},
  series = {POPL '96}
}

@misc{gitrees,
  title={Modular Denotational Semantics for Effects with Guarded Interaction Trees},
  author={Dan Frumin and Amin Timany and Lars Birkedal},
  year={2023},
  eprint={2307.08514},
  archivePrefix={arXiv},
  primaryClass={cs.PL}
}

@article{Backhouse:04,
  title = {Safety of abstract interpretations for free, via logical relations and Galois connections},
  journal = {Science of Computer Programming},
  volume = {51},
  number = {1},
  pages = {153-196},
  year = {2004},
  note = {Mathematics of Program Construction (MPC 2002)},
  issn = {0167-6423},
  doi = {https://doi.org/10.1016/j.scico.2003.06.002},
  url = {https://www.sciencedirect.com/science/article/pii/S0167642304000164},
  author = {Kevin Backhouse and Roland Backhouse},
  keywords = {Abstract interpretation, Logical relations, Parametricity, Theorems for free, Galois connections},
  abstract = {Algebraic properties of logical relations on partially ordered sets are studied. It is shown how to construct a logical relation that extends a collection of base Galois connections to a Galois connection of arbitrary higher-order type. “Theorems-for-free” is used to show that the construction ensures safe abstract interpretation of parametrically polymorphic functions. The properties are used to show how abstract interpretations of program libraries can be constructed.}
}

@inproceedings{Reynolds:83,
  author       = {John C. Reynolds},
  editor       = {R. E. A. Mason},
  title        = {Types, Abstraction and Parametric Polymorphism},
  booktitle    = {Information Processing 83, Proceedings of the {IFIP} 9th World Computer
                  Congress, Paris, France, September 19-23, 1983},
  pages        = {513--523},
  publisher    = {North-Holland/IFIP},
  year         = {1983},
  timestamp    = {Sun, 28 Jul 2019 17:03:41 +0200},
  biburl       = {https://dblp.org/rec/conf/ifip/Reynolds83.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Wadler:89,
  author       = {Philip Wadler},
  editor       = {Joseph E. Stoy},
  title        = {Theorems for Free!},
  booktitle    = {Proceedings of the fourth international conference on Functional programming
                  languages and computer architecture, {FPCA} 1989, London, UK, September
                  11-13, 1989},
  pages        = {347--359},
  publisher    = {{ACM}},
  year         = {1989},
  url          = {https://doi.org/10.1145/99370.99404},
  doi          = {10.1145/99370.99404},
  timestamp    = {Wed, 14 Nov 2018 10:57:36 +0100},
  biburl       = {https://dblp.org/rec/conf/fpca/Wadler89.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Spies:21,
  author = {Spies, Simon and G\"{a}her, Lennard and Gratzer, Daniel and Tassarotti, Joseph and Krebbers, Robbert and Dreyer, Derek and Birkedal, Lars},
  title = {Transfinite Iris: Resolving an Existential Dilemma of Step-Indexed Separation Logic},
  year = {2021},
  isbn = {9781450383912},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3453483.3454031},
  doi = {10.1145/3453483.3454031},
  abstract = {Step-indexed separation logic has proven to be a powerful tool for modular reasoning about higher-order stateful programs. However, it has only been used to reason about safety properties, never liveness properties. In this paper, we observe that the inability of step-indexed separation logic to support liveness properties stems fundamentally from its failure to validate the existential property, connecting the meaning of existential quantification inside and outside the logic. We show how to validate the existential property—and thus enable liveness reasoning—by moving from finite step-indices (natural numbers) to transfinite step-indices (ordinals). Concretely, we transform the Coq-based step-indexed logic Iris to Transfinite Iris, and demonstrate its effectiveness in proving termination and termination-preserving refinement for higher-order stateful programs.},
  booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
  pages = {80–95},
  numpages = {16},
  keywords = {Iris, step-indexing, transfinite, ordinals, liveness properties, Separation logic},
  location = {Virtual, Canada},
  series = {PLDI 2021}
}

@manual{iris-lecture-notes,
  title = {Lecture Notes on Iris: Higher-Order Concurrent Separation Logic},
  author = {Lars Birkedal and Aleš Bizjak},
  year = {2023},
  month = {August},
  organization = {Aarhus University},
  address = {Aarhus, Denmark},
  note = {\url{https://iris-project.org/tutorial-pdfs/iris-lecture-notes.pdf}},
}

@book{SharirPnueli:78,
  title={Two approaches to interprocedural data flow analysis},
  author={Sharir, Micha and Pnueli, Amir and others},
  year={1978},
  publisher={New York University. Courant Institute of Mathematical Sciences~…}
}

@inproceedings{Owens:16,
  author="Owens, Scott
  and Myreen, Magnus O.
  and Kumar, Ramana
  and Tan, Yong Kiam",
  editor="Thiemann, Peter",
  title="Functional Big-Step Semantics",
  booktitle="Programming Languages and Systems",
  year="2016",
  publisher="Springer Berlin Heidelberg",
  address="Berlin, Heidelberg",
  pages="589--615",
  abstract="When doing an interactive proof about a piece of software, it is important that the underlying programming language's semantics does not make the proof unnecessarily difficult or unwieldy. Both small-step and big-step semantics are commonly used, and the latter is typically given by an inductively defined relation. In this paper, we consider an alternative: using a recursive function akin to an interpreter for the language. The advantages include a better induction theorem, less duplication, accessibility to ordinary functional programmers, and the ease of doing symbolic simulation in proofs via rewriting. We believe that this style of semantics is well suited for compiler verification, including proofs of divergence preservation. We do not claim the invention of this style of semantics: our contribution here is to clarify its value, and to explain how it supports several language features that might appear to require a relational or small-step approach. We illustrate the technique on a simple imperative language with C-like for-loops and a break statement, and compare it to a variety of other approaches. We also provide ML and lambda-calculus based examples to illustrate its generality.",
  isbn="978-3-662-49498-1"
}

@article{refun,
  title = {Refunctionalization at work},
  journal = {Science of Computer Programming},
  volume = {74},
  number = {8},
  pages = {534-549},
  year = {2009},
  note = {Special Issue on Mathematics of Program Construction (MPC 2006)},
  issn = {0167-6423},
  doi = {https://doi.org/10.1016/j.scico.2007.10.007},
  url = {https://www.sciencedirect.com/science/article/pii/S0167642309000227},
  author = {Olivier Danvy and Kevin Millikin},
  keywords = {Defunctionalization, Refunctionalization, Abstract machines, Continuations, Continuation-passing style (CPS), Shunting-yard algorithm},
  abstract = {We present the left inverse of Reynolds’ defunctionalization and we show its relevance to programming and to programming languages. We propose two methods to transform a program that is almost in defunctionalized form into one that is actually in defunctionalized form, and we illustrate them with a recognizer for Dyck words and with Dijkstra’s shunting-yard algorithm.}
}

@misc{Might:10,
  title = {Architectures for interpreters: Substitutional, denotational, big-step and small-step},
  author = {Might, Matthew},
  howpublished = {\url{https://web.archive.org/web/20100216131108/https://matt.might.net/articles/writing-an-interpreter-substitution-denotational-big-step-small-step/}},
  note = {Accessed: 2010-02-16},
  year = {2010}
}

@article{Bodin:19,
  author = {Bodin, Martin and Gardner, Philippa and Jensen, Thomas and Schmitt, Alan},
  title = {Skeletal Semantics and Their Interpretations},
  year = {2019},
  issue_date = {January 2019},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {3},
  number = {POPL},
  url = {https://doi.org/10.1145/3290357},
  doi = {10.1145/3290357},
  abstract = {The development of mechanised language specification based on structured operational semantics, with applications to verified compilers and sound program analysis, requires huge effort. General theory and frameworks have been proposed to help with this effort. However, none of this work provides a systematic way of developing concrete and abstract semantics, connected together by a general consistency result. We introduce a skeletal semantics of a language, where each skeleton describes the complete semantic behaviour of a language construct. We define a general notion of interpretation, which provides a systematic and language-independent way of deriving semantic judgements from the skeletal semantics. We explore four generic interpretations: a simple well-formedness interpretation; a concrete interpretation; an abstract interpretation; and a constraint generator for flow-sensitive analysis. We prove general consistency results between interpretations, depending only on simple language-dependent lemmas. We illustrate our ideas using a simple While language.},
  journal = {Proc. ACM Program. Lang.},
  month = {jan},
  articleno = {44},
  numpages = {31},
  keywords = {semantics, programming language, abstract interpretation}
}

@article{Josephs:89,
  title = {The semantics of lazy functional languages},
  journal = {Theoretical Computer Science},
  volume = {68},
  number = {1},
  pages = {105-111},
  year = {1989},
  issn = {0304-3975},
  doi = {https://doi.org/10.1016/0304-3975(89)90122-9},
  url = {https://www.sciencedirect.com/science/article/pii/0304397589901229},
  author = {Mark B. Josephs},
  abstract = {A denotational semantics for the λ-calculus is described. The semantics is continuation-based, and so reflects the order in which expressions are evaluated. It provides a means by which lazy functional languages can be better understood.}
}

@inproceedings{Cousot:94,
  author={Cousot, P. and Cousot, R.},
  booktitle={Proceedings of 1994 IEEE International Conference on Computer Languages (ICCL'94)},
  title={Higher-order abstract interpretation (and application to comportment analysis generalizing strictness, termination, projection and PER analysis of functional languages)},
  year={1994},
  volume={},
  number={},
  pages={95-112},
  keywords={Lattices;Concrete;Computer languages;Logic programming;Pressing;Joining processes},
  doi={10.1109/ICCL.1994.288389}
}

@InProceedings{Cousot:02,
  author="Cousot, Patrick
  and Cousot, Radhia",
  editor="Horspool, R. Nigel",
  title="Modular Static Program Analysis",
  booktitle="Compiler Construction",
  year="2002",
  publisher="Springer Berlin Heidelberg",
  address="Berlin, Heidelberg",
  pages="159--179",
  abstract="The purpose of this paper is to present four basic methods for compositional separate modular static analysis of programs by abstract interpretation: - simplification-based separate analysis; - worst-case separate analysis; - separate analysis with (user-provided) interfaces; - symbolic relational separate analysis; as well as a fifth category which is essentially obtained by composition of the above separate local analyses together with global analysis methods.",
  isbn="978-3-540-45937-8"
}

@inproceedings{Mycroft:80,
  author       = {Alan Mycroft},
  editor       = {Bernard J. Robinet},
  title        = {The Theory and Practice of Transforming Call-by-need into Call-by-value},
  booktitle    = {International Symposium on Programming, Proceedings of the Fourth
                  'Colloque International sur la Programmation', Paris, France, 22-24
                  April 1980},
  series       = {Lecture Notes in Computer Science},
  volume       = {83},
  pages        = {269--281},
  publisher    = {Springer},
  year         = {1980},
  url          = {https://doi.org/10.1007/3-540-09981-6\_19},
  doi          = {10.1007/3-540-09981-6\_19},
  timestamp    = {Thu, 24 Feb 2022 13:41:39 +0100},
  biburl       = {https://dblp.org/rec/conf/programm/Mycroft80.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Wadler:87,
  author = {Wadler, Philip and Hughes, R. J. M.},
  title = {Projections for strictness analysis},
  year = {1987},
  isbn = {0387183175},
  publisher = {Springer-Verlag},
  address = {Berlin, Heidelberg},
  booktitle = {Proc. of a Conference on Functional Programming Languages and Computer Architecture},
  pages = {385–407},
  numpages = {23},
  location = {Portland, Oregon, USA}
}

@inproceedings{Pfenning:88,
author = {Pfenning, F. and Elliott, C.},
title = {Higher-order abstract syntax},
year = {1988},
isbn = {0897912691},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/53990.54010},
doi = {10.1145/53990.54010},
abstract = {We describe motivation, design, use, and implementation of higher-order abstract syntax as a central representation for programs, formulas, rules, and other syntactic objects in program manipulation and other formal systems where matching and substitution or unification are central operations. Higher-order abstract syntax incorporates name binding information in a uniform and language generic way. Thus it acts as a powerful link integrating diverse tools in such formal environments. We have implemented higher-order abstract syntax, a supporting matching and unification algorithm, and some clients in Common Lisp in the framework of the Ergo project at Carnegie Mellon University.},
booktitle = {Proceedings of the ACM SIGPLAN 1988 Conference on Programming Language Design and Implementation},
pages = {199–208},
numpages = {10},
location = {Atlanta, Georgia, USA},
series = {PLDI '88}
}

@inproceedings{Allen:74,
  author       = {Frances E. Allen},
  editor       = {Jack L. Rosenfeld},
  title        = {Interprocedural Data Flow Analysis},
  booktitle    = {Information Processing, Proceedings of the 6th {IFIP} Congress 1974,
                  Stockholm, Sweden, August 5-10, 1974},
  pages        = {398--402},
  publisher    = {North-Holland},
  year         = {1974},
  timestamp    = {Fri, 26 Jul 2019 22:58:51 +0200},
  biburl       = {https://dblp.org/rec/conf/ifip/Allen74.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@book{Rosen:75,
  title={Data flow analysis for recursive PL/I programs},
  author={Rosen, Barry K},
  year={1975},
  publisher={IBM Thomas J. Watson Research Center}
}

@article{Lomet:77,
  author={Lomet, D. B.},
  journal={IBM Journal of Research and Development},
  title={Data Flow Analysis in the Presence of Procedure Calls},
  year={1977},
  volume={21},
  number={6},
  pages={559-571},
  keywords={},
  doi={10.1147/rd.216.0559}
}

@Inbook{SPJ:94,
  author="Peyton Jones, Simon and Partain, Will",
  editor="O'Donnell, John T.
  and Hammond, Kevin",
  title="Measuring the effectiveness of a simple strictness analyser",
  bookTitle="Functional Programming, Glasgow 1993: Proceedings of the 1993 Glasgow Workshop on Functional Programming, Ayr, Scotland, 5--7 July 1993",
  year="1994",
  publisher="Springer London",
  address="London",
  pages="201--221",
  abstract="We describe a simple strictness analyser for purely-functional programs, show how its results are used to improve programs, and provide measurements of the effects of these improvements. These measurements are given both in terms of overall run-time, and in terms of internal operations such as allocations, updates, etc.",
  isbn="978-1-4471-3236-3",
  doi="10.1007/978-1-4471-3236-3_17",
  url="https://doi.org/10.1007/978-1-4471-3236-3_17"
}

@inproceedings{Sun:16,
  author = {Sun, Chengnian and Le, Vu and Zhang, Qirun and Su, Zhendong},
  title = {Toward understanding compiler bugs in GCC and LLVM},
  year = {2016},
  isbn = {9781450343909},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/2931037.2931074},
  doi = {10.1145/2931037.2931074},
  abstract = {Compilers are critical, widely-used complex software. Bugs in them have significant impact, and can cause serious damage when they silently miscompile a safety-critical application. An in-depth understanding of compiler bugs can help detect and fix them. To this end, we conduct the first empirical study on the characteristics of the bugs in two main-stream compilers, GCC and LLVM. Our study is significant in scale — it exhaustively examines about 50K bugs and 30K bug fix revisions over more than a decade’s span. This paper details our systematic study. Summary findings include: (1) In both compilers, C++ is the most buggy component, accounting for around 20\% of the total bugs and twice as many as the second most buggy component; (2) the bug revealing test cases are typically small, with 80\% having fewer than 45 lines of code; (3) most of the bug fixes touch a single source file with small modifications (43 lines for GCC and 38 for LLVM on average); (4) the average lifetime of GCC bugs is 200 days, and 111 days for LLVM; and (5) high priority tends to be assigned to optimizer bugs, most notably 30\% of the bugs in GCC’s inter-procedural analysis component are labeled P1 (the highest priority). This study deepens our understanding of compiler bugs. For application developers, it shows that even mature production compilers still have many bugs, which may affect development. For researchers and compiler developers, it sheds light on interesting characteristics of compiler bugs, and highlights challenges and opportunities to more effectively test and debug compilers.},
  booktitle = {Proceedings of the 25th International Symposium on Software Testing and Analysis},
  pages = {294–305},
  numpages = {12},
  keywords = {compiler bugs, compiler testing, empirical studies},
  location = {Saarbr\"{u}cken, Germany},
  series = {ISSTA 2016}
}

@book{tapl,
  author = {Pierce, Benjamin C.},
  title = {Types and Programming Languages},
  year = {2002},
  isbn = {0262162091},
  publisher = {The MIT Press},
  edition = {1st},
  abstract = {A type system is a syntactic method for automatically checking the absence of certain erroneous behaviors by classifying program phrases according to the kinds of values they compute. The study of type systems -- and of programming languages from a type-theoretic perspective -- has important applications in software engineering, language design, high-performance compilers, and security.This text provides a comprehensive introduction both to type systems in computer science and to the basic theory of programming languages. The approach is pragmatic and operational; each new concept is motivated by programming examples and the more theoretical sections are driven by the needs of implementations. Each chapter is accompanied by numerous exercises and solutions, as well as a running implementation, available via the Web. Dependencies between chapters are explicitly identified, allowing readers to choose a variety of paths through the material.The core topics include the untyped lambda-calculus, simple type systems, type reconstruction, universal and existential polymorphism, subtyping, bounded quantification, recursive types, kinds, and type operators. Extended case studies develop a variety of approaches to modeling the features of object-oriented languages.}
}

@article{Blondal:18,
  author = {Bl\"{o}ndal, Baldur and L\"{o}h, Andres and Scott, Ryan},
  title = {Deriving via: or, how to turn hand-written instances into an anti-pattern},
  year = {2018},
  issue_date = {July 2018},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {53},
  number = {7},
  issn = {0362-1340},
  url = {https://doi.org/10.1145/3299711.3242746},
  doi = {10.1145/3299711.3242746},
  abstract = {Haskell's deriving construct is a cheap and cheerful way to quickly generate instances of type classes that follow common patterns. But at present, there is only a subset of such type class patterns that deriving supports, and if a particular class lies outside of this subset, then one cannot derive it at all, with no alternative except for laboriously declaring the instances by hand.  To overcome this deficit, we introduce Deriving Via, an extension to deriving that enables programmers to compose instances from named programming patterns, thereby turning deriving into a high-level domain-specific language for defining instances. Deriving Via leverages newtypes---an already familiar tool of the Haskell trade---to declare recurring patterns in a way that both feels natural and allows a high degree of abstraction.},
  journal = {SIGPLAN Not.},
  month = {sep},
  pages = {55–67},
  numpages = {13},
  keywords = {type classes, instances, functional programming, deriving, Haskell}
}

@article{Wei:18,
  author = {Wei, Guannan and Decker, James and Rompf, Tiark},
  title = {Refunctionalization of abstract abstract machines: bridging the gap between abstract abstract machines and abstract definitional interpreters (functional pearl)},
  year = {2018},
  issue_date = {September 2018},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {2},
  number = {ICFP},
  url = {https://doi.org/10.1145/3236800},
  doi = {10.1145/3236800},
  abstract = {Abstracting abstract machines is a systematic methodology for constructing sound static analyses for higher-order languages, by deriving small-step abstract abstract machines (AAMs) that perform abstract interpretation from abstract machines that perform concrete evaluation. Darais et al. apply the same underlying idea to monadic definitional interpreters, and obtain monadic abstract definitional interpreters (ADIs) that perform abstract interpretation in big-step style using monads. Yet, the relation between small-step abstract abstract machines and big-step abstract definitional interpreters is not well studied.  In this paper, we explain their functional correspondence and demonstrate how to systematically transform small-step abstract abstract machines into big-step abstract definitional interpreters. Building on known semantic interderivation techniques from the concrete evaluation setting, the transformations include linearization, lightweight fusion, disentanglement, refunctionalization, and the left inverse of the CPS transform. Linearization expresses nondeterministic choice through first-order data types, after which refunctionalization transforms the first-order data types that represent continuations into higher-order functions. The refunctionalized AAM is an abstract interpreter written in continuation-passing style (CPS) with two layers of continuations, which can be converted back to direct style with delimited control operators. Based on the known correspondence between delimited control and monads, we demonstrate that the explicit use of monads in abstract definitional interpreters is optional.  All transformations properly handle the collecting semantics and nondeterminism of abstract interpretation. Remarkably, we reveal how precise call/return matching in control-flow analysis can be obtained by refunctionalizing a small-step abstract abstract machine with proper caching.},
  journal = {Proc. ACM Program. Lang.},
  month = {jul},
  articleno = {105},
  numpages = {28},
  keywords = {refunctionalization, control-flow analysis, abstract machines, Scala}
}

@inproceedings{Atkey:13,
author = {Atkey, Robert and McBride, Conor},
title = {Productive coprogramming with guarded recursion},
year = {2013},
isbn = {9781450323260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2500365.2500597},
doi = {10.1145/2500365.2500597},
abstract = {Total functional programming offers the beguiling vision that, just by virtue of the compiler accepting a program, we are guaranteed that it will always terminate. In the case of programs that are not intended to terminate, e.g., servers, we are guaranteed that programs will always be productive. Productivity means that, even if a program generates an infinite amount of data, each piece will be generated in finite time. The theoretical underpinning for productive programming with infinite output is provided by the category theoretic notion of final coalgebras. Hence, we speak of coprogramming with non-well-founded codata, as a dual to programming with well-founded data like finite lists and trees.Systems that offer facilities for productive coprogramming, such as the proof assistants Coq and Agda, currently do so through syntactic guardedness checkers. Syntactic guardedness checkers ensure that all self-recursive calls are guarded by a use of a constructor. Such a check ensures productivity. Unfortunately, these syntactic checks are not compositional, and severely complicate coprogramming.Guarded recursion, originally due to Nakano, is tantalising as a basis for a flexible and compositional type-based approach to coprogramming. However, as we show, by itself, guarded recursion is not suitable for coprogramming due to the fact that there is no way to make finite observations on pieces of infinite data. In this paper, we introduce the concept of clock variables that index Nakano's guarded recursion. Clock variables allow us to "close over" the generation of infinite data, and to make finite observations, something that is not possible with guarded recursion alone.},
booktitle = {Proceedings of the 18th ACM SIGPLAN International Conference on Functional Programming},
pages = {197–208},
numpages = {12},
keywords = {total functional programming, guarded recursion, corecursion, coalgebras},
location = {Boston, Massachusetts, USA},
series = {ICFP '13}
}

@inproceedings{Henglein:94,
author = {Henglein, Fritz and J\o{}rgensen, Jesper},
title = {Formally optimal boxing},
year = {1994},
isbn = {0897916360},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/174675.177874},
doi = {10.1145/174675.177874},
abstract = {An important implementation decision in polymorphically typed functional programming language is whether to represent data in boxed or unboxed form and when to transform them from one representation to the other. Using a language with explicit representation types and boxing/unboxing operations we axiomatize equationally the set of all explicitly boxed versions, called completions, of a given source program. In a two-stage process we give some of the equations a rewriting interpretation that captures eliminating boxing/unboxing operations without relying on a specific implementation or even semantics of the underlying language. The resulting reduction systems operate on congruence classes of completions defined by the remaining equations E, which can  be understood as moving boxing/unboxing operations along data flow paths in the source program. We call a completion eopt formally optimal if every other completion for the same program (and at the same representation type) reduces to eopt under this two-stage reduction.We show that every source program has formally optimal completions, which are unique modulo E. This is accomplished by first “polarizing” the equations in E and orienting them to obtain two canonical (confluent and strongly normalizing) rewriting systems. The completions produced by Leroy's and Poulsen's algorithms are generally not formally optimal in our sense.The rewriting systems have  been implemented and applied to some simple Standard ML programs. Our results show that the amount of boxing and unboxing operations is also in practice substantially reduced in comparison to Leroy's completions. This analysis is intended to be integrated into Tofte's region-based implementation of Standard ML currently underway at DIKU.},
booktitle = {Proceedings of the 21st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
pages = {213–226},
numpages = {14},
keywords = {polymorphism, representation analysis, type inference},
location = {Portland, Oregon, USA},
series = {POPL '94}
}

@article{Gill:09,
  title={The worker/wrapper transformation},
  volume={19},
  DOI={10.1017/S0956796809007175},
  number={2},
  journal={Journal of Functional Programming},
  author={Gill, Andy and Hutton, Graham},
  year={2009},
  pages={227–251}
}

@inproceedings{Breitner:14,
  author = {Breitner, Joachim and Eisenberg, Richard A. and Peyton Jones, Simon and Weirich, Stephanie},
  title = {Safe zero-cost coercions for Haskell},
  year = {2014},
  isbn = {9781450328739},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/2628136.2628141},
  doi = {10.1145/2628136.2628141},
  abstract = {Generative type abstractions -- present in Haskell, OCaml, and other languages -- are useful concepts to help prevent programmer errors. They serve to create new types that are distinct at compile time but share a run-time representation with some base type. We present a new mechanism that allows for zero-cost conversions between generative type abstractions and their representations, even when such types are deeply nested. We prove type safety in the presence of these conversions and have implemented our work in GHC.},
  booktitle = {Proceedings of the 19th ACM SIGPLAN International Conference on Functional Programming},
  pages = {189–202},
  numpages = {14},
  keywords = {type class, newtype deriving, haskell, coercion},
  location = {Gothenburg, Sweden},
  series = {ICFP '14}
}

@inproceedings{Launchbury:94,
  author = {Launchbury, John and Peyton Jones, Simon L.},
  title = {Lazy functional state threads},
  year = {1994},
  isbn = {089791662X},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/178243.178246},
  doi = {10.1145/178243.178246},
  abstract = {Some algorithms make critical internal use of updatable state, even though their external specification is purely functional. Based on earlier work on monads, we present a way of securely encapsulating stateful computations that manipulate multiple, named, mutable objects, in the context of a non-strict, purely-functional language.The security of the encapsulation is assured by the type system, using parametricity. Intriguingly, this parametricity requires the provision of a (single) constant with a rank-2 polymorphic type.},
  booktitle = {Proceedings of the ACM SIGPLAN 1994 Conference on Programming Language Design and Implementation},
  pages = {24–35},
  numpages = {12},
  location = {Orlando, Florida, USA},
  series = {PLDI '94}
}

@misc{SPJ:06,
  author = {Peyton Jones, Simon and Sestoft, Peter and Hughes, John},
  title = {Demand Analysis},
  year = {2006},
  month = {July},
  url = {https://www.microsoft.com/en-us/research/publication/demand-analysis/},
}

@inproceedings{Brandl:23,
  author =	{Brandl, Katharina and Erdweg, Sebastian and Keidel, Sven and Hansen, Nils},
  title =	{{Modular Abstract Definitional Interpreters for WebAssembly}},
  booktitle =	{37th European Conference on Object-Oriented Programming (ECOOP 2023)},
  pages =	{5:1--5:28},
  series =	{Leibniz International Proceedings in Informatics (LIPIcs)},
  ISBN =	{978-3-95977-281-5},
  ISSN =	{1868-8969},
  year =	{2023},
  volume =	{263},
  editor =	{Ali, Karim and Salvaneschi, Guido},
  publisher =	{Schloss Dagstuhl -- Leibniz-Zentrum f{\"u}r Informatik},
  address =	{Dagstuhl, Germany},
  URL =		{https://drops.dagstuhl.de/entities/document/10.4230/LIPIcs.ECOOP.2023.5},
  URN =		{urn:nbn:de:0030-drops-181982},
  doi =		{10.4230/LIPIcs.ECOOP.2023.5},
  annote =	{Keywords: Static Analysis, WebAssembly}
}

@phdthesis{Ahmed:04,
  author = {Ahmed, Amal Jamil},
  title = {Semantics of types for mutable state},
  year = {2004},
  publisher = {Princeton University},
  address = {USA},
  abstract = {Proof-carrying code (PCC) is a framework for mechanically verifying the safety of machine language programs. A program that is successfully verified by a PCC system is guaranteed to be safe to execute, but this safety guarantee is contingent upon the correctness of various trusted components. For instance, in traditional PCC systems the trusted computing base includes a large set of low-level typing rules. Foundational PCC systems seek to minimize the size of the trusted computing base. In particular, they eliminate the need to trust complex, low-level type systems by providing machine-checkable proofs of type soundness for real machine languages. In this thesis, I demonstrate the use of logical relations for proving the soundness of type systems for mutable state. Specifically, I focus on type systems that ensure the safe allocation, update, and reuse of memory. For each type in the language, I define logical relations that explain the meaning of the type in terms of the operational semantics of the language. Using this model of types, I prove each typing rule as a lemma. The major contribution is a model of System F with general references—that is, mutable cells that can hold values of any closed type including other references, functions, recursive types, and impredicative quantified types. The model is based on ideas from both possible worlds and the indexed model of Appel and McAllester. I show how the model of mutable references is encoded in higher-order logic. I also show how to construct an indexed possible-worlds model for a von Neumann machine. The latter is used in the Princeton Foundational PCC system to prove type safety for a full-fledged low-level typed assembly language. Finally, I present a semantic model for a region calculus that supports type-invariant references as well as memory reuse.},
  note = {AAI3136691}
}

@article{Kripke:63,
	author = {Saul Kripke},
	journal = {Acta Philosophica Fennica},
	pages = {83--94},
	title = {Semantical Considerations on Modal Logic},
	volume = {16},
	year = {1963}
}


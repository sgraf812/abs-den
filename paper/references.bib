@inproceedings{Cousot-Cousot:POPL77,
  author = {Cousot, Patrick and Cousot, Radhia},
  title = {Abstract Interpretation: A Unified Lattice Model for Static Analysis of Programs by Construction or Approximation of Fixpoints},
  year = {1977},
  isbn = {9781450373500},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/512950.512973},
  doi = {10.1145/512950.512973},
  abstract = {A program denotes computations in some universe of objects. Abstract interpretation of programs consists in using that denotation to describe computations in another universe of abstract objects, so that the results of abstract execution give some information on the actual computations. An intuitive example (which we borrow from Sintzoff [72]) is the rule of signs. The text -1515 * 17 may be understood to denote computations on the abstract universe {(+), (-), (±)} where the semantics of arithmetic operators is defined by the rule of signs. The abstract execution -1515 * 17 → -(+) * (+) → (-) * (+) → (-), proves that -1515 * 17 is a negative number. Abstract interpretation is concerned by a particular underlying structure of the usual universe of computations (the sign, in our example). It gives a summary of some facets of the actual executions of a program. In general this summary is simple to obtain but inaccurate (e.g. -1515 + 17 → -(+) + (+) → (-) + (+) → (±)). Despite its fundamentally incomplete results abstract interpretation allows the programmer or the compiler to answer questions which do not need full knowledge of program executions or which tolerate an imprecise answer, (e.g. partial correctness proofs of programs ignoring the termination problems, type checking, program optimizations which are not carried in the absence of certainty about their feasibility, …).},
  booktitle = {Proceedings of the 4th ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages},
  pages = {238–252},
  numpages = {15},
  location = {Los Angeles, California},
  series = {POPL '77}
}

@techreport{Scott:81,
  title = {LECTURES ON A MATHEMATICAL THEORY OF COMPUTATION},
  author = {Dana S. Scott},
  year = {1981},
  institution = {Department of Computer Science, University of Oxford},
  number = {PRG19},
  pages = {152},
}

@inproceedings{imprecise-exceptions,
  author = {Peyton Jones, Simon and Reid, Alastair and Henderson, Fergus and Hoare, Tony and Marlow, Simon},
  title = {A Semantics for Imprecise Exceptions},
  year = {1999},
  isbn = {1581130945},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/301618.301637},
  doi = {10.1145/301618.301637},
  abstract = {Some modern superscalar microprocessors provide only imprecise exceptions. That is, they do not guarantee to report the same exception that would be encountered by a straightforward sequential execution of the program. In exchange, they offer increased performance or decreased chip area (which amount to much the same thing).This performance/precision tradeoff has not so far been much explored at the programming language level. In this paper we propose a design for imprecise exceptions in the lazy functional programming language Haskell. We discuss several designs, and conclude that imprecision is essential if the language is still to enjoy its current rich algebra of transformations. We sketch a precise semantics for the language extended with exceptions.The paper shows how to extend Haskell with exceptions without crippling the language or its compilers. We do not yet have enough experience of using the new mechanism to know whether it strikes an appropriate balance between expressiveness and performance.},
  booktitle = {Proceedings of the ACM SIGPLAN 1999 Conference on Programming Language Design and Implementation},
  pages = {25–36},
  numpages = {12},
  location = {Atlanta, Georgia, USA},
  series = {PLDI '99}
}

@book{DaveyPriestley:02,
  place={Cambridge},
  edition={2},
  title={Introduction to Lattices and Order},
  DOI={10.1017/CBO9780511809088},
  publisher={Cambridge University Press},
  author={Davey, B. A. and Priestley, H. A.},
  year={2002}
}

@article{Sestoft:97,
  title={Deriving a lazy abstract machine},
  volume={7},
  DOI={10.1017/S0956796897002712},
  number={3},
  journal={Journal of Functional Programming},
  publisher={Cambridge University Press},
  author={Sestoft, Peter},
  year={1997},
  pages={231–264}
}

@book{Nielson:99,
  author    = {Flemming Nielson and
               Hanne Riis Nielson and
               Chris Hankin},
  title     = {Principles of program analysis},
  publisher = {Springer},
  year      = {1999},
  url       = {https://doi.org/10.1007/978-3-662-03811-6},
  doi       = {10.1007/978-3-662-03811-6},
  isbn      = {978-3-540-65410-0},
  timestamp = {Tue, 16 May 2017 01:00:00 +0200},
  biburl    = {https://dblp.org/rec/books/daglib/0098888.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{Cousot:21,
  title={Principles of Abstract Interpretation},
  author={Cousot, Patrick},
  isbn={9780262044905},
  lccn={2020041256},
  url={https://mitpress.mit.edu/9780262044905/principles-of-abstract-interpretation/},
  year={2021},
  publisher={MIT Press}
}

@techreport{Scott:70,
  title = "Outline of a Mathematical Theory of Computation",
  author = "Dana Scott",
  year = "1970",
  institution = "OUCL",
  month = "November",
  number = "PRG02",
  pages = "30",
}

@techreport{ScottStrachey:71,
  title = "Toward a Mathematical Semantics for Computer Languages",
  author = "Dana Scott and Christopher Strachey",
  year = "1971",
  institution = "OUCL",
  month = "August",
  number = "PRG06",
  pages = "49",
}

@inproceedings{Shivers:91,
  Author = {Shivers, Olin Grigsby},
  Title = {Control-Flow Analysis of Higher-Order Languages or Taming Lambda},
  School = {Carnige-Mellon Univeristy},
  Month = {May},
  Year = {1991}
}

@inproceedings{cardinality,
  author = {Sergey, Ilya and Vytiniotis, Dimitrios and Peyton Jones, Simon},
  title = {Modular, Higher-Order Cardinality Analysis in Theory and Practice},
  year = {2014},
  isbn = {9781450325448},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/2535838.2535861},
  doi = {10.1145/2535838.2535861},
  abstract = {Since the mid '80s, compiler writers for functional languages (especially lazy ones) have been writing papers about identifying and exploiting thunks and lambdas that are used only once. However it has proved difficult to achieve both power and simplicity in practice. We describe a new, modular analysis for a higher-order language, which is both simple and effective, and present measurements of its use in a full-scale, state of the art optimising compiler. The analysis finds many single-entry thunks and one-shot lambdas and enables a number of program optimisations.},
  booktitle = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  pages = {335–347},
  numpages = {13},
  keywords = {types and effects, operational semantics, thunks, compilers, functional programming languages, program optimisation, lazy evaluation, haskell, static analysis, cardinality analysis},
  location = {San Diego, California, USA},
  series = {POPL '14}
}

@inproceedings{aam,
  author = {Van Horn, David and Might, Matthew},
  title = {Abstracting Abstract Machines},
  year = {2010},
  isbn = {9781605587943},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/1863543.1863553},
  doi = {10.1145/1863543.1863553},
  abstract = {We describe a derivational approach to abstract interpretation that yields novel and transparently sound static analyses when applied to well-established abstract machines. To demonstrate the technique and support our claim, we transform the CEK machine of Felleisen and Friedman, a lazy variant of Krivine's machine, and the stack-inspecting CM machine of Clements and Felleisen into abstract interpretations of themselves. The resulting analyses bound temporal ordering of program events; predict return-flow and stack-inspection behavior; and approximate the flow and evaluation of by-need parameters. For all of these machines, we find that a series of well-known concrete machine refactorings, plus a technique we call store-allocated continuations, leads to machines that abstract into static analyses simply by bounding their stores. We demonstrate that the technique scales up uniformly to allow static analysis of realistic language features, including tail calls, conditionals, side effects, exceptions, first-class continuations, and even garbage collection.},
  booktitle = {Proceedings of the 15th ACM SIGPLAN International Conference on Functional Programming},
  pages = {51–62},
  numpages = {12},
  keywords = {abstract machines, abstract interpretation},
  location = {Baltimore, Maryland, USA},
  series = {ICFP '10}
}

@inproceedings{Felleisen:87,
  author = {Felleisen, Mattias and Friedman, D. P.},
  title = {A Calculus for Assignments in Higher-Order Languages},
  year = {1987},
  isbn = {0897912152},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/41625.41654},
  doi = {10.1145/41625.41654},
  abstract = {Imperative assignments are abstractions of recurring programming patterns in purely functional programming languages. When added to higher-order functional languages, they provide a higher-level of modularity and security but invalidate the simple substitution semantics. We show that, given an operational interpretation of a denotational semantics for such a language, it is possible to design a two-level extension of the λu-calculus. This calculus provides a location-free rewriting semantics of the language and offers new possibilities for reasoning with assignments. The upper level of the calculus factors out all the steps in a reduction sequence which must be in a linear order; the lower level allows a partial ordering of reduction steps.},
  booktitle = {Proceedings of the 14th ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages},
  pages = {314},
  location = {Munich, West Germany},
  series = {POPL '87}
}

@article{cardinality-ext,
  title={Modular, higher order cardinality analysis in theory and practice},
  volume={27},
  DOI={10.1017/S0956796817000016},
  journal={Journal of Functional Programming},
  publisher={Cambridge University Press},
  author={Sergey, Ilya and Vytiniotis, Dimitrios and Peyton Jones, Simon and Breitner, Joachim},
  year={2017},
  pages={e11}
}

@inproceedings{Ariola:95,
  author = {Ariola, Zena M. and Maraist, John and Odersky, Martin and Felleisen, Matthias and Wadler, Philip},
  title = {A Call-by-Need Lambda Calculus},
  year = {1995},
  isbn = {0897916921},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/199448.199507},
  doi = {10.1145/199448.199507},
  abstract = {The mismatch between the operational semantics of the lambda calculus and the actual behavior of implementations is a major obstacle for compiler writers. They cannot explain the behavior of their evaluator in terms of source level syntax, and they cannot easily compare distinct implementations of different lazy strategies. In this paper we derive an equational characterization of call-by-need and prove it correct with respect to the original lambda calculus. The theory is a strictly smaller theory than the lambda calculus. Immediate applications of the theory concern the correctness proofs of a number of implementation strategies, e.g., the call-by-need continuation passing transformation and the realization of sharing via assignments.},
  booktitle = {Proceedings of the 22nd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  pages = {233–246},
  numpages = {14},
  location = {San Francisco, California, USA},
  series = {POPL '95}
}

@inproceedings{Atkey:18,
  author = {Atkey, Robert},
  title = {Syntax and Semantics of Quantitative Type Theory},
  year = {2018},
  isbn = {9781450355834},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3209108.3209189},
  doi = {10.1145/3209108.3209189},
  abstract = {We present Quantitative Type Theory, a Type Theory that records usage information for each variable in a judgement, based on a previous system by McBride. The usage information is used to give a realizability semantics using a variant of Linear Combinatory Algebras, refining the usual realizability semantics of Type Theory by accurately tracking resource behaviour. We define the semantics in terms of Quantitative Categories with Families, a novel extension of Categories with Families for modelling resource sensitive type theories.},
  booktitle = {Proceedings of the 33rd Annual ACM/IEEE Symposium on Logic in Computer Science},
  pages = {56–65},
  numpages = {10},
  keywords = {Type Theory, Linear Logic},
  location = {Oxford, United Kingdom},
  series = {LICS '18}
}

@inproceedings{Ullrich:19,
  author = {Ullrich, Sebastian and de Moura, Leonardo},
  title = {Counting Immutable Beans: Reference Counting Optimized for Purely Functional Programming},
  year = {2019},
  isbn = {9781450375627},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3412932.3412935},
  doi = {10.1145/3412932.3412935},
  abstract = {Most functional languages rely on some kind of garbage collection for automatic memory management. They usually eschew reference counting in favor of a tracing garbage collector, which has less bookkeeping overhead at runtime. On the other hand, having an exact reference count of each value can enable optimizations such as destructive updates. We explore these optimization opportunities in the context of an eager, purely functional programming language. We propose a new mechanism for efficiently reclaiming memory used by nonshared values, reducing stress on the global memory allocator. We describe an approach for minimizing the number of reference counts updates using borrowed references and a heuristic for automatically inferring borrow annotations. We implemented all these techniques in a new compiler for an eager and purely functional programming language with support for multi-threading. Our preliminary experimental results demonstrate our approach is competitive and often outperforms state-of-the-art compilers.},
  booktitle = {Proceedings of the 31st Symposium on Implementation and Application of Functional Languages},
  articleno = {3},
  numpages = {12},
  keywords = {lean, purely functional programming, reference counting},
  location = {Singapore, Singapore},
  series = {IFL '19}
}

@inproceedings{perceus,
author = {Reinking, Alex and Xie, Ningning and de Moura, Leonardo and Leijen, Daan},
title = {Perceus: Garbage Free Reference Counting with Reuse},
year = {2021},
isbn = {9781450383912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453483.3454032},
doi = {10.1145/3453483.3454032},
abstract = {We introduce Perceus, an algorithm for precise reference counting with reuse and specialization. Starting from a functional core language with explicit control-flow, Perceus emits precise reference counting instructions such that (cycle-free) programs are _garbage free_, where only live references are retained. This enables further optimizations, like reuse analysis that allows for guaranteed in-place updates at runtime. This in turn enables a novel programming paradigm that we call _functional but in-place_ (FBIP). Much like tail-call optimization enables writing loops with regular function calls, reuse analysis enables writing in-place mutating algorithms in a purely functional way. We give a novel formalization of reference counting in a linear resource calculus, and prove that Perceus is sound and garbage free. We show evidence that Perceus, as implemented in Koka, has good performance and is competitive with other state-of-the-art memory collectors.},
booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
pages = {96–111},
numpages = {16},
keywords = {Reference Counting, Handlers, Algebraic Effects},
location = {Virtual, Canada},
series = {PLDI 2021}
}

@article{Lorenzen:22,
  author = {Lorenzen, Anton and Leijen, Daan},
  title = {Reference Counting with Frame Limited Reuse},
  year = {2022},
  issue_date = {August 2022},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {6},
  number = {ICFP},
  url = {https://doi.org/10.1145/3547634},
  doi = {10.1145/3547634},
  abstract = {The recently introduced _Perceus_ algorithm can automatically insert reference count instructions such that the resulting (cycle-free) program is _garbage free_: objects are freed at the very moment they can no longer be referenced. An important extension is reuse analysis. This optimization pairs objects of known size with fresh allocations of the same size and tries to reuse the object in-place at runtime if it happens to be unique. Unfortunately, current implementations of reuse analysis are fragile with respect to small program transformations, or can cause an arbitrary increase in the peak heap usage. We present a novel _drop-guided_ reuse algorithm that is simpler and more robust than previous approaches. Moreover, we generalize the linear resource calculus to precisely characterize garbage-free and frame-limited evaluations. On each function call, a frame-limited evaluation may hold on to memory longer if the size is bounded by a constant factor. Using this framework we show that our drop-guided reuse _is_ frame-limited and find that an implementation of our new reuse approach in Koka can provide significant speedups.},
  journal = {Proc. ACM Program. Lang.},
  month = {aug},
  articleno = {103},
  numpages = {24},
  keywords = {Koka, Reuse, Frame Limited, Reference Counting}
}

@article{Cartwright:16,
  author    = {Robert Cartwright and
               Rebecca Parsons and
               Moez A. AbdelGawad},
  title     = {Domain Theory: An Introduction},
  journal   = {CoRR},
  volume    = {abs/1605.05858},
  year      = {2016},
  url       = {http://arxiv.org/abs/1605.05858},
  eprinttype = {arXiv},
  eprint    = {1605.05858},
  timestamp = {Mon, 13 Aug 2018 16:46:41 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/CartwrightPA16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Launchbury:93,
  author = {Launchbury, John},
  title = {A Natural Semantics for Lazy Evaluation},
  year = {1993},
  isbn = {0897915607},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/158511.158618},
  doi = {10.1145/158511.158618},
  abstract = {We define an operational semantics for lazy evaluation which provides an accurate model for sharing. The only computational structure we introduce is a set of bindings which corresponds closely to a heap. The semantics is set at a considerably higher level of abstraction than operational semantics for particular abstract machines, so is more suitable for a variety of proofs. Furthermore, because a heap is explicitly modelled, the semantics provides a suitable framework for studies about space behaviour of terms under lazy evaluation.},
  booktitle = {Proceedings of the 20th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  pages = {144–154},
  numpages = {11},
  location = {Charleston, South Carolina, USA},
  series = {POPL '93}
}

@article{AgerDanvyMidtgaard:04,
  title = {A functional correspondence between call-by-need evaluators and lazy abstract machines},
  journal = {Information Processing Letters},
  volume = {90},
  number = {5},
  pages = {223-232},
  year = {2004},
  issn = {0020-0190},
  doi = {https://doi.org/10.1016/j.ipl.2004.02.012},
  url = {https://www.sciencedirect.com/science/article/pii/S0020019004000638},
  author = {Mads Sig Ager and Olivier Danvy and Jan Midtgaard},
  keywords = {Functional programming, Program derivation, Interpreters, Abstract machines, Closure conversion, CPS transformation, Defunctionalization},
  abstract = {We bridge the gap between compositional evaluators and abstract machines for the lambda-calculus, using closure conversion, transformation into continuation-passing style, and defunctionalization of continuations. This article is a followup of our article at PPDP 2003, where we consider call by name and call by value. Here, however, we consider call by need. We derive a lazy abstract machine from an ordinary call-by-need evaluator that threads a heap of updatable cells. In this resulting abstract machine, the continuation fragment for updating a heap cell naturally appears as an ‘update marker’, an implementation technique that was invented for the Three Instruction Machine and subsequently used to construct lazy variants of Krivine's abstract machine. Tuning the evaluator leads to other implementation techniques such as unboxed values. The correctness of the resulting abstract machines is a corollary of the correctness of the original evaluators and of the program transformations used in the derivation.}
}

@article{Milner:78,
  title = {A theory of type polymorphism in programming},
  journal = {Journal of Computer and System Sciences},
  volume = {17},
  number = {3},
  pages = {348-375},
  year = {1978},
  issn = {0022-0000},
  doi = {https://doi.org/10.1016/0022-0000(78)90014-4},
  url = {https://www.sciencedirect.com/science/article/pii/0022000078900144},
  author = {Robin Milner},
  abstract = {The aim of this work is largely a practical one. A widely employed style of programming, particularly in structure-processing languages which impose no discipline of types, entails defining procedures which work well on objects of a wide variety. We present a formal type discipline for such polymorphic procedures in the context of a simple programming language, and a compile time type-checking algorithm W which enforces the discipline. A Semantic Soundness Theorem (based on a formal semantics for the language) states that well-type programs cannot “go wrong” and a Syntactic Soundness Theorem states that if W accepts a program then it is well typed. We also discuss extending these results to richer languages; a type-checking algorithm based on W is in fact already implemented and working, for the metalanguage ML in the Edinburgh LCF system.}
}

@misc{Czajka:2019,
  title={Coinduction: an elementary approach},
  author={Łukasz Czajka},
  year={2019},
  eprint={1501.04354},
  archivePrefix={arXiv},
  primaryClass={cs.LO}
}

@article{Plotkin:77,
  title = {LCF considered as a programming language},
  journal = {Theoretical Computer Science},
  volume = {5},
  number = {3},
  pages = {223-255},
  year = {1977},
  issn = {0304-3975},
  doi = {https://doi.org/10.1016/0304-3975(77)90044-5},
  url = {https://www.sciencedirect.com/science/article/pii/0304397577900445},
  author = {G.D. Plotkin},
  abstract = {The paper studies connections between denotational and operational semantics for a simple programming language based on LCF. It begins with the connection between the behaviour of a program and its denotation. It turns out that a program denotes ⊥ in any of several possible semantics if it does not terminate. From this it follows that if two terms have the same denotation in one of these semantics, they have the same behaviour in all contexts. The converse fails for all the semantics. If, however, the language is extended to allow certain parallel facilities behavioural equivalence does coincide with denotational equivalence in one of the semantics considered, which may therefore be called “fully abstract”. Next a connection is given which actually determines the semantics up to isomorphism from the behaviour alone. Conversely, by allowing further parallel facilities, every r.e. element of the fully abstract semantics becomes definable, thus characterising the programming language, up to interdefinability, from the set of r.e. elements of the domains of the semantics.}
}

@article{LeroyGrall:09,
  title = {Coinductive big-step operational semantics},
  journal = {Information and Computation},
  volume = {207},
  number = {2},
  pages = {284-304},
  year = {2009},
  note = {Special issue on Structural Operational Semantics (SOS)},
  issn = {0890-5401},
  doi = {https://doi.org/10.1016/j.ic.2007.12.004},
  url = {https://www.sciencedirect.com/science/article/pii/S0890540108001296},
  author = {Xavier Leroy and Hervé Grall},
  keywords = {Coinduction, Operational semantics, Big-step semantics, Natural semantics, Small-step semantics, Reduction semantics, Type soundness, Compiler correctness, Mechanized proofs, The Coq proof assistant},
  abstract = {Using a call-by-value functional language as an example, this article illustrates the use of coinductive definitions and proofs in big-step operational semantics, enabling it to describe diverging evaluations in addition to terminating evaluations. We formalize the connections between the coinductive big-step semantics and the standard small-step semantics, proving that both semantics are equivalent. We then study the use of coinductive big-step semantics in proofs of type soundness and proofs of semantic preservation for compilers. A methodological originality of this paper is that all results have been proved using the Coq proof assistant. We explain the proof-theoretic presentation of coinductive definitions and proofs offered by Coq, and show that it facilitates the discovery and the presentation of the results.}
}

@article{WrightFelleisen:94,
  title = {A Syntactic Approach to Type Soundness},
  journal = {Information and Computation},
  volume = {115},
  number = {1},
  pages = {38-94},
  year = {1994},
  issn = {0890-5401},
  doi = {https://doi.org/10.1006/inco.1994.1093},
  url = {https://www.sciencedirect.com/science/article/pii/S0890540184710935},
  author = {A.K. Wright and M. Felleisen},
  abstract = {We present a new approach to proving type soundness for Hindley/Milner-style polymorphic type systems. The keys to our approach are (1) an adaptation of subject reduction theorems from combinatory logic to programming languages, and (2) the use of rewriting techniques for the specification of the language semantics. The approach easily extends from polymorphic functional languages to imperative languages that provide references, exceptions, continuations, and similar features. We illustrate the technique with a type soundness theorem for the core of Standard ML, which includes the first type soundness proof for polymorphic exceptions and continuations.}
}

@article{Cousot:02,
  title = {Constructive design of a hierarchy of semantics of a transition system by abstract interpretation},
  journal = {Theoretical Computer Science},
  volume = {277},
  number = {1},
  pages = {47-103},
  year = {2002},
  note = {Static Analysis},
  issn = {0304-3975},
  doi = {https://doi.org/10.1016/S0304-3975(00)00313-3},
  url = {https://www.sciencedirect.com/science/article/pii/S0304397500003133},
  author = {Patrick Cousot},
  abstract = {We construct a hierarchy of semantics by successive abstract interpretations. Starting from the maximal trace semantics of a transition system, we derive the big-step semantics, termination and nontermination semantics, Plotkin's natural, Smyth's demoniac and Hoare's angelic relational semantics and equivalent nondeterministic denotational semantics (with alternative powerdomains to the Egli–Milner and Smyth constructions), D. Scott's deterministic denotational semantics, the generalized and Dijkstra's conservative/liberal predicate transformer semantics, the generalized/total and Hoare's partial correctness axiomatic semantics and the corresponding proof methods. All the semantics are presented in a uniform fixpoint form and the correspondences between these semantics are established through composable Galois connections, each semantics being formally calculated by abstract interpretation of a more concrete one using Kleene and/or Tarski fixpoint approximation transfer theorems.}
}

@article{AppelMcAllester:01,
  author = {Appel, Andrew W. and McAllester, David},
  title = {An Indexed Model of Recursive Types for Foundational Proof-Carrying Code},
  year = {2001},
  issue_date = {September 2001},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {23},
  number = {5},
  issn = {0164-0925},
  url = {https://doi.org/10.1145/504709.504712},
  doi = {10.1145/504709.504712},
  abstract = {The proofs of "traditional" proof carrying code (PCC) are type-specialized in the sense that they require axioms about a specific type system. In contrast, the proofs of foundational PCC explicitly define all required types and explicitly prove all the required properties of those types assuming only a fixed foundation of mathematics such as higher-order logic. Foundational PCC is both more flexible and more secure than type-specialized PCC.For foundational PCC we need semantic models of type systems on von Neumann machines. Previous models have been either too weak (lacking general recursive types and first-class function-pointers), too complex (requiring machine-checkable proofs of large bodies of computability theory), or not obviously applicable to von Neumann machines. Our new model is strong, simple, and works either in λ-calculus or on Pentiums.},
  journal = {ACM Trans. Program. Lang. Syst.},
  month = {sep},
  pages = {657–683},
  numpages = {27}
}

@article{DreyerAhmedBirkedal:11,
  author       = {Derek Dreyer and
                  Amal Ahmed and
                  Lars Birkedal},
  title        = {Logical Step-Indexed Logical Relations},
  journal      = {Log. Methods Comput. Sci.},
  volume       = {7},
  number       = {2},
  year         = {2011},
  url          = {https://doi.org/10.2168/LMCS-7(2:16)2011},
  doi          = {10.2168/LMCS-7(2:16)2011},
  timestamp    = {Sun, 16 Apr 2023 20:31:20 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1103-0510.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{ClarksonSchneider:10,
  author = {Clarkson, Michael R. and Schneider, Fred B.},
  title = {Hyperproperties},
  year = {2010},
  issue_date = {September 2010},
  publisher = {IOS Press},
  address = {NLD},
  volume = {18},
  number = {6},
  issn = {0926-227X},
  abstract = {Trace properties, which have long been used for reasoning about systems, are sets of execution traces. Hyperproperties, introduced here, are sets of trace properties. Hyperproperties can express security policies, such as secure information flow and service level agreements, that trace properties cannot. Safety and liveness are generalized to hyperproperties, and every hyperproperty is shown to be the intersection of a safety hyperproperty and a liveness hyperproperty. A verification technique for safety hyperproperties is given and is shown to generalize prior techniques for verifying secure information flow. Refinement is shown to be applicable with safety hyperproperties. A topological characterization of hyperproperties is given.},
  journal = {J. Comput. Secur.},
  month = {sep},
  pages = {1157–1210},
  numpages = {54},
  keywords = {liveness, safety, Security policies, information flow}
}

@book{Abramsky:94,
  author       = {Samson Abramsky and
                  Dov M. Gabbay and
                  T. S. E. Maibaum},
  title        = {Handbook of logic in computer science. Volume 3. Semantic Structures},
  publisher    = {Clarendon Press},
  year         = {1994},
  url          = {https://global.oup.com/academic/product/handbook-of-logic-in-computer-science-9780198537625},
  isbn         = {019853762X},
  timestamp    = {Tue, 16 Mar 2021 16:16:12 +0100},
  biburl       = {https://dblp.org/rec/books/lib/Abramsky94.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Nakano:00,
  author = {Nakano, Hiroshi},
  title = {A Modality for Recursion},
  year = {2000},
  isbn = {0769507255},
  publisher = {IEEE Computer Society},
  address = {USA},
  abstract = {We propose a modal logic that enables us to handle self-referential formulae, including ones with negative self-references, which on one hand, would introduce a logical contradiction, namely Russell's paradox, in the conventional setting, while on the other hand, are necessary to capture a certain class of programs such as fixed point combinators and objects with so-called binary methods in object-oriented programming. Our logic provides a basis for axiomatic semantics of such a wider range of programs and a new framework for natural construction of recursive programs in the proofs-as-programs paradigm.},
  booktitle = {Proceedings of the 15th Annual IEEE Symposium on Logic in Computer Science},
  pages = {255},
  keywords = {Logics of Programs, Lambda and Combinatory Caluculi, Type Systems and Type Theory, Specifications, Modal and Temporal Logics},
  series = {LICS '00}
}

@inproceedings{gdtt,
  author="Bizjak, Ale{\v{s}} and Grathwohl, Hans Bugge and Clouston, Ranald and M{\o}gelberg, Rasmus E. and Birkedal, Lars", editor="Jacobs, Bart and L{\"o}ding, Christof",
  title="Guarded Dependent Type Theory with Coinductive Types",
  booktitle="Foundations of Software Science and Computation Structures",
  year="2016",
  publisher="Springer Berlin Heidelberg",
  address="Berlin, Heidelberg",
  pages="20--35",
  isbn="978-3-662-49630-5"
}

@inproceedings{Coquand:94,
  author="Coquand, Thierry",
  editor="Barendregt, Henk
  and Nipkow, Tobias",
  title="Infinite objects in type theory",
  booktitle="Types for Proofs and Programs",
  year="1994",
  publisher="Springer Berlin Heidelberg",
  address="Berlin, Heidelberg",
  pages="62--78",
  abstract="We show that infinite objects can be constructively understood without the consideration of partial elements, or greatest fixed-points, through the explicit consideration of proof objects. We present then a proof system based on these explanations. According to this analysis, the proof expressions should have the same structure as the program expressions of a pure functional lazy language: variable, constructor, application, abstraction, case expressions, and local let expressions.",
  isbn="978-3-540-48440-0"
}

@inproceedings{BirkedalMogelbergEjlers:13,
  author = {Birkedal, Lars and Mogelberg, Rasmus Ejlers},
  title = {Intensional Type Theory with Guarded Recursive Types qua Fixed Points on Universes},
  year = {2013},
  isbn = {9780769550206},
  publisher = {IEEE Computer Society},
  address = {USA},
  url = {https://doi.org/10.1109/LICS.2013.27},
  doi = {10.1109/LICS.2013.27},
  booktitle = {Proceedings of the 2013 28th Annual ACM/IEEE Symposium on Logic in Computer Science},
  pages = {213–222},
  numpages = {10},
  series = {LICS '13}
}

@article{Moggi:07,
  title = {Structuring Operational Semantics: Simplification and Computation},
  journal = {Electronic Notes in Theoretical Computer Science},
  volume = {172},
  pages = {479-497},
  year = {2007},
  note = {Computation, Meaning, and Logic: Articles dedicated to Gordon Plotkin},
  issn = {1571-0661},
  doi = {https://doi.org/10.1016/j.entcs.2007.02.016},
  url = {https://www.sciencedirect.com/science/article/pii/S1571066107000898},
  author = {Eugenio Moggi},
  keywords = {Operational Semantics, Confluent Rewriting, Multiset Rewriting},
}

@article{McBridePaterson:08,
  title={Applicative programming with effects},
  volume={18},
  DOI={10.1017/S0956796807006326},
  number={1},
  journal={Journal of Functional Programming},
  publisher={Cambridge University Press},
  author={McBride, Conor and Paterson, Ross},
  year={2008},
  pages={1–13}
}

@inproceedings{Reynolds:72,
  author = {Reynolds, John C.},
  title = {Definitional Interpreters for Higher-Order Programming Languages},
  year = {1972},
  isbn = {9781450374927},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/800194.805852},
  doi = {10.1145/800194.805852},
  abstract = {Higher-order programming languages (i.e., languages in which procedures or labels can occur as values) are usually defined by interpreters which are themselves written in a programming language based on the lambda calculus (i.e., an applicative language such as pure LISP). Examples include McCarthy's definition of LISP, Landin's SECD machine, the Vienna definition of PL/I, Reynolds' definitions of GEDANKEN, and recent unpublished work by L. Morris and C. Wadsworth. Such definitions can be classified according to whether the interpreter contains higher-order functions, and whether the order of application (i.e., call-by-value versus call-by-name) in the defined language depends upon the order of application in the defining language. As an example, we consider the definition of a simple applicative programming language by means of an interpreter written in a similar language. Definitions in each of the above classifications are derived from one another by informal but constructive methods. The treatment of imperative features such as jumps and assignment is also discussed.},
  booktitle = {Proceedings of the ACM Annual Conference - Volume 2},
  pages = {717–740},
  numpages = {24},
  keywords = {GEDANKEN, Lambda calculus, Continuation, Language definition, SECD machine, Higher-order function, PAL, Interpreter, Order of application, Closure, LISP, Programming language, Applicative language, J-operator, Reference},
  location = {Boston, Massachusetts, USA},
  series = {ACM '72}
}

@article{tctt,
  author = {M\o{}gelberg, Rasmus Ejlers and Veltri, Niccol\`{o}},
  title = {Bisimulation as Path Type for Guarded Recursive Types},
  year = {2019},
  issue_date = {January 2019},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {3},
  number = {POPL},
  url = {https://doi.org/10.1145/3290317},
  doi = {10.1145/3290317},
  abstract = {In type theory, coinductive types are used to represent processes, and are thus crucial for the formal verification of non-terminating reactive programs in proof assistants based on type theory, such as Coq and Agda. Currently, programming and reasoning about coinductive types is difficult for two reasons: The need for recursive definitions to be productive, and the lack of coincidence of the built-in identity types and the important notion of bisimilarity. Guarded recursion in the sense of Nakano has recently been suggested as a possible approach to dealing with the problem of productivity, allowing this to be encoded in types. Indeed, coinductive types can be encoded using a combination of guarded recursion and universal quantification over clocks. This paper studies the notion of bisimilarity for guarded recursive types in Ticked Cubical Type Theory, an extension of Cubical Type Theory with guarded recursion. We prove that, for any functor, an abstract, category theoretic notion of bisimilarity for the final guarded coalgebra is equivalent (in the sense of homotopy type theory) to path equality (the primitive notion of equality in cubical type theory). As a worked example we study a guarded notion of labelled transition systems, and show that, as a special case of the general theorem, path equality coincides with an adaptation of the usual notion of bisimulation for processes. In particular, this implies that guarded recursion can be used to give simple equational reasoning proofs of bisimilarity. This work should be seen as a step towards obtaining bisimilarity as path equality for coinductive types using the encodings mentioned above.},
  journal = {Proc. ACM Program. Lang.},
  month = {jan},
  articleno = {4},
  numpages = {29},
  keywords = {cubical type theory, Dependent types, bisimulation, guarded recursion, homotopy type theory, coinductive types, labelled transition systems, CCS}
}

@inproceedings{MoranSands:99,
  author = {Moran, Andrew and Sands, David},
  title = {Improvement in a Lazy Context: An Operational Theory for Call-by-Need},
  year = {1999},
  isbn = {1581130953},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/292540.292547},
  doi = {10.1145/292540.292547},
  abstract = {The standard implementation technique for lazy functional languages is call-by-need, which ensures that an argument to a function in any given call is evaluated at most once. A significant problem with call-by-need is that it is difficult -- even for compiler writers -- to predict the effects of program transformations. The traditional theories for lazy functional languages are based on call-by-name models, and offer no help in determining which transformations do indeed optimize a program.In this article we present an operational theory for call-by-need, based upon an improvement ordering on programs: M is improved by N if in all program-contexts C, when C[M] terminates then C[N] terminates at least as cheaply.We show that this improvement relation satisfies a "context lemma", and supports a rich inequational theory, subsuming the call-by-need lambda calculi of Ariola et al. [AFM+95]. The reduction-based call-by-need calculi are inadequate as a theory of lazy-program transformation since they only permit transformations which speed up programs by at most a constant factor (a claim we substantiate); we go beyond the various reduction-based calculi for call-by-need by providing powerful proof rules for recursion, including syntactic continuity -- the basis of fixed-point-induction style reasoning, and an improvement theorem, suitable for arguing the correctness and safety of recursion-based program transformations.},
  booktitle = {Proceedings of the 26th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  pages = {43–56},
  numpages = {14},
  location = {San Antonio, Texas, USA},
  series = {POPL '99}
}

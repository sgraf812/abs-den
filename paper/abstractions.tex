\section{Abstract Interpretation}
\label{sec:abstractions}

\subsection{Lazy Denotational Deadness}

Let us first try to reformulate semantic deadness in terms of $\semevt{\wild}$
and $\equiv$:

\begin{definition}[Denotational deadness, lazily]
  \label{defn:deadness3}
  An address $\pa$ is \emph{dead} in a denotation $d$ if and only if,
  for all $μ_1 \approx μ_2$ such that $\pa$ is dead in $\rng(μ_i)$,
  $\dom(μ_1) ∪ \{\pa\} ⊦ d$ and $d_1,d_2$, we have
  \[
    \dom(μ_1) ⊦ d(μ_1[\pa↦d_1]) \sim d(μ_2[\pa↦d_2]).
  \]
  A variable $\px$ is \emph{dead} in an expression $\pe$ if and only
  if, for all $ρ ∈ \Var \to \EventD, \pe_1,\pe_2 ∈ \Exp$, we have
  \[\semevt{\Let{\px}{\pe_1}{\pe}}_{ρ} \equiv \semevt{\Let{\px}{\pe_2}{\pe}}_{ρ}.\]
  Otherwise, $\px$ is \emph{live}.
\end{definition}

If $x$ is dead in $\pe_2$ according to this definition, then we can justify the
following rewrite:
\[
  \Let{x}{\pe_1}{\pe_2} \equiv \Let{x}{\mathit{crash}}{\pe_2}
\]
A syntactic \emph{occurrence analysis} could subsequently figure out whether the
binding for $x$ can be dropped without introducing scoping errors.

We can now prove \Cref{thm:semusg-correct-live} in terms of this new
characterisation of deadness by induction:

\begin{theorem}[$\semusg{\wild}$ is a correct deadness analysis]
  \label{thm:semusg-correct-live-3}
  Let $\pe$ be an expression, $\px$ a variable and $\tr$ a usage environment.
  If $\tr(\px) \not⊑ \semusg{\pe}_{\tr}$
  then $\px$ is dead in $\pe$.
\end{theorem}
\begin{proof}
  By induction over $\pe$.
  The full proof can be found in \Cref{prf:semusg-correct-live-3}.
\end{proof}

\subsubsection*{Discussion}

The main proof of \Cref{thm:semusg-correct-live-3} is hardly longer than
\Cref{thm:semusg-correct-live}, but we have to admit that we needed to prove
quite a few metatheoretic properties to get there, so we declare only partial
victory on Goal 1 from \Cref{sec:problem}.
For obvious reasons, it seems preferable to stick to a simpler call-by-name
semantics without a heap if the property in question (\eg, deadness) can be
understood there as well.

Still, with \Cref{defn:deadness3} we were at least able to break
down the proof into manageable intermediate steps.
That is a huge step forward compares to the operational deadness definition of
\Cref{defn:deadness2} where we weren't even able to come up with a suitable
correctness relation.

In hindsight, we could trace back the intermediate steps to come up with at
least one proof strategy with \Cref{defn:deadness2}:
The structural induction principle is an important enabling factor and
the focus on maximal traces in \Cref{fig:semst-correctness} suggests that
we should likely strive for a correctness relation characterising a property of
maximal traces.
This was not obvious to us when we first set out to do the proof; the gap was
too large to see how to get to the other side due to the structural mismatch.
A nice consequence of successfully avoiding structural mismatch, which we set
out to in Goal 4.

\subsection{Improvement}

Proving that dead bindings can be soundly rewritten is a nice litmus test
for the semantics.
But does it really make the program faster, or at least not slower?

We can affirm that indirectly:
A diverging program $\mathit{loop}$ takes more steps than a stuck program
$\mathit{crash}$, hence the former runs ``slower'' than the latter.
The stuck and the diverging program are not semantically equivalent,
but $\Let{x}{\mathit{crash}}{\pe}$ is semantically equivalent to
$\Let{x}{\mathit{loop}}{\pe}$ when $x$ is dead in $\pe$.
Since $(\betastep)$ runs sub-programs to completion, we would observe
execution of ${\mathit{crash}}$ or ${\mathit{loop}}$, which is the only
way in which we could have made the program slower or faster.
Since there is no semantic difference, the performance of the program must be
unaffected.

This argument is quite vague.
In order to put it on firm ground, we define an \emph{improvement relation}
$(\faster)$ in the style of \citet{MoranSands:99} in \Cref{fig:improv}.

\begin{figure}
\[\begin{array}{c}
 \ruleform{ A ⊦_n τ_1 \lesssim τ_2 \qquad μ_1 \lessapprox μ_2 \qquad d_1 \faster d_2 }
 \\
 \\[-0.5em]
 \inferrule*[right=\implrcons]
    {\later (A ⊦_n τ_1 \lesssim τ_2)}
    {A ⊦_{n} \wild \cons τ_1 \lesssim \wild \cons τ_2}
 \quad
 \inferrule*[right=\imprcons]
    {A ⊦_{n+1} τ_1 \lesssim τ_2}
    {A ⊦_{n} τ_1 \lesssim \wild \cons τ_2}
 \quad
 \inferrule*[right=\implcons]
    {A ⊦_{n} τ_1 \lesssim τ_2}
    {A ⊦_{n+1} \wild \cons τ_1 \lesssim τ_2}
 \\
 \\[-0.5em]
 \inferrule*[right=\impstuck]
    {\quad}
    {A ⊦_{0} \stuckend{} \lesssim \stuckend{}}
 \quad
 \inferrule*[right=\impcon]
    {\later (A ⊦_{n} \many{d_1(μ_1) \lesssim d_2(μ_2)})}
    {A ⊦_{\Sigma \{\many{n}\}} \goodend{\ConV(K, \many{d_1}), μ_1} \lesssim \goodend{\ConV(K, \many{d_2}), μ_2}}
 \\
 \\[-0.5em]
 \inferrule*[right=\impfun]
    {\forall d.\ A ⊦ d ⟹  A ⊦_{n} f_1(d)(μ_1) \lesssim f_2(d)(μ_2)}
    {A ⊦_{n} \goodend{\FunV(f_1), μ_1} \lesssim \goodend{\FunV(f_2), μ_2}}
 \\
 \\[-0.5em]
 \inferrule*[right=\impheap]
    {\dom(μ_1) = \dom(μ_2) \quad \forall \pa.\ \later(\dom(μ_1) ⊦_{0} μ_1(\pa)(μ_1) \lesssim μ_2(\pa)(μ_2))}
    {μ_1 \lessapprox μ_2}
 \\
 \\[-0.5em]
 \inferrule*[right=\impdenot]
    {\forall μ_1,μ_2.\ μ_1 \lessapprox μ_2 \wedge  \dom(μ_1) ⊦ d_1,d_2 ⟹  \dom(μ_1) ⊦_{0} d_1(μ_1) \lesssim d_2(μ_2)}
    {d_1 \faster d_2}
\end{array}\]
\caption{Improvement relation}
  \label{fig:improv}
\end{figure}

Structurally, $(\faster)$ is very similar to $(\equiv)$; the main differences are in
the rules for $(\cons)$ and the resulting tracking of \emph{skew credits} $n∈ℕ$,
\eg, we count the applications of $\imprcons$ to spend them on $\implcons$ or
when a value is further scrutinised.
Naturally, it is easier to prove $A ⊦_n τ_1 \lesssim τ_2$ the more credits we
have at our expense and thus the larger $n$ is.

We write $d_1 \lockstep d_2$ when $d_1 \faster d_2$ and $d_2 \faster d_1$, in
which case both denotations operate in lockstep.
Already it is the case that $d_1 \faster d_2$ implies $d_1 \equiv d_2$,
so $(\faster)$ corresponds to the strong notion of improvement in
\citet{MoranSands:99}.
The weaker notion can be recovered by defining $\imprcons$ and $\implcons$ by
coinduction, thus accepting skew credits in $ℕ_ω$ instead of $ℕ$.

We conjecture that $(\faster)$ is a sub-relation of the strong contextual
improvement relation of \citet{MoranSands:99}, just as $(\equiv)$ is finer than
contextual equivalence.

We could now once again refine our notion of deadness:

\begin{definition}[Denotational deadness, improving]
  \label{defn:deadness4}
  A variable $\px$ is \emph{dead} in an expression $\pe$ if and only
  if, for all $ρ ∈ \Var \to \EventD, \pe_1,\pe_2 ∈ \Exp$, we have
  \[\semevt{\Let{\px}{\pe_1}{\pe}}_{ρ} \lockstep \semevt{\Let{\px}{\pe_2}{\pe}}_{ρ}.\]
  Otherwise, $\px$ is \emph{live}.
\end{definition}

And we will now assume that we have proven $\semusg{\wild}$ correct \wrt to this
new notion of deadness:
\begin{theorem}[$\semusg{\wild}$ is an improving deadness analysis]
  \label{thm:semusg-correct-live-4}
  Let $\pe$ be an expression, $\px$ a variable and $\tr$ a usage environment.
  If $\tr(\px) \not⊑ \semusg{\pe}_{\tr}$
  then $\px$ is dead in $\pe$.
\end{theorem}

The proof is much the same as \Cref{thm:semusg-correct-live-3}, because
intuitively, any derivation of $A ⊦ τ_1 \sim τ_2$ can be rewritten to never use
$\eqlcons$ and $\eqrcons$.
Such a derivation can be directly transformed into a proof for
$A ⊦_0 τ_1 \lesssim τ_2$, as we will never need $\implcons$ and $\imprcons$
and skew credits remain at $0$.

\subsection{Evaluation Cardinality}

Since our new semantics is able to express evaluation cardinality and thunk
update, we can try to add a new $\mathbf{let1}$ construct to our language that
opts out of memoisation:
\[
 \begin{array}{rcl}
  \derefn(\pa)(μ)   & = & \LookupA(\pa) \cons μ(\pa)(μ) \\
  \\[-0.5em]
  \semevt{\Letn{\px}{\pe_1}{\pe_2}}_ρ(μ) & = &
    \begin{letarray}
      \text{let} & ρ' = ρ[\px ↦ \highlight{\derefn(\pa)}] \quad \text{where $\pa \not∈ \dom(μ)$} \\
                 & d_1^\later = \semevt{\pe_1}_{ρ'} \\
      \text{in}  & \BindA(\px,\pa↦d_1^\later) \cons \semevt{\pe_2}_{ρ'}(μ[\pa ↦ d_1^\later])
    \end{letarray} \\
 \end{array}
\]
Any program in which we switch from memoised $\mathbf{let}$ to $\mathbf{let1}$
is semantically equivalent.
This is a simple consequence of the fact that $\deref(\pa) \equiv \derefn(\pa)$
and compositionality.

However, omitting thunk memoisation has measurable effect on performance
if the same variable is evaluated repeatedly!
We should rather show that whenever $x$ is \emph{evaluated at most
once}, it is an improvement to rewrite $\Let{x}{\pe_1}{\pe_2}$ to
$\Letn{x}{\pe_1}{\pe_2}$.

We can sharpen this statement by making use of \emph{tick algebra}
\citep{MoranSands:99}.
For that, we need to add a notion of ticks to our language, a routine extension:
\[
 \begin{array}{rcl}
  ε ∈ \Events   & ::= & ... \mid \TickA \\
  \\[-0.5em]
  \semevt{\tick \pe}_ρ(μ) & = & \TickA \cons \semevt{\pe}_{ρ}(μ) \\
 \end{array}
\]
Now, whenever $x$ is ``evaluated at most once'', we
have $\semevt{\Let{x}{\pe_1}{\pe_2}}_ρ \lockstep
      \semevt{\Letn{x}{\tick\pe_1}{\pe_2}}_ρ$.
What remains before we can formalise this statement is a precise notion of
evaluation cardinality, one that works in arbitrary contexts.

Intuitively, we need a function $\mathit{count}_\pa : \Traces \to ℕ_ω$ that counts
$\LookupA(\pa)$ actions at a particular address $\pa$ over the course of a
head-normal form reduction.
Then, ``$\pa$ is evaluated at most once'' roughly corresponds to
$\mathit{count}_\pa(\semevt{\pE[\pe_2]}_{[\px↦\deref(\pa)]}([\pa↦\semevt{\pe_1}_{[\px↦\deref(\pa)]}])) \leq 1$
in arbitrary contexts such that $\pE$ does not capture $\px$ nor preoccupies $\pa$.

This sketch has two flaws:
The first is that the proposition and thus $\mathit{count}$ needs to know the
address it should count, so we were forced to unfold the definition of
$\semevt{\Let{\px}{\pe_1}{\pe_2}}_ρ(μ)$ without knowing $μ$, leading to new side
conditions begging for an intricate definition, such as ``$\pE$ preoccupies
$\pa$''.
The second flaw is that $\pe_1$ can't have any free variables besides $\px$ this
way, so it is not a faithful model of $\mathbf{let}$-binding and perhaps too
weak to prove lockstep bisimilarity.

Both flaws are a result of the need to escape and re-enter the syntactic (or,
perhaps \emph{static}) realm of $\pE$ and $\pe_2$ to name the semantic (or
\emph{dynamic}) address which the proposition needs to refer to.
This could be solved in at least two ways:

Our first option would be to extend the semantics function to evaluation
contexts, with functionality
$\semevt{\pE} : ((\Var \to \EventD) \to \EventD) \to ((\Var \to \EventD) \to \EventD)$
such that $\semevt{\pE[\pe]}_ρ = \semevt{\pE}(\semevt{\pe})_ρ$.
This equation has a solution because of compositionality
and would allow for a faithful model of $\mathbf{let}$-binding.

The second option is the one we pursue in this work, because we believe it
yields interesting perspective:
Rather than to let syntactic evaluation contexts dictate how $\pe_2$ might be
evaluated to produce (essentially first-order) traces that we count addresses
in, we lift the counting of addresses through the whole domain, including
function values.
The resulting \emph{usage abstraction} $\usg_\EventD : \EventD \to \LookD$ is
the most precise usage analysis possible and is induced componentwise by the
event abstraction
\[
  \usg_{\Events}(ε) = \begin{cases}
      [\pa↦1] & ε = \LookupT(\pa) \\
      \constfn{0} & \text{otherwise}
    \end{cases}
\]
in \Cref{fig:usg-abs}.
The use of the term ``abstraction'' is justifiable by the following lemma:

% Nope, for the following we need preservation of (arbitrary) joins
%\begin{lemma}[Monotone functions have upper adjoints]
%  Let $α : C \to A$ be a monotone function between partial orders $(C,\leq)$ and $(A,⊑)$.
%  Then $γ : A \to C, γ(a) = \bigvee \{ c \mid α(c) ⊑ a \}$ is a monotone function as well
%  and $γ$ is upper adjoint to $α$.
%\end{lemma}
%\begin{proof}
%  For any $a,c$, let $α(c) ⊑ a$.
%  Then $c ∈ \{ c \mid α(c) ⊑ a \}$ and hence $c \leq γ(a)$.
%  Conversely, let $c \leq γ(a)$.
%  Then, by monotonicity, $α(c) ⊑ α(γ(a)) = α(\bigvee \{ c \mid α(c) ⊑ a \})$.
%\end{proof}

\begin{lemma}[Pointwise abstraction, after \citet{Cousot:21}]
  Let $f : C \to A$ be a function.
  Then $f^* : A \to \poset{C}, f^*(C) = \Lub \{ f(c) \mid c ∈ C \}$
  and $f^⊣ : A \to \poset{C}, f^⊣(a) = \bigcup \{ C \mid f^*(C) ⊑ a \}$
  form a Galois connection between partial orders
  $(A,⊑) \galois{f^*}{f^⊣}(\poset{C},⊆)$.
\end{lemma}
\begin{proof}
  For any $a,C$, let $f^*(C) ⊑ a$.
  Then $C ∈ \{ C \mid f^*(C) ⊑ a \}$ and hence $C ⊆ f^⊣(a)$.
  Conversely, let $C ⊆ f^⊣(a) = \bigcup \{ C \mid f^*(C) ⊑ a \} = \bigcup \{ C \mid \forall c∈C.\ f(c) ⊑ a \} = \{ c \mid f(c) ⊑ a \}$.
  Then, by monotonicity of $f^*$, $f^*(C) ⊑ \Lub \{ f(c) \mid f(c) ⊑ a \}$.
  But $a$ is an upper bound of $\{ f(c) \mid f(c) ⊑ a \}$, so it can't be below the least upper bound.
\end{proof}

And $\usg_\EventD$ is exactly such a pointwise abstraction.
We will apply $\usg_\EventD^*$ to the \emph{collecting semantics}
$\semcoll{\pe}_ρ \triangleq \{\semevt{\pe}_ρ\}$, but
$\usg_\EventD^*(\{\semevt{\pe}_ρ\}) = \usg_\EventD(\semevt{\pe}_ρ)$, so
$\usg_\EventD^*$ is barely worth talking about.

\Cref{fig:usg-abs} gives
the necessary upper adjoints $\usg^⊣_\EventD,$.

, because $\usg_\EventD$
gives rise to a monotone function on the powerset lattice
Leaving the obligation to prove that $\usg_\EventD$ is actually a Galois
Connection in a suitable sense for later,

we define
would be to compute one to fuse $\usg$ with
$\semevt{\pE[\pe]}_ρ$ to arrive at definition that is similar to


$\semevt{\pe_1}_{ρ[\px↦\deref(\pa)]}(μ[\pa↦\semevt{\pe_2}_ρ])$

\begin{definition}
  \label{defn:card}
  Let $d$ be a trace $\pa$ be an address.
  Then $\card_d(d)$ is the \emph{evaluation cardinality abstraction} of $\pa$ in $d$.
\end{definition}

\begin{figure}
\[\begin{array}{c}
 \arraycolsep=3pt
 \begin{array}{rclcl}
  μ & ∈ & \Heaps^{\Look} & =   & \Addresses \pfun \later\Domain{\Look} \\
  d & ∈ & \Domain{\Look} & =   & \Heaps^{\Look} \to \LookTraces \\
  u & ∈ & \Card & =   & \{ 0 ⊏ 1 ⊏ ω \} ⊂ ℕ_ω \\
  τ & ∈ & \LookTraces & =   & (l ∈ \Addresses \to \Card) \lcons \someend{v,μ} \\
  v & ∈ & \Values{\Look} & =   & \FunV(f ∈ \later\Domain{\Look} \to \Domain{\Look}) \\
 \end{array} \\
 \\[-0.5em]
 \begin{array}{lcl}
  \multicolumn{3}{c}{ \ruleform{ \card_{\Events} : \Events \to (\Addresses \to \Card)} } \\
  \\[-0.5em]
  \card_{\Events}(ε) & = & \begin{cases}
      [\pa↦1] & ε = \LookupT(\pa) \\
      \constfn{0} & \text{otherwise}
    \end{cases} \\
  l_1 +_1 (l_2 \lcons \someend{\tilde{v},\tm}) & = & (l_1+l_2) \lcons \someend{\tilde{v},\tm} \\
  \usg_{\Traces}(ε \cons τ) & = & \usg_{\Events}(ε) +_1 \usg_{\Traces}(τ) \\
  \usg_{\Traces}(\goodend{\FunV(f),μ}) & = & \constfn{0} \lcons \someend{\usg_{\EventV}(\FunV(f)), \usg_\Heaps(μ)} \\
  \usg_{\Traces}(\stuckend{}) & = & \constfn{0} \lcons \someend{\bot_{\Values{\UsgD}}, (\constfn{\bot_\EventD}) \circ μ} \\
  \usg_{\Heaps}(μ) & = & \usg_\EventD \circ μ \\
  \usg_{\EventD}(d) & = & \usg_\Traces \circ d \circ \usg^{⊣}_{\Heaps} \\
  \usg_{\EventV}(\FunV(f)) & = & \FunV(\usg_{\EventD} \circ f \circ \usg^{⊣}_{\EventD}) \\
  \usg^{⊣}_{\Heaps}(\tm) & = & \bigcup \{ μ \mid \usg_{\Heaps}(μ) ⊑ \tm \} \\
  \usg^{⊣}_{\EventD}(\td) & = & \bigcup \{ d \mid \usg_{\EventD}(d) ⊑ \td \} \\
  \\[-0.5em]
  \multicolumn{3}{c}{ \ruleform{ \byname : ((\Var \to \LookD) \to_m \LookD) \to_m ((\Var \to \LookD) \to_m \LookD) } } \\
  \\[-0.5em]
  \byname(\mathcal{S})(ρ)(μ) & = & \mathcal{S}(\fn{\px}{\fn{μ'}{ρ(\px)(μ)}})(μ) \\
  \\[-0.5em]
  \multicolumn{3}{c}{ \ruleform{ \ctx : \LookTraces \to_m \Addresses \to \Card } } \\
  \\[-0.5em]
  \ctx_\LookTraces(A,l \lcons \someend{\tv,\tm}) & = & l + \ctx_\LookValues(A,\tv,\tm) \\
  \ctx_\LookValues(A,\FunV(\tilde{f}),\tm) & = & \Lub \{ ω*\tilde{f}(\usg_\EventD(d))(\tm) \mid A ⊦ d \} \\
  \ctx_\LookD(\td) & = & \Lub \{ \tilde{f}(\usg_\EventD(d))(\tm) \mid A ⊦ d \} \\
  \\[-0.5em]
  \multicolumn{3}{c}{ \ruleform{ α : \Traces \to \UsgD } } \\
  \\[-0.5em]
  α(τ) & = & fst(intra(usg_\Traces(blub(τ)(\fn{\px}{[\px↦1]}))(τ))) \\
 \end{array}
\end{array}\]
\caption{Usage abstraction}
\label{fig:usg-abs}
\end{figure}

\begin{figure}
\[\begin{array}{c}
  \forall \tr.\ \tr(x) ⊑ \semusg{\pe}_{\tr} \\
  \Uparrow \\
  \exists \tr,\td_1,\td_2.\ \semusg{\pe}_{\tr[x ↦ \td_1]} \not= \semusg{\pe}_{\tr[x ↦ \td_2]} \\
  \Uparrow \text{ This one is non-trivial, but we can take any $α(d_1),α(d_2)$ and add $[x_1↦1],[x_2↦1]$ for some "fresh" vars}\\
  \exists ρ,d_1,d_2,ι_1,ι_2.\ \semevt{\pe}_{ρ[x ↦ d_1]}(ι_1) \not= \semevt{\pe}_{ρ[x ↦ d_2]}(ι_2) \\
  \Uparrow \\
  \exists \pe_1,\pe_2.\ \semevt{\Let{x}{\pe_1}{\pe}} \not= \semevt{\Let{x}{\pe_2}{\pe}} \\
  \Updownarrow \\
  x \text{ live in } \pe \\
\end{array}\]

\[\begin{array}{c}
  \exists \tr.\ \tr(x) \not⊑ \semusg{\pe}_{\tr} \\
  \Downarrow \\
  \forall \tr,\td_1,\td_2.\ \semusg{\pe}_{\tr[x ↦ \td_1]} = \semusg{\pe}_{\tr[x ↦ \td_2]} \\
  \Downarrow \text{ This one is non-trivial, but we can take any $α(d_1),α(d_2)$ and add $[x_1↦1],[x_2↦1]$ for some "fresh" vars}\\
  \forall ρ,d_1,d_2,ι_1,ι_2.\ \semevt{\pe}_{ρ[x ↦ d_1]}(ι_1) = \semevt{\pe}_{ρ[x ↦ d_2]}(ι_2) \\
  \Downarrow \\
  \forall \pe_1,\pe_2.\ \semevt{\Let{x}{\pe_1}{\pe}} = \semevt{\Let{x}{\pe_2}{\pe}} \\
  \Updownarrow \\
  x \text{ dead in } \pe \\
\end{array}\]

\end{figure}
